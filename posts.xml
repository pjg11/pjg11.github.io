<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator>
<link href="https://pjg1.site/posts.xml" rel="self" type="application/atom+xml" />
<link href="https://pjg1.site/" rel="alternate" type="text/html" />
<updated>2025-08-24T04:00:00+04:00</updated>
<id>https://pjg1.site/posts.xml</id>
<title type="html">pjg1.site</title>
<subtitle></subtitle>
<entry>
<title type="html">A minimal keyboard key effect with CSS</title>
<link href="https://pjg1.site/kbd-css.html" rel="alternate" type="text/html" title="A minimal keyboard key effect with CSS" />
<published>2025-02-27T00:00:00+04:00</published>
<updated>2025-02-27T00:00:00+04:00</updated>
<id>https://pjg1.site/kbd-css.html</id>
<content type="html" xml:base="https://pjg1.site/kbd-css.html">
<![CDATA[<p>I use the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/kbd"><code>kbd</code></a> element to specify keypresses in my posts. To differentiate it from the inline <code>code</code> element, I've styled it as a minimal version of an actual key:</p>

<pre><code>kbd {
  font-family: ui-monospace, monospace;
  font-size: 90%;
  margin: 0 0.07rem;
  padding: 0.07rem 0.35rem;
  border: 0.07rem solid;
  border-bottom: 0.18rem solid;
  border-radius: 0.21rem;
}

kbd:hover {
  border-bottom: 0.07rem solid;
  vertical-align: -0.1rem;
  cursor: text;
}</code></pre>

<p>The hover effect is inspired from <a href="https://dylanatsmith.com/wrote/styling-the-kbd-element">Styling the kbd element</a> by Dylan Smith. I experimented with the <code>border-bottom</code> and <code>vertical-align</code> values till I found a combination that recreated the hover effect well.</p>

<p>Here are keys from the QWERTY keyboard layout as an example:</p>

<div class="keyboard">
<kbd>Q</kbd><kbd>W</kbd><kbd>E</kbd><kbd>R</kbd><kbd>T</kbd><kbd>Y</kbd><kbd>U</kbd><kbd>I</kbd><kbd>O</kbd><kbd>P</kbd><br>
<kbd>A</kbd><kbd>S</kbd><kbd>D</kbd><kbd>F</kbd><kbd>G</kbd><kbd>H</kbd><kbd>J</kbd><kbd>K</kbd><kbd>L</kbd><br>
<kbd>Z</kbd><kbd>X</kbd><kbd>C</kbd><kbd>V</kbd><kbd>B</kbd><kbd>N</kbd><kbd>M</kbd>
</div>

]]>
</content>

<category term="css" />

</entry>
<entry>
<title type="html">Faded codeblocks using CSS</title>
<link href="https://pjg1.site/fade-block-css.html" rel="alternate" type="text/html" title="Faded codeblocks using CSS" />
<published>2025-02-26T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/fade-block-css.html</id>
<content type="html" xml:base="https://pjg1.site/fade-block-css.html">
<![CDATA[<p>One aspect I've gotten stuck on with the styling of this blog is code blocks. I've tried adding a border and/or a background color in the past, but I couldn't stick with either of them and constantly kept changing styles.</p>

<p>I wanted a subtler indication to scroll if a block overflows, and a fading gradient towards seemed like a good option.</p>

<p>Here's the CSS I wrote for it:</p>

<pre><code>pre {
  position: relative;
  background: white;
}
pre::after {
  content: "";
  position: absolute;
  top: 0;
  bottom: 0;
  left: 95%;
  right: 0;
  background-image: linear-gradient(to right, transparent, white);
}
@media (prefers-color-scheme: dark) {
  pre {
    background: black;
  }
  pre::after {
    background-image: linear-gradient(to right, transparent, black);
  }
}
pre code {
  display: block;
  padding: 0.75rem 0;
  overflow: auto;
  padding-inline-end: 1.5rem;
}</code></pre>

<p>The key behind this effect is the <code>::after</code> pseudo-element, which is a linear-gradient positioned to the right end of the <code>pre</code> block. The <code>left</code> value ensures that the gradient doesn't overlap the block completely, and acts as a subtle gradient, suggesting the user to scroll to see the code.</p>

<p>Usually I'd add the scroll to the <code>pre</code> block, however since we want the pseudoelement to stay at a fixed position, the scroll and overflow is applied to the child block - the <code>code</code> element in this case.</p>

<p>The gradient hiding overflow text makes sense, but this also covers the text at the end of the scroll. This is fixed by adding <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/padding-inline-end"><code>padding-inline-end</code></a> to the <code>code</code> element, which adds padding at the end of scroll.</p>

<p>Here's how it looks:</p>

<pre class="block"><code>This is a super long string of text that appears to be faded where the text overflows.</code></pre>

<p>While I've demonstrated this for codeblocks, this styling could be extended for any block element - say a paragraph within a div, or a paragraph within a blockquote.</p>

]]>
</content>

<category term="css" />

</entry>
<entry>
<title type="html">Baby's first monitoring system</title>
<link href="https://pjg1.site/first-monitoring-system.html" rel="alternate" type="text/html" title="Baby's first monitoring system" />
<published>2025-02-17T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/first-monitoring-system.html</id>
<content type="html" xml:base="https://pjg1.site/first-monitoring-system.html">
<![CDATA[<p>Till last week, I didn't know what a monitoring system really looked like. A week later, I'm in the process of setting up one for the RC's shared computing cluster<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup>  which is community maintained. Here are some notes about the various tools I'm using and how they work together (<a href="#tldr">TL;DR</a>).</p>

<p>I'm halfway through my second batch at RC, and one of my batch goals was to learn DevOps/SRE skills by contributing to this cluster. Having put it off for the first 5 weeks, I finally reached out to folks in the weekly meeting about the cluster, where I was recommended to look into <a href="https://prometheus.io">Prometheus</a>.</p>

<h2 id="prometheus">Prometheus</h2>

<p>The Prometheus server at its core is a database. More specifically, it is a time-series database, which means it stores key-value pairs with the key being a timestamp, thus showing how a particular value changed over time. This data can be used to create graphs and dashboards or trigger alerts if the values cross a certain threshold (more on both later). It has its own query language called PromQL.</p>

<p>The server can pull and store data from multiple machines, so it runs on only one of the machines in the cluster. However, if this machine goes down for some reason, our monitoring system is down.</p>

<h2 id="node_exporter">node_exporter</h2>

<p>We have a database, cool, but where does the data come from? There are a variety of tools for this<sup id="fnref:2"><a class="footnote" href="#fn:2" rel="footnote" role="doc-noteref">2</a></sup>, but the one I'm using here is Prometheus' own tool - <a href="https://github.com/prometheus/node_exporter">Node Exporter</a> - that captures metrics from the system - things like CPU usage, memory usage, filesystem sizes, etc. This runs on each machine in the cluster.</p>

<h2 id="alertmanager">Alertmanager</h2>

<p>The last piece in the puzzle is alerting. The Prometheus server takes in alert rules written in PromQL - things like checking for low disk space, checking if certain services failed to run, high CPU or memory usage, etc.</p>

<p>It checks the metrics against the rules as they arrive. If a rule is met, it sends the alert to a tool called <a href="https://github.com/prometheus/alertmanager">Alertmanager</a> that handles sending of notifications via email or a chat platform. RC uses Zulip for communication, which has an <a href="https://zulip.com/integrations/doc/alertmanager">integration for Alertmanager</a> that I'm using in this case.</p>

<h2 id="grafana">Grafana</h2>

<p>Grafana can be integrated with Prometheus to visualize the metrics via graphs and dashboards. I've mainly been focused on getting alerting to work so far, so I am yet to try making a dashboard.</p>

<h2 id="adding-kubernetes-to-the-mix">Adding Kubernetes to the mix</h2>

<p>The reason I was recommended Prometheus in the first place is because it had been deployed within a Kubernetes cluster by a fellow Recurser.</p>

<p>The above setup would have worked just fine if I ran them as individual services directly on the machine. However, I went ahead with the Kubernetes option for two reasons:</p>

<ol>
  <li>I preferred using something that was already deployed over re-inventing the wheel</li>
  <li>I'd been hearing a lot about Kubernetes, so this would finally be my introduction to the tool</li>
</ol>

<p>One advantage of using Kubernetes is that it makes multiple machines operate as one big unit - you provide it a list of services to deploy, and it'll figure out which machine's resources to utilize and how. Except for Node Exporter which is deployed on all machines, other services like Grafana, Alertmanager and the Prometheus server are deployed automagically.</p>

<p>In my case they're deployed using Kubernetes' package manager, Helm. While these eases setup, it also adds some layers of complexity.</p>

<p>Accessing the Prometheus web interface locally now requires more steps than a direct install, as it is isolated from the main system and has its own network and IP address. So multiple port forwards would be required.</p>

<p>Making changes to configuration files is also harder. Kubernetes containers don't have persistent disk space, so it isn't possible to exec into the containers and change files directly<sup id="fnref:3"><a class="footnote" href="#fn:3" rel="footnote" role="doc-noteref">3</a></sup> like I would with a direct install. So I have to add the configuration to some external file and then pass that file during deployment.</p>

<p>For applications deployed using Helm, I have to modify something called a Helm chart. I don't completely understand what the various files are for, but one of them is <a href="https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml"><code>values.yaml</code></a>, where I would add custom configuration like the alert rules for example. This file is passed to the install command, which then applies the custom config.</p>

<p>This complexity was preventing me from testing stuff quickly, so I decided to break the project into two phases. The first phase was testing Prometheus and alerting using a direct install, which I'm almost done with. Once I have a working set of config files, the next phase would be to figure out the Helm install and adding my configuration to the values file.</p>

<h2 id="tldr">TL;DR</h2>

<p>The following diagram is based on my limited understanding of the above concepts.</p>

<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ node 1        â”‚   â”‚ kubernetes cluster               â”‚
â”‚               â”‚   â”‚                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ node exporter â”‚â—€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”          â”‚ prometheus   â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚          â”‚ server       â”‚ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚  pull    â”‚              â”‚ â”‚
â”‚ node 2        â”‚   â”‚      â”‚ metrics  â”‚              â”‚ â”‚
â”‚               â”‚   â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚              â”‚ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚      â”‚          â”‚              â”‚ â”‚
â”‚ node exporter â”‚â—€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤          â”‚              â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚          â”‚ alert rules  â”‚ â”‚
â”‚ node 3        â”‚   â”‚      â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚   â”‚      â”‚                  â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚      â”‚      rule  â”Œâ”€â”€â”€â”€â”€â”˜        â”‚
â”‚ node exporter â”‚â—€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤      met   â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚            â–¼              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ node 4        â”‚   â”‚      â”‚    â”‚              â”‚  fire alert  â”‚         â”‚
â”‚               â”‚   â”‚      â”‚    â”‚ alertmanager â”‚â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–¶â”‚  zulip  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚      â”‚    â”‚              â”‚       â”‚      â”‚         â”‚
â”‚ node exporter â”‚â—€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

<p>Is baby's first monitoring system a bit complex? Yes.</p>

<p>Is baby learning new things and reaching <a href="https://www.recurse.com/self-directives#work-at-the-edge">the edge of their abilities</a> thanks to the complexity? ALSO YES!</p>
<h2 id="notes">Notes</h2>
  <ol class="footnotes" role="doc-endnotes">
    <li id="fn:1" role="doc-endnote">
      <p>I knew about the cluster in my first batch, but the thought of contributing to it came to mind only a year later, thanks to some folks from a later batch starting a meeting to discuss stuff relating to the cluster.&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>I found that metrics were already being captured on the machines using another tool while working on this. That tool didn't provide any support for alerts though, so I chose to switch to using Prometheus.&nbsp;<a class="reversefootnote" href="#fnref:2" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Prometheus has a web interface, so I expected that I would be able to change configuration and alert rules from the interface directly. I didn't find a way to do so though, and had to edit the file and restart the service each time.&nbsp;<a class="reversefootnote" href="#fnref:3" role="doc-backlink">â†©</a></p>
    </li>
  </ol>

]]>
</content>

<category term="recurse" />

<category term="sysadmin" />

</entry>
<entry>
<title type="html">How does a Linux machine connect to the internet, really?</title>
<link href="https://pjg1.site/linux-internet-from-scratch.html" rel="alternate" type="text/html" title="How does a Linux machine connect to the internet, really?" />
<published>2025-02-09T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/linux-internet-from-scratch.html</id>
<content type="html" xml:base="https://pjg1.site/linux-internet-from-scratch.html">
<![CDATA[<p>Recently, I was brainstorming networking project ideas, I got curious on what goes behind connecting to the internet, and if I could do it from scratch.</p>

<p>I'm delighted to report that the experiment was successful, and I thought of sharing it here! I've tested this on Ubuntu, but I think it should work on any Linux distribution. If not, <a href="mailto:piya@pjg1.site">let me know</a>.</p>

<ul>
  <li><a href="#identify-and-disable-existing-configuration">Identify and disable existing configuration</a></li>
  <li><a href="#additional-setup-for-wireless-interfaces">Additional setup for wireless interfaces</a></li>
  <li><a href="#setup-network-interface">Setup network interface</a></li>
  <li><a href="#set-a-default-gateway">Set a default gateway</a></li>
  <li><a href="#setup-dns">Setup DNS</a></li>
  <li><a href="#bonus-dynamic-addresses-via-dhcp">Bonus: Dynamic addresses via DHCP</a></li>
</ul>

<h2 id="identify-and-disable-existing-configuration">Identify and disable existing configuration</h2>

<p>Before I could set stuff up manually, I had to figure out my machine's existing configuration and disable it, so it wouldn't interfere with my handcrafted setup.</p>

<p>The <a href="https://documentation.ubuntu.com/server/explanation/networking/configuring-networks/">Ubuntu documentation</a> was a useful resource to find out the services in use. The network on my machine is configured using NetworkManager and DNS is managed using the systemd-resolved service.</p>

<p>I figured out what the above tools had setup using by trying out some of the code snippets in the docs, so I had a plan and a final result in mind.</p>

<p>Based on this, I made a note of the following from  the existing configuration, which can be found by running <code>ip addr show</code>:</p>

<ul>
  <li>Interface name - typically starts with one of <code>eth</code>, <code>en</code>, <code>wlan</code> or <code>wl</code>.</li>
  <li>IP address associated with the interface</li>
  <li>Subnet mask - the slash next to the IP address</li>
</ul>

<p>Once I had the information noted down, I disabled<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup> NetworkManager and systemd-resolved (both running as <code>systemd</code> services) and set the network interface to down:</p>

<pre><code># systemctl stop NetworkManager
# systemctl disable NetworkManager
Removed "/etc/systemd/system/network-online.target.wants/NetworkManager-wait-online.service".
Removed "/etc/systemd/system/multi-user.target.wants/NetworkManager.service".
Removed "/etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service".
# systemctl stop systemd-resolved
# systemctl disable systemd-resolved
# ip link set dev wlp3s0 down</code></pre>

<p>With this, the machine is no longer connected to the internet.</p>

<h2 id="additional-setup-for-wireless-interfaces">Additional setup for wireless interfaces</h2>

<p>There are two types of interfaces you could be setting up.</p>

<p>One is for a connection made by connecting an Ethernet cable to your machine. If you were to try out this post on a Linux VM, you would be setting up an Ethernet connection and can skip this section. The other is a wireless interface, which can connect to WiFi networks.</p>

<p>An Ethernet interface appears up/enabled at all times - even before it has actual internet access - as its connected via cable. Wireless interfaces on the other hand remain down/disabled until you connect to a WiFi network.</p>

<p>This led to differences during setup, which required me to add separate instructions for both, making the post long and confusing.</p>

<p>It is possible to connect to a WiFi network before having internet access - this would be similar to situations when your phone or laptop displays a "No Internet Connection" message while being connected to a network.</p>

<p>The tool that helps connect to WiFi is <a href="https://www.linuxfromscratch.org/blfs/view/stable-systemd/basicnet/wpa_supplicant.html"><code>wpa_supplicant</code></a>. This is what the previous setup used, so I went with it. There may be a process for it running in the background from the previous setup which is no longer required, so you can terminate it if it exists:</p>

<pre><code># ps -ef | grep -i [w]pa
root         883       1  0 01:59 ?        00:00:00 /usr/sbin/wpa_supplicant -u -s -O DIR=/run/wpa_supplicant GROUP=netdev
# systemctl stop wpa_supplicant
# systemctl disable wpa_supplicant</code></pre>

<p>The tool takes a configuration file, <code>wpa_supplicant.conf</code>, which contains information about the WiFi network you wish to connect to.</p>

<pre><code># cat &lt;&lt;EOF &gt; /etc/wpa_supplicant/wpa_supplicant.conf
ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1

network={
	ssid="&lt;name&gt;"
	psk="&lt;password&gt;"
}
EOF</code></pre>

<p>Replace <code>&lt;name&gt;</code> and <code>&lt;password&gt;</code> with your WiFi's name and password in plaintext. Yes, you read that right - a PASSWORD stored in PLAINTEXT<sup id="fnref:2"><a class="footnote" href="#fn:2" rel="footnote" role="doc-noteref">2</a></sup>. I'm pretty shocked by this, but it seems to be a norm for WiFi tools, not sure why<sup id="fnref:3"><a class="footnote" href="#fn:3" rel="footnote" role="doc-noteref">3</a></sup>.</p>

<p>I was following <a href="https://ubuntuforums.org/showthread.php?t=571188">this tutorial</a> which added further details to the network block like the protocol type and the encryption used. However, adding just the username and password seemed to work in my case.</p>

<p>Then, I ran <code>wpa_supplicant</code> with the config file:</p>

<pre><code># wpa_supplicant -D nl80211 -i wlp3s0 -c /etc/wpa_supplicant/wpa_supplicant.conf -B
Successfully initialized wpa_supplicant</code></pre>

<p>This is run as a background process (<code>-B</code>) so I can continue using the terminal to type other commands. I can confirm if the connection took place successfully via <code>iw</code>:</p>

<pre><code># iw dev wlp3s0 info
Interface wlp3s0
	ifindex 2
	wdev 0x1
	addr &lt;MAC&gt;
	ssid &lt;name&gt;
	type managed
	wiphy 0
...</code></pre>

<p>If the name next to the <code>ssid</code> field matches with name set in the configuration, that means the connection was successful.</p>

<h2 id="setup-network-interface">Setup network interface</h2>

<p>Without internet access, my network interface looked like this:</p>

<pre><code># ip addr show dev wlp3s0
2: wlp3s0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000
    link/ether &lt;MAC&gt; brd ff:ff:ff:ff:ff:ff</code></pre>

<p>It needed an IP address to be able to talk to other machines that was missing. I assigned it one based on the information I noted down from the previous setup:</p>

<pre><code># ip addr add 192.168.100.128/24 dev wlp3s0</code></pre>

<p><code>192.168.100.128</code> is the IP address and <code>/24</code> is the subnet. The subnet, - a shorthand for <code>255.255.255.0</code> - means that this network assigns addresses in the range of <code>192.168.100.X</code>, where <code>X</code> can be anywhere between 1 and 254 (0 and 255 are reserved).</p>

<p>I checked the interface after setting it to up, after which I can see the address!</p>

<pre><code># ip link set dev wlp3s0 up
# ip addr show dev wlp3s0
2: wlp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether &lt;MAC&gt; brd ff:ff:ff:ff:ff:ff
    inet 192.168.100.128/24 scope global wlp3s0
       valid_lft forever preferred_lft forever</code></pre>

<p>With this, I was online!</p>

<p>Well, sort of. If I tried to ping another machine in the same network, it worked!</p>

<pre><code># ping -c3 192.168.100.141
PING 192.168.100.141 (192.168.100.141) 56(84) bytes of data.
64 bytes from 192.168.100.141: icmp_seq=1 ttl=64 time=4.09 ms
64 bytes from 192.168.100.141: icmp_seq=2 ttl=64 time=92.1 ms
64 bytes from 192.168.100.141: icmp_seq=3 ttl=64 time=113 ms

--- 192.168.100.141 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 4.088/69.678/112.835/47.144 ms</code></pre>

<p>However, pinging a machine outside the network didn't.</p>

<pre><code># ping 1.1.1.1
ping: connect: Network is unreachable</code></pre>

<p>There had to be a way to route packets outside of the network.</p>

<h2 id="set-a-default-gateway">Set a default gateway</h2>

<p>Accessing machines outside of the network requires a default gateway - an address that forwards packets to other networks when the destination address isn't part of the network's address range. In a home network, this address would likely be assigned to your router.</p>

<p>This information is added to the routing table, and is typically the first assignable address in the address range, <code>192.168.100.1</code> in this case. The default gateway was set using <code>ip route</code>:</p>

<pre><code># ip route add default via 192.168.100.1 dev wlp3s0
# ip route show
default via 192.168.100.1 dev wlp3s0
192.168.100.0/24 wlp3s0 proto kernel scope link src 192.168.100.128</code></pre>

<p><code>ip route show</code> displays the routing table. The first rule is the one I just set, and the second one specifies routing for the entire address range, which was set after I assigned the address in the previous step.</p>

<p>Pinging to addresses outside of the network now worked!</p>

<pre><code># ping -c3 1.1.1.1
PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.
64 bytes from 1.1.1.1: icmp_seq=1 ttl=59 time=23.4 ms
64 bytes from 1.1.1.1: icmp_seq=2 ttl=59 time=8.74 ms
64 bytes from 1.1.1.1: icmp_seq=3 ttl=59 time=7.11 ms

--- 1.1.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 7.113/13.093/23.426/7.336 ms</code></pre>

<p>But if I were to try pinging a domain name, that wouldn't work.</p>

<pre><code># ping example.com
ping: example.com: Temporary failure in name resolution</code></pre>

<p>So close, yet so far. The error message meant that it is unable to translate example.com to an IP address, which points towards a DNS issue.</p>

<h2 id="setup-dns">Setup DNS</h2>

<p>The process of translating domain names to IP addresses is done by a nameserver. These name servers are defined in <code>/etc/resolv.conf</code>, which on my machine was a symbolic link:</p>

<pre><code># ls -l /etc/resolv.conf
lrwxrwxrwx 1 root root 39 Feb  7 04:49 /etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf</code></pre>

<p>This was part of the previous configuration, as DNS was setup using systemd-resolved on this machine. Since I've disabled that, I removed the symlink and added my nameservers of choice. I used <a href="https://www.cloudflare.com/learning/dns/what-is-1.1.1.1/">Cloudflare's public DNS server</a> in this case:</p>

<pre><code># rm /etc/resolv.conf
# cat &lt;&lt;EOF &gt; /etc/resolv.conf
nameserver 1.1.1.1
nameserver 1.0.0.1
EOF</code></pre>

<p>Pinging domain names finally worked!</p>

<pre><code># ping -c 1 example.com
PING example.com (96.7.128.198) 56(84) bytes of data.
64 bytes from a96-7-128-198.deploy.static.akamaitechnologies.com (96.7.128.198): icmp_seq=1 ttl=51 time=269 ms

--- example.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 269.178/269.178/269.178/0.000 ms</code></pre>

<p>This brings mission "connect to the Internet from scratch" to an end! I had a lot of fun working on this and learnt a lot, I hope you enjoyed reading this too! Before I end the post though, there's one little side quest I wanted to cover.</p>

<h2 id="bonus-dynamic-addresses-via-dhcp">Bonus: Dynamic addresses via DHCP</h2>

<p>The IP address I set above is a static IP, which doesn't change. Each time I connect to the network, I can assign it the same address.</p>

<p>There are two problems with this:</p>
<ul>
  <li>I need to know the address range for each network before I connect to it, which is time-consuming.</li>
  <li>Setting an IP this way might cause confusion if another machine has been assigned the same address.</li>
</ul>

<p>The solution for this is to let the network assign an address when you connect to it, which is how your default setup most likely works. This is done using DHCP or the Dynamic Host Configuration Protocol.</p>

<p>I got it working using a tool called <code>dhclient</code>. It doesn't work if an IP address is already assigned to the interface, so I removed the static IP and default gateway I had set first:</p>

<pre><code># ip addr flush dev wlp3s0
# ip route flush dev wlp3s0
# dhclient -v wlp3s0
Internet Systems Consortium DHCP Client 4.4.3-P1
Copyright 2004-2022 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/

Listening on LPF/wlp3s0/&lt;MAC&gt;
Sending on   LPF/wlp3s0/&lt;MAC&gt;
Sending on   Socket/fallback
xid: warning: no netdev with useable HWADDR found for seed's uniqueness enforcement
xid: rand init seed (0x67d6369b) built using gethostid
DHCPDISCOVER on wlp3s0 to 255.255.255.255 port 67 interval 3 (xid=0xf29dbc1c)
DHCPOFFER of 192.168.100.128 from 192.168.100.1
DHCPREQUEST for 192.168.100.128 on wlp3s0 to 255.255.255.255 port 67 (xid=0x1cbc9df2)
DHCPACK of 192.168.100.128 from 192.168.100.1 (xid=0xf29dbc1c)
bound to 192.168.100.128 -- renewal in 271244 seconds.</code></pre>

<p>From the output, it looks like the network's router (<code>192.168.100.1</code>) assigned this machine with the address <code>192.168.100.128</code>, which is what I was setting statically too.</p>

<p>What I also noticed was that running this also setup the default gateway and DNS automagically - and that too to the same address?!?!?</p>

<pre><code># ip route show | awk '/default via/{print $3}'
192.168.100.1
# cat /etc/resolv.conf
192.168.100.1</code></pre>

<p>After some searching, I <a href="https://lobste.rs/s/563zjp/how_does_linux_machine_connect_internet">found</a> that my router (aka the default gateway) is also capable of handling DNS requests. More specifically, it can forward DNS requests to servers it has configured that are likely specified by my ISP, and then send it back to my machine. Pretty cool!</p>

<p>What's not so cool though, is that this one command basically automated almost everything I set up lovingly by hand :/ The experiment was still worth it though, as I now know exactly what steps the tool is automating.</p>
<h2 id="notes">Notes</h2>
  <ol class="footnotes" role="doc-endnotes">
    <li id="fn:1" role="doc-endnote">
      <p>In my first attempt, I removed NetworkManager from the system all together, but reached a dead end and had to reinstall it. That's why I recommend disabling instead, as its easy to start over by enabling the service.&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>It is possible to generate a password hash with a tool called <code>wpa_passphrase</code>, but turns out that you can use the hash as is to connect to a network without knowing the actual password. This kind of makes hashing pointless.&nbsp;<a class="reversefootnote" href="#fnref:2" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Even NetworkManager had my WiFi password stored in plaintext in a config file, which was a shocker. The rationale provided is that the file permissions are set such that only root can access it, making it safe. I'm not so sure about that.&nbsp;<a class="reversefootnote" href="#fnref:3" role="doc-backlink">â†©</a></p>
    </li>
  </ol>

]]>
</content>

<category term="linux" />

<category term="recurse" />

<category term="networking" />

</entry>
<entry>
<title type="html">Things I learnt while working on ZulipFS</title>
<link href="https://pjg1.site/zulipfs.html" rel="alternate" type="text/html" title="Things I learnt while working on ZulipFS" />
<published>2025-02-03T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/zulipfs.html</id>
<content type="html" xml:base="https://pjg1.site/zulipfs.html">
<![CDATA[<p>I came across <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace">FUSE</a> recently, which brought up an idea: <em>what if I could access a <a href="https://zulip.com">Zulip</a> instance as a filesystem?</em></p>

<p>This led to the creation of ZulipFS, where channels are represented as directories, and topics as files within those directories. The usage and code for the project is available <a href="https://github.com/pjg11/zulipfs">here</a>. This post talks about things I learnt about FUSE, filesystems in general and design choices I made in the process.</p>

<h2 id="how-fuse-works-in-a-nutshell">How FUSE works in a nutshell</h2>

<p>FUSE lets you mount a folder containing files and folders. These could be actual files and folders (eg: <a href="https://github.com/libfuse/sshfs">SSHFS</a> where files from a remote network are mounted) or virtual files (eg: <a href="https://omar.website/tabfs/">TabFS</a>, where information from browser tabs are presented as files and folders). You write your own implementation for relevant <a href="https://libfuse.github.io/doxygen/structfuse__operations.html">system calls</a> relating to file operations, and FUSE will run your implementation instead of the standard implementation.</p>

<p>The system calls I implemented in ZulipFS are:</p>

<ul>
  <li><code>read</code> + <code>write</code> for reading and writing files</li>
  <li><code>readdir</code> to list directory information</li>
  <li><code>getattr</code> that returns the metadata for each file and directory</li>
</ul>

<h2 id="file-metadata-is-more-important-than-i-thought">File metadata is more important than I thought</h2>

<p>When I implemented and tested reading a message from a topic, only part of a message got printed. I checked if the API call was returning a partial message, but that seemed fine.</p>

<p>Turns out, I had the file size set to 512 bytes as I was working off of <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/dnsfs.py">this example code</a>. So <code>read</code> checks for the size of a file and prints only that many bytes, which makes sense!</p>

<p>I now had to figure out a way to get the file size before the message has been read.</p>

<h2 id="reading-and-knowing-a-file-are-two-different-things">Reading and knowing a file are two different things</h2>

<p>The file size is set in the <code>getattr</code> function, which is called each time a file is read or listed. So the Zulip API would be called twice for a file read - once in <code>getattr</code> to get the length of the message, and then in <code>read</code> to display the message itself.</p>

<p>For this, I created a function called <code>get_topic</code> which can be called by <code>getattr</code> and <code>read</code>:</p>

<pre><code>def get_topic(self, channel, topic):
  channel_id = self.channels[channel]['stream_id']

  # returns the ID of the last message along with the topic name
  topicslist = self.client.get_stream_topics(channel_id)['topics']
  self.topics[channel] = { self.normalize(t['name']): t for t in topicslist }

  # get the message contents using the message ID
  message = self.client.get_raw_message(self.topics[channel][topic]['max_id'])
  message_fmt = f"""[{datetime.fromtimestamp(message['message']['timestamp'])}] {message['message']['sender_full_name']}
{message['raw_content']}
""".encode()

  self.topics[channel][topic] = {
    'last_message': message_fmt,
    'last_timestamp': float(message['message']['timestamp']),
  }
  return self.topics[channel][topic]</code></pre>

<p>Making the same API calls twice felt a bit excessive for reading, but it was okay as long as it wasn't slowing things down. Then I tried listing all the topics in a channel via <code>ls</code>, and things slowed downâ€¦A LOT.</p>

<p>Why, you ask? The function handling directory listing, <code>readdir</code>, calls <code>getattr</code> for EACH FILE in the directory. If a channel has 300 topics, that's 300 API calls before <code>ls</code> completes execution. To add to the chaos, <code>get_topic</code> above uses API calls instead of one, which means 600 API calls before <code>ls</code> completes execution. I had to find ways to optimize this.</p>

<h2 id="lazy-loading-files">Lazy loading files?!?!?</h2>

<p>The first optimization attempt was to remove the <code>get_topic</code> call from <code>getattr</code>, and call it only in <code>read</code>. I placed an exception block in <code>getattr</code>, which would assign a file size of 65535 bytes on mount, and a subsequent <code>read</code> would fill the hash map with the correct values, which <code>getattr</code> would take the next time its called.</p>

<pre><code>def getattr(self, path):
# ...snip...
# topic/file
try:
  channel, topic = path[1:].split('/')
  try:
    timestamp = self.topics[channel][topic]['last_timestamp']
  except KeyError:
    timestamp = now

  try:
    st.st_size = len(self.topics[channel][topic]['last_message'])
  except KeyError:
    st.st_size = 65535
# ...snip...</code></pre>

<p>This worked initially, but caused problems when I wanted to append new messages instead of just displaying the last one. After lots of trial and error, a question popped up in my head: <em>What if I don't create all topic files right away, and add them only after someone tries to read or list it?</em></p>

<p>This seemed like a great idea as it would significantly reduce the number of API calls made at once. Things might slow down eventually as you read more and more topics, but it would still be faster than trying to list all topics at once.</p>

<p>Another optimization I was able to make was combining the two API calls into one using  <a href="https://zulip.com/api/get-messages"><code>get_messages</code></a>, which powers Zulip's search functionality. I can pass it the name of a channel and topic, and ask it to return the last message of that topic. If either the channel or topic doesn't exist, it'll return an empty result.</p>

<pre><code>def get_topic(self, channel, topic):
  request = {
    "anchor": "newest",
    "num_before": 1,
    "num_after": 0,
    "narrow": [
      {"operator": "channel", "operand": self.channels[channel]['name']},
      {"operator": "topic", "operand": self.zulip_name(topic)},
    ],
    "apply_markdown": False,
  }
  try:
    message = self.client.get_messages(request)['messages'][0]
    message_fmt = f"""[{datetime.fromtimestamp(message['timestamp'])}] {message['sender_full_name']}
{message['content']}
""".encode()

    self.topics[channel][topic] = {
      'last_message': message_fmt,
      'last_timestamp': float(message['timestamp']),
    }
  except IndexError:
    # channel or topic doesn't exist
    pass

  # if a channel or topic doesn't exist, this statement will cause an
  # exception in the function where this is called.
  return self.topics[channel][topic]</code></pre>

<p>These optimizations made things fast enough that I could call <code>get_topic</code> from <code>getattr</code> again, so I could get rid of the extra try/except blocks:</p>

<pre><code># topic/file
try:
  channel, topic = path[1:].split('/')
  t = self.get_topic(channel, topic)
  st.st_mode = stat.S_IFREG | 0o644
  st.st_nlink = 1
  st.st_size = len(t['last_message'])
  st.st_mtime = t['last_timestamp']
except (KeyError, ValueError):
  return -errno.ENOENT</code></pre>

<h2 id="appending-new-messages">Appending new messages</h2>

<p>I presented the pre-optimization version at the weekly <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a> presentations, and fellow batchmates <a href="https://eieio.games">Nolen</a> and <a href="https://bsky.app/profile/ohsh.it">Kevin O</a> suggested to add the ability to read new messages from a topic as they arrive by running <code>tail -f</code> on the file. This seemed like a good idea, and more useful than displaying just the last message.</p>

<p>I initially thought appending would require implementing a system call, but it was easier than I thought - if the timestamp of the current message is newer than the previous one, I append the new message to the end of the previous one in <code>get_topic</code>. I also needed an additional check for whether the topic had been read before or not, to initialize the file for the first time.</p>

<p>The <code>try</code> block in <code>get_topic</code> now looks like this:</p>

<pre><code>try:
  message = self.client.get_messages(request)['messages'][0]
  timestamp = float(message['timestamp'])
  message_fmt = f"""[{datetime.fromtimestamp(message['timestamp'])}] {message['sender_full_name']}
{message['content']}
""".encode()

  if topic not in self.topics[channel]:
    # First message in file
    self.topics[channel][topic] = {
      'last_message': message_fmt,
      'last_timestamp': timestamp,
    }
    else:
      # Subsequent messages appended to file
      if timestamp &gt; self.topics[channel][topic]['last_timestamp']:
        self.topics[channel][topic] = {
          'last_message': self.topics[channel][topic]['last_message'] + b"\n" + message_fmt,
          'last_timestamp': timestamp,
        }
except IndexError:
  # channel or topic doesn't exist
  pass</code></pre>

<h2 id="filename-gotchas">Filename gotchas</h2>

<p>One of the earliest errors I encountered was displaying names that had slashes in them. In Linux and other Unix-based OS's, a slash is considered as a delimiter for a directory rather than part of a filename. One thing I'd seen certain apps do is change special characters to their URL-encoded versions, so I replaced all instances of <code>/</code> with <code>%2F</code>.</p>

<p>Another set of characters that are inconvenient to type in the terminal are emojis. I was initially thinking of getting rid of them, but then I realized that looking up the channel name would become tricky.</p>

<p>I remembered seeing textual representations of emojis, which turns out are called <a href="https://emojipedia.org/shortcodes">shortcodes</a>, and they're written as text in-between colons <code>:</code>. For example, the shortcode for ğŸ“ is <code>:memo:</code>â€‹, and these are understood by Zulip. Python has an <code>emoji</code> package that converts emojis to shortcodes and vice versa.</p>

<p>With that, I had two functions to convert Zulip names to a valid filename and vice versa.</p>

<pre><code>def file_name(self, name):
  return emoji.demojize(name.replace('/', '%2F'))

def zulip_name(self, name):
  return emoji.emojize(name.replace('%2F', '/'))</code></pre>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>Thanks to <a href="https://fractalkitty.com">Sophia</a> for reviewing a draft of this post.</p>

]]>
</content>

<category term="recurse" />

</entry>
<entry>
<title type="html">Making sense of zsh history shell options</title>
<link href="https://pjg1.site/zsh-history-opts.html" rel="alternate" type="text/html" title="Making sense of zsh history shell options" />
<published>2025-01-09T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/zsh-history-opts.html</id>
<content type="html" xml:base="https://pjg1.site/zsh-history-opts.html">
<![CDATA[<p>I was refactoring my <code>.zshrc</code> recently, and found these existing options for managing the command history:</p>

<pre><code>setopt INC_APPEND_HISTORY
setopt SHARE_HISTORY
setopt HIST_IGNORE_DUPS
setopt HIST_IGNORE_ALL_DUPS
setopt HIST_SAVE_NO_DUPS
setopt HIST_IGNORE_SPACE</code></pre>

<p>I think I copied these from somewhere whenever I last modified the file. But this time, I wanted to apply a more systematic approach to setting options, where I was sure I knew exactly what each option in the file was doing.</p>

<p>I stared at these options for a bit. Some of these look similar to each other, and I don't know how each one is different despite comments. Do I even need all of these?</p>

<p>For the purposes of this article, I'll be dividing the options into categories:</p>

<ul>
<li>Appending history - <code>SHARE_HISTORY</code>, <code>INC_APPEND_HISTORY</code></li>
<li>Managing duplicates - <code>HIST_IGNORE_DUPS</code>, <code>HIST_IGNORE_ALL_DUPS</code>, <code>HIST_SAVE_NO_DUPS</code></li>
<li><code>HIST_IGNORE_SPACE</code> makes sense right away, so I'll add it to my config directly.</li>
</ul>

<h2 id="appending-history">Appending history</h2>

<p>The default behavior for writing to the history file is to write all commands from a session in bulk at the end of the session. I'm looking for a way to append commands from different sessions as they're entered to the history file, aka sharing the same file across sessions.</p>

<p>The solution for this was simple, <code>man zshoptions</code>:</p>

<pre><code><strong>APPEND_HISTORY</strong> &lt;D&gt;
   If this is set, zsh sessions will append their history list to the
   history file, rather than replace it. Thus, multiple parallel zsh
   sessions will all have the new entries from their history lists
   added to the history file, in the order that they exit. [...]</code></pre>

<pre><code><strong>INC_APPEND_HISTORY</strong>
   This option works like <strong>APPEND_HISTORY</strong> except that new history lines
   are added to the <strong>$HISTFILE</strong> incrementally (as soon as they are
   entered), rather than waiting until the shell exits.</code></pre>

<pre><code><strong>SHARE_HISTORY</strong> &lt;K&gt;
   This option both imports new commands from the history file, and
   also causes your typed commands to be appended to the history file
   (the latter is like specifying <strong>INC_APPEND_HISTORY</strong>, which should be
   turned off if this option is in effect).</code></pre>

<p>One of the first things that stood out was the fact that only one of these options needs to be set. <code>INC_APPEND_HISTORY</code> has the functionality of <code>APPEND_HISTORY</code> and part of <code>SHARE_HISTORY</code> works like <code>INC_APPEND_HISTORY</code>.</p>

<p><code>APPEND_HISTORY</code> didn't do what I wanted, so it was up to me to decide of making a choice between the latter two.</p>

<p>I'm primarily concered around writing commands to the file, so that they're available in any sessions I start after it, not so much existing shell sessions (which <code>SHARE_HISTORY</code> does), so I chose <code>INC_APPEND_HISTORY</code>.</p>

<p>Another option I found in the <code>man</code> page was <code>INC_APPEND_HISTORY_TIME</code>, which works like <code>INC_APPEND_HISTORY</code> but appends the commands to the file once they've completed, which I thought was cool.</p>

<h2 id="managing-duplicates">Managing duplicates</h2>

<p>The default behavior is to keep duplicates. I'm looking for a way to store only the most recent version of a command and delete all instances of it from the file, as the command is entered.</p>

<p>Starting with the <code>man</code> page again:</p>

<pre><code><strong>HIST_IGNORE_ALL_DUPS</strong>
   If a new command line being added to the history list duplicates an
   older one, the older command is removed from the list (even if it
   is not the previous event).</code></pre>

<pre><code><strong>HIST_IGNORE_DUPS</strong> (-h)
   Do not enter command lines into the history list if they are
   duplicates of the previous event.</code></pre>

<pre><code><strong>HIST_SAVE_NO_DUPS</strong>
   When writing out the history file, older commands that duplicate
   newer ones are omitted.</code></pre>

<p><code>HIST_IGNORE_DUPS</code> is a subset of <code>HIST_IGNORE_ALL_DUPS</code>, and so the choice is between <code>HIST_SAVE_NO_DUPS</code> and <code>HIST_IGNORE_ALL_DUPS</code>.</p>

<h3 id="hist_save_no_dups"><code>HIST_SAVE_NO_DUPS</code></h3>

<p>Just going by the description and names, <code>HIST_SAVE_NO_DUPS</code> should have worked, but it didn't:</p>

<pre><code>$ setopt HIST_SAVE_NO_DUPS
$ tail -2 ~/.zsh_history
setopt HIST_SAVE_NO_DUPS
tail -2 ~/.zsh_history
$ tail -2 ~/.zsh_history
tail -2 ~/.zsh_history
tail -2 ~/.zsh_history
$ # it's saving duplicates :o</code></pre>

<p>If I close the above session and view the file in a new session, it removes the duplicate <code>tail -2</code> command:</p>

<pre><code>$ tail -5 ~/.zsh_history
m ~/.zshrc
source ~/.zshrc
setopt HIST_SAVE_NO_DUPS
tail -2 ~/.zsh_history
tail -5 ~/.zsh_history</code></pre>

<p>I'm probably misunderstanding how the option works. I thought "writing out the history file" meant each time the command got appended, now that I've set the appending to be that way. But it looks like the removal of duplicates happens only at the end of the session, irrespective of the append behavior.</p>

<p>I looked at the zsh source code for evidence of this, and it turns out this option is only referenced in <a href="https://github.com/zsh-users/zsh/blob/263659acb73d0222e641dfd8d37e48e96582de02/Src/hist.c#L2951">hist.c</a>, in a function called <code>hend</code>, indicating the end of history related operations. This seems like something that would run at the end of a shell session.</p>

<h3 id="hist_ignore_all_dups"><code>HIST_IGNORE_ALL_DUPS</code></h3>

<p>Setting this option seemed to work, sort of.</p>

<pre><code>$ setopt HIST_IGNORE_ALL_DUPS
$ tail -2 ~/.zsh_history
setopt HIST_IGNORE_ALL_DUPS
tail -2 ~/.zsh_history
$ tail -2 ~/.zsh_history
setopt HIST_IGNORE_ALL_DUPS
tail -2 ~/.zsh_history
$ # that worked :D</code></pre>

<p>While it avoided adding immediate repeated commands (like <code>HIST_IGNORE_DUPS</code>), it removed older instances only once the session was closed (like <code>HIST_SAVE_NO_DUPS</code>).</p>

<p>Then I looked back at the man page, and noticed something I hadn't noticed before: <code>HIST_SAVE_NO_DUPS</code> makes changes to the history "file", whereas <code>HIST_IGNORE_ALL_DUPS</code> makes changes to the history "list".</p>

<p>How is a history "list" different from a history "file"? The history list stores commands for a particular shell session, before they're written to the history file. Keeping in mind the default behavior for saving history, having a temporary list per session makes sense. However, it looks like this list is in use even when the append behavior is changed.</p>

<p>To see how this option affects the list, we can view it using the <code>history</code> command:</p>

<pre><code>$ setopt HIST_IGNORE_ALL_DUPS
$ echo hello
hello
$ echo hello
hello
$ history -2
 1394  setopt HIST_IGNORE_ALL_DUPS
 1395  echo hello</code></pre>

<p>Commands don't repeat in the history list, and hence aren't repeated in the history file too! To confirm that it works for all older instances of a command, I tried running a command that appears slightly early on in the list:</p>

<pre><code>$ history 0 | grep 'echo test'
 1413  echo test
$ echo test
test
$ history 0 | grep 'echo test'
 1428  echo test</code></pre>

<p>The line number changed, which means that the older instance was removed from the history list. However, the history file still has the older duplicate:</p>

<pre><code>$ cat ~/.zsh_history | grep '^echo test$'
echo test
echo test</code></pre>

<p>This is due to the same reason as for why <code>HIST_SAVE_NO_DUPS</code> didn't work - the removal of duplicates from the history file happens only once a shell session ends.</p>

<p>In a nutshell, <code>HIST_IGNORE_ALL_DUPS</code> works like <code>HIST_SAVE_NO_DUPS</code> with the added functionality of removing dupes in the history list. While I expected a shell option to remove older dupes from the file as they were added, this option seems like a resonable alternative.</p>

<h2 id="fin">Fin.</h2>

<p>Phew, that was an unexpectedly long adventure! But my history config has now reduced from 6 lines that I wasn't sure about, to 3 lines that I can confidently reason about!</p>

<pre><code>setopt INC_APPEND_HISTORY
setopt HIST_IGNORE_ALL_DUPS
setopt HIST_IGNORE_SPACE</code></pre>

]]>
</content>

<category term="shell" />

</entry>
<entry>
<title type="html">Re-thinking the way I manage personal projects</title>
<link href="https://pjg1.site/personal-proj-mgmt.html" rel="alternate" type="text/html" title="Re-thinking the way I manage personal projects" />
<published>2024-09-27T00:00:00+04:00</published>
<updated>2025-02-10T00:00:00+04:00</updated>
<id>https://pjg1.site/personal-proj-mgmt.html</id>
<content type="html" xml:base="https://pjg1.site/personal-proj-mgmt.html">
<![CDATA[<p>I have been trying a new way to keeping track of projects, and I went down a fun rabbit hole before settling on an approach, that I thought would be fun to share.</p>

<h2 id="but-first-some-context">But first, some context</h2>

<p>I've been maintaining a <a href="https://stackoverflow.blog/2024/05/22/you-should-keep-a-developer-s-journal/">developer journal</a> for the past few months. I create a note each day and write down things I learnt as I'm working on them, or use it to break down and keep track of tasks when I'm stuck. If I'm working on multiple projects in the same day, they all go in the same daily note.</p>

<p>Recently, I came across the concept of <a href="https://nesslabs.com/interstitial-journaling">interstitial journaling</a>, where you write stuff throughout the day, but with a timestamp before each message. It seemed like a good idea as it gives me a better sense of how my day went, and was easy to incorporate with my then writing app of choice, <a href="http://logseq.com/">Logseq</a>.</p>

<p>While looking for Logseq plugins, I came across the following comment on a <a href="https://www.reddit.com/r/logseq/comments/tdysnj/comment/k0skrb8/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">Reddit thread</a>:</p>

<blockquote>
  <p>This is why I used personal IRC and Discord servers/channels for logging/journaling. Just start typing; all entries are automatically time-stamped, sorted, and cloud-backed.</p>
</blockquote>

<p>My mind was BLOWN. The idea of a chat platform as a means of logging personal projects made so much sense! I had basically been trying to recreate the feeling of chatting with myself, and I didn't realize it until I read this comment.</p>

<p>I had been coming across the idea of <a href="https://github.com/awesome-selfhosted/awesome-selfhosted">self-hosting</a> while looking for project ideas and saw IRC mentioned there, so I looked into it. <strong>That's where the idea clicked</strong>.</p>

<h2 id="channels-as-project-logs">Channels as project logs</h2>

<p>Trying out IRC felt like using something like Slack or Discord, but much more minimal. It is a text-based chat protocol based on the client-server model. There are servers that consist of one or more channels, and clients with which you can connect to and interact on channels by joining them.</p>

<p>The <a href="https://weechat.org/about/screenshots/weechat/weechat_2010-02-22_hullap.png/">interface</a> is also a minimal version of Slack or Discord, with a list of channels you've joined on the left and time-stamped messages from a channel in the center.</p>

<p>Now how does this relate to project management, you ask?</p>

<ul>
  <li>Each channel represents one project I'm working on</li>
  <li>When I want to log something specific to a project, I post a message in that channel</li>
</ul>

<p>This simple concept solves issues I've faced with the existing dev journal setup.</p>

<p>When I revisit a project after a break (which I often do), that would require searching through older notes to figure out where I last left off. While the daily notes made things easier on a day-to-day basis, I often felt the lack of a overall view of a project.</p>

<p>A channel helps with both time-tracking and project management:</p>

<ul>
  <li>The messages are time-stamped, so I know how my day went</li>
  <li>The channel serves as the overall picture of my project - what I've done, what I need to do and where I last left off. Everything relating to a project is in one place.</li>
  <li>The channels list in the side tells me at a glance which projects I'm working on, and lets me easily switch between them.</li>
</ul>

<p>This also helps in cases when I want to write a post about something I did over a long period of time - say weeks - and I forget details of what I did earlier.</p>

<p>A chat interface encourages me to write smaller updates, so I end up with a reliable log of events, as opposed to me having logged only certain aspects and trying to remember other details while writing the post (extremely annoying, trust me). And <a href="https://pjg1.site/writing-is-thinking">more writing leads to more thinking</a>, so I consider this approach a win!</p>

<p>So I have the idea set, now comes the rabbit hole I went into while trying to look for ways to implement this.</p>

<h2 id="finding-the-perfect-implementation">Finding the perfect implementation</h2>

<h3 id="self-hosted-irc-server">Self-hosted IRC server</h3>

<p>Since the idea came from IRC, my first idea was to use the self-hosted server I used to try IRC in the first place. I had setup <a href="http://unrealircd.org/">UnrealIRCd</a> on my daily driver, and installed two IRC clients to try - <a href="https://www.codeux.com/textual/">Textual</a> (a GUI) and <a href="http://irssi.org/">irssi</a> (a TUI).</p>

<p>It all worked fine, but I came across its limitations pretty quickly. It doesn't support multi-line messages and making the chat persist across sessions required additional setup, which didn't seem worth the effort.</p>

<h3 id="self-hosted-zulip-server">Self-hosted Zulip server</h3>

<p>A great open-source alternative to Slack/Discord is <a href="https://zulipchat.com/">Zulip</a>, which is used as the primary chat platform at the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a>. I'm familiar with the interface and it has the concept of channels, which I can use to manage projects.</p>

<p>After some trial and error, I was able to self-host an instance using their <a href="https://github.com/zulip/docker-zulip">docker image</a>. It worked well, but it also felt overkill for some reason. I realized I wanted something that didn't rely on the internet to function, but something that looked like a chat platform visually.</p>

<h3 id="custom-built-tui">Custom-built TUI</h3>

<p>I really like the <a href="https://github.com/charmbracelet/bubbletea">Bubbletea framework</a> for TUIs, and had it in my "make something with this someday" list. This seemed like a good chance, and I even found some projects as inspiration!</p>

<ul>
  <li><a href="https://github.com/bashbunni/pjs">pjs</a> follows the same concept of time-stamped project logging, but with a different implementation. It uses a SQLite DB to store notes, and each note is displayed separately as opposed to a chat interface.</li>
  <li><a href="https://github.com/maaslalani/nap">nap</a> is a code snippet manager that reads and displays plaintext files, and the layout is pretty close to what I had in mind.</li>
</ul>

<p>However, my feelings about TUIs are mixed - I find them really cool but don't use them much for some reason. This made me hesitant from really giving the idea a fair chance. I feared I would spend all this effort making something, and lose interest while making it.</p>

<p>This would have made for a great programming project though. Maybe someday.</p>

<h3 id="existing-gui-apps">Existing GUI apps</h3>

<p>I didn't expect to find much here, but I ended up finding multiple apps.</p>

<p>The one app I ended up trying was <a href="http://strflow.app/">Strflow</a>, a macOS app. It provides a chat-like interface, where you can type messages with a subset of Markdown and the option to add tags. Each tag then becomes its own view, so you either see all messages at once or filter by a specific tag. While I would have preferred separate chats for each project, filtering by tags seemed fair enough.</p>

<p>I tried Strflow for about a day, and it worked really well! It allowed me to import my existing notes (there's an option to select an earlier date) and had a floating window shortcut, which displays a tinier version of the chat interface. It didn't make me go "this is it" though.</p>

<p>Somewhere, I was still craving on making my own implementation, so I finally settled on an idea involving good 'ol plaintext files and Markdown.</p>

<h2 id="current-setup">Current setup</h2>

<p>I created a folder called <code>logs</code> which contains all of the project logs. Each project corresponds to one log file - <code>PROJNAME.md</code>, and follows this format:</p>

<pre><code>#### 27 Sep 2024

##### 16:54
Lorem ipsum dolor sit amet, consectetur adipiscing elit.

##### 20:55
Curabitur laoreet fermentum enim malesuada volutpat.

Praesent semper non odio at vestibulum.

...</code></pre>

<p>I thought of choosing a shorter custom syntax, but if I ever want to export these logs as HTML, using existing syntax would make conversion and styling easier. I also don't use <code>h4</code> and <code>h5</code> tags in my writing, so using them here makes it easy to parse individual messages if I ever need to do that.</p>

<p>I added custom CSS set to my Markdown editor, <a href="https://typora.io/">Typora</a>, to render the log files in a cool way. The above log file renders like so:</p>

<blockquote class="log">
  <h4 id="sep-2024">27 Sep 2024</h4>

  <h5 id="section">16:54</h5>
  <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>

  <h5 id="section-1">20:55</h5>
  <p>Curabitur laoreet fermentum enim malesuada volutpat.</p>

  <p>Praesent semper non odio at vestibulum.</p>

  <p>â€¦</p>
</blockquote>

<p>Then there's the shell script to automate the logging process (parts of the script are omitted):</p>

<pre><code>file="$LOGDIR/$1.md"
shift

if [[ -f "$file" ]]; then
        lastdate="$(grep '^#### ' "$file" | tail -n1 | cut -c5- )"
        todate="$(date '+%b %d %Y')"

        cat &lt;&lt;-EndOfMessage &gt;&gt; "$file"
                $(if [[ "$lastdate" != "$todate" ]]; then echo "#### $todate"; fi)
                ##### $(date '+%H:%M')
                ${@:-$($EDITOR $(mktemp))}
                EndOfMessage
fi</code></pre>

<p>If the file exists, it extracts the last date in a file, and compares it with the current date. If the dates are different (for instance posting a note past midnight), it inserts the date, followed by the timestamp.</p>

<p>Line 11 is the content of the message. That cool bash syntax means "take the remaining command line arguments if not empty, otherwise open a temporary file using an editor and take its contents after the editor is closed instead".</p>

<h2 id="next-steps">Next steps</h2>

<p>The interactivity of a chat interface is still missing - having a chat box that appears with a shortcut, or being able to post multiple messages in succession instead of calling the shell script everytime.</p>

<p>I'm also starting to get interest in the TUI idea again, now that I recently learnt that my terminal emulator can <a href="https://iterm2.com/documentation-hotkey.html">display terminal windows with a shortcut</a>, and they can be <strong>floating windows</strong>.</p>

<p>I'm also thinking of a web-based approach, with a frontend that resembles IRC/Slack.</p>

<p>Too many ideas in different directions, let's see where this goes.</p>

]]>
</content>

<category term="productivity" />

</entry>
<entry>
<title type="html">Installing Ubuntu on a 2017 MBP - Power management</title>
<link href="https://pjg1.site/mbp-linux-power.html" rel="alternate" type="text/html" title="Installing Ubuntu on a 2017 MBP - Power management" />
<published>2024-09-13T00:00:00+04:00</published>
<updated>2025-01-30T00:00:00+04:00</updated>
<id>https://pjg1.site/mbp-linux-power.html</id>
<content type="html" xml:base="https://pjg1.site/mbp-linux-power.html">
<![CDATA[<p>Continuing from the <a href="/mbp-linux-wifi">previous post</a>, there were two other issues I faced after installing Ubuntu:</p>
<ul>
  <li>The battery drained super quickly despite less usage</li>
  <li>The laptop would get warm and remain warm even when the system was idle</li>
</ul>

<p>I didn't get any warnings regarding battery life or heat issues from the family member I got the laptop from, neither was I using the laptop enough for it to be this warm or use this much power.</p>

<p>Solving this wasn't as straightforward as the fixing WiFi, becuase the searches didn't lead to any one single solution. So I had to debug my way through this somehow.</p>

<h2 id="finding-a-starting-point">Finding a starting point</h2>

<p>I started by finding ways to see the temperature of my laptop, for which I found a package called <code>lm-sensors</code>. Before checking the temps, I ran <code>sensors-detect</code> and selected all of the default options.</p>

<pre><code>$ sudo apt install lm-sensors
$ sudo sensors-detect</code></pre>

<p>When I ran <code>sensors</code> for the first time, there was too much output and it barely made sense. I spent some time deciphering the output, and then came the second problem - I didn't know the ideal temperatures to know which ones were high.</p>

<p>So I tried a different approach. I decided to capture the <code>sensors</code> output twice - once after boot, and one 30mins after that - and compare the two. In those 30mins, I tried to keep the system idle or used it minimally.</p>

<p>This approach worked, as I saw a difference in <code>coretemp-isa-0000</code>, which shows the temperatures of the CPU cores:</p>

<div class="flex">
<figure>

<pre><code>coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +45.0Â°C
...</code></pre>

    <figcaption>After boot</figcaption>
  </figure>
<figure>

<pre><code>coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +55.0Â°C
...</code></pre>

    <figcaption>30mins after boot</figcaption>
  </figure>
</div>

<p><code>Package id 0</code> refers to the temperature of the CPU as a whole, and there is a 10 degree increase in about 30mins, with little to no activity in that duration.</p>

<p>I was off to search again, and I landed with two possible causes of this:</p>
<ol>
  <li>Some process is hogging CPU</li>
  <li>Bad power management of Linux on Macs</li>
</ol>

<p>The first cause got eliminated pretty quickly, as <code>htop</code> didn't show any process with a high CPU usage, and the CPU usage was also fairly low overall. Bad power management was a very common issue reported in online forums, and I knew my machine worked fine on macOS, so this seemed like a valid cause.</p>

<p>One of the tools I came across to enable better power management was <code>powertop</code>, that displays the energy usage of a system and offers default settings for better power management. I enabled the defaults using the <code>--auto-tune</code> flag after installing.</p>

<pre><code>$ sudo apt install powertop
$ sudo powertop --auto-tune</code></pre>

<p>When <code>powertop</code> is run without any flags, it runs in a similar fashion to <code>top</code>, displaying the energy usage and other statistics that update in real time.</p>

<pre><code>The battery reports a discharge rate of:  14.5  W
The energy consumed was :  325  J
The estimated remaining time is 2 hours, 6 minutes

Summary: 123.1 wakeups/second,  0.0 GPU ops/seconds, 0.0 VFS ops/sec and 3.1% CPU use

            Usage       Events/s    Category       Description
        675.2 Âµs/s      46.6        Timer          tick_sched_timer
          0.8 ms/s      21.0        Interrupt      [79] amdgpu
...</code></pre>

<p>Some things stood out here:</p>
<ul>
  <li>The battery discharge rate seemed high</li>
  <li>As a result, the energy consumption was also high</li>
  <li><code>amdgpu</code> was second highest in the energy usage list</li>
</ul>

<p>The appearance of <code>amdgpu</code> seemed something to look into further, and saw that there was an option to disable it all together. I wasn't planning on doing any heavy-duty work on this machine, so it seemed like a reasonable solution if it would help reduce temperatures.</p>

<p>I started following <a href="https://medium.com/codeflu/disabling-discrete-amd-graphics-card-in-linux-5d365738fc97">this tutorial</a>, which first checks if you have two graphics on your system or not.</p>

<pre><code>$ lspci | grep VGA
01:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Baffin [Radeon RX 460/560D / Pro 450/455/460/555/555X/560/560X] (rev ff)</code></pre>

<p>I don't see a second GPU, but <a href="https://support.apple.com/en-us/111947">this machine</a> has two GPUs - an integrated GPU and a discrete/dedicated GPU.</p>

<p>OH WAIT, I found the root cause - the iGPU didn't get detected for whatever reason, and the dGPU is being used as the main graphics driver in its place. The dGPU uses a lot of power, which explains the the high energy usage in <code>powertop</code>, the quick battery drain and the laptop getting warm!</p>

<p>Other people have also <a href="https://github.com/Dunedan/mbp-2016-linux/issues/6">faced the same issue</a> and have documented solutions for it, which I followed along.</p>

<h2 id="enabling-the-igpu">Enabling the iGPU</h2>

<p>The iGPU is not detected thanks to the way Apple's firmware works. If it recognizes that it's booting an OS other than macOS, it will power down some of the hardware, the iGPU being one of them. Thanks Apple!</p>

<p>The TL;DR solution to this is to make the firmware believe that it is booting macOS by running custom code before boot.</p>

<h3 id="step-1-build-the-custom-efi-code">Step 1: Build the custom EFI code</h3>

<p>The custom code is available in the <a href="https://github.com/0xbb/apple_set_os.efi">apple_set_os.efi</a> repository. All I had to do was build the file.</p>

<pre><code>$ git clone https://github.com/0xbb/apple_set_os.efi
$ cd apple_set_os.efi
$ make
cc -I/usr/include/efi -I/usr/include/efi/x86_64 -DGNU_EFI_USE_MS_ABI -fPIC -fshort-wchar -ffreestanding -fno-stack-protector -maccumulate-outgoing-args -Wall -Dx86_64 -Werror -m64 -mno-red-zone   -c -o apple_set_os.o apple_set_os.c
ld -T /usr/lib/elf_x86_64_efi.lds -Bsymbolic -shared -nostdlib -znocombreloc /usr/lib/crt0-efi-x86_64.o -o apple_set_os.so apple_set_os.o /usr/lib/gcc/x86_64-linux-gnu/11/libgcc.a \
/usr/lib/libgnuefi.a
objcopy -j .text -j .sdata -j .data -j .dynamic -j .dynsym -j .rel \
        -j .rela -j .reloc -S --target=efi-app-x86_64 apple_set_os.so apple_set_os.efi
rm apple_set_os.o apple_set_os.so</code></pre>

<h3 id="step-2-move-the-code-to-the-boot-partition">Step 2: move the code to the boot partition</h3>

<p>Next, the code needs to be in a location that is accessible during boot, aka the boot partition. I can put the code in <code>/boot/efi/EFI</code> directly too, but the instructions I was following put this in a sub-directory called <code>custom</code> instead.</p>

<pre><code>$ sudo mkdir /boot/efi/EFI/custom
$ sudo cp apple_set_os.efi /boot/efi/EFI/custom</code></pre>

<h3 id="step-3-ask-grub-to-run-the-code-before-boot">Step 3: Ask GRUB to run the code before boot</h3>

<p>Placing the code in the boot partition alone isn't enough, I needed to add instructions to run the code before boot somewhere. That somewhere is the bootloader configuration, which in this case is GRUB. I added the following lines to a file created for users to add custom configurations: <code>/etc/grub.d/40_custom</code>:</p>

<pre><code>$ cat &lt;&lt;EOF &gt;&gt; /etc/grub.d/40_custom
search --no-floppy --set=root --file /EFI/custom/apple_set_os.efi
chainloader /EFI/custom/apple_set_os.efi
boot
EOF</code></pre>

<p>The GRUB menu display was disabled on my machine. To be able to debug any issues on boot, I made the following changes to <code>/etc/default/grub</code>:</p>

<pre><code># Comment the following line
# GRUB_TIMEOUT_STYLE=hidden

# Change the timeout value
GRUB_TIMEOUT=10

# Uncomment the following lines
GRUB_TERMINAL=console
GRUB_GFXMODE=640x480</code></pre>

<p>Then I ran <code>sudo update-grub</code> to save the changes.</p>

<h3 id="step-4-switch-to-using-the-igpu-on-boot">Step 4: Switch to using the iGPU on boot</h3>

<p>This is done using a shell script called <a href="https://github.com/0xbb/gpu-switch">gpu-switch</a> that writes the required values to an EFI variable to use the iGPU. The changes were applied on the next boot, so I rebooted the machine.</p>

<pre><code>$ git clone https://github.com/0xbb/gpu-switch
$ cd gpu-switch
$ sudo ./gpu-switch -i
$ sudo reboot now</code></pre>

<p>After rebooting, the iGPU now appears in the <code>lspci</code> output!</p>

<pre><code>$ lspci | grep VGA
00:02.0 VGA compatible controller: Intel Corporation HD Graphics 630 (rev 04)
01:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Baffin [Radeon RX 460/560D / Pro 450/455/460/555/555X/560/560X] (rev ff)</code></pre>

<h2 id="disable-dgpu">Disable dGPU</h2>

<p>The dGPU continued to run and warm up the laptop despite the iGPU being detected, so I disabled it with the following commands:</p>

<pre><code>$ echo OFF | sudo tee /sys/kernel/debug/vgaswitcheroo/switch
$ sudo modprobe -r amdgpu</code></pre>

<p>And slowly, my laptop started to cool down. I checked the output of <code>sensors</code> after a while, and the temperatures were MUCH lower than with the dGPU enabled:</p>

<pre><code>coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +42.0Â°C
...</code></pre>

<p>The <code>powertop</code> output also reflected this:</p>

<pre><code>The battery reports a discharge rate of 7.60 W
The energy consumed was 151 J
The estimated remaining time is 8 hours, 35 minutes

Summary: 62.1 wakeups/second,  0.0 GPU ops/seconds, 0.0 VFS ops/sec and 0.7% CPU use

            Usage       Events/s    Category       Description
        100.0%                      Device         Audio codec hwC1D0: ATI
        491.8 Âµs/s      27.3        Timer          tick_sched_timer
...</code></pre>

<p>The battery discharge rate and energy consumption values were lower, battery life became longer and <code>amgdpu</code> no longer appeared at the top of the list!</p>

<p>Lastly, I created a systemd service to disable the dGPU on boot. Thanks to this, my machine remains cool throughout:</p>

<pre><code># disable-dgpu.service
[Unit]
Description=Disable discrete GPU
Before=display-manager.service

[Service]
Type=oneshot
ExecStart=/usr/sbin/modprobe amdgpu
ExecStart=/bin/sh -c 'echo OFF &gt; /sys/kernel/debug/vgaswitcheroo/switch'
ExecStart=/usr/sbin/modprobe -r amdgpu
RemainAfterExit=yes
TimeoutSec=0

[Install]
WantedBy=multi-user.target</code></pre>

<p>I remember being scared when I noticed these issues for the first time. I'd been used to things "just working" on macOS and Windows, and this was the opposite of that. Going from a feeling of fear to slowly gaining the courage to fix stuff has felt great. I think I'm less scared now.</p>

]]>
</content>

<category term="linux" />

<category term="mac" />

</entry>
<entry>
<title type="html">Installing Ubuntu on a 2017 MBP - fixing WiFi troubles</title>
<link href="https://pjg1.site/mbp-linux-wifi.html" rel="alternate" type="text/html" title="Installing Ubuntu on a 2017 MBP - fixing WiFi troubles" />
<published>2024-09-09T00:00:00+04:00</published>
<updated>2025-01-24T00:00:00+04:00</updated>
<id>https://pjg1.site/mbp-linux-wifi.html</id>
<content type="html" xml:base="https://pjg1.site/mbp-linux-wifi.html">
<![CDATA[<p>I'd been curious on what it's like to use Linux as the main OS, and wanted to upgrade from my <a href="/linux-vm-setup">Linux VM setup</a> for a while now. So when a family member was upgrading to a newer machine, I asked for their older machine - a <a href="https://support.apple.com/en-us/111947">2017 MacBook Pro</a> - and decided to install Linux on it.</p>

<p>I chose Ubuntu as that's the distro I'm most comfortable with. After some initial research, I created a bootable installer using <a href="https://ubuntu.com/tutorials/create-a-usb-stick-on-macos#1-overview">this guide</a> and proceeded with the install which went smoothly. Ubuntu also booted much faster compared to macOS on the same machine, so I was feeling good about this.</p>

<p>The feel-good-ness went away fairly quickly, as I started to notice major problems - one of them being that the WiFi wouldn't connect. I could see the list of networks, but it got stuck at the connecting stage after I entered the password. I failed to catch this during the installation process as I skipped the "Connect to WiFi" step.</p>

<p>With the power of the search engine, I came to the conclusion that the WiFi driver was missing. The driver was available via <code>apt</code>, but I needed WiFi for <code>apt</code> to work. The solution I kept coming across was to get a direct connection via Ethernet and then install. Great idea, except my machine doesn't have an Ethernet port.</p>

<p><em>womp womp</em></p>

<p>While I was sure I would need an Ethernet adapter and even planned on buying one, I wrote about this situation in <a href="/weeknotes-32-2024">a weeknote</a> in the slight hopes of finding an alternate solution. And I DID! A fellow Recurser reached out with links I hadn't come across, and <a href="https://www.amirootyet.com/post/how-to-get-wifi-to-work-after#without-internet-connection">one of them</a> provided the required driver as a ZIP file with installation instructions.</p>

<p>And that's when it clicked - I can download the required files on a machine that has WiFi and transfer them via USB. A very obvious idea in hindsight, I'm still wondering why I didn't think of this earlier.</p>

<p>While the file in the post didn't work, I now knew what to search for, and then came across <a href="https://askubuntu.com/questions/730799/installing-firmware-b43-installer-offline/730813#730813">this answer on askubuntu</a>. I downloaded the two files it suggests:</p>
<ul>
  <li>the driver itself - <code>broadcom-wl-5.100.138</code></li>
  <li>a package called <code>b43-fwcutter</code> that extracts the firmware specific to the hardware on my system</li>
</ul>

<p>Once I transferred these files via USB, I ran the remaining steps, and had working WiFi!!!</p>

<pre><code>$ sudo dpkg -i b43-fwcutter_015-9_amd64.deb
$ tar xfvj broadcom-wl-5.100.138.tar.bz2
$ sudo b43-fwcutter -w /lib/firmware broadcom-wl-5.100.138/linux/wl_apsta.o
$ sudo modprobe b43</code></pre>

<p><a href="https://askubuntu.com/questions/626642/how-to-install-broadcom-wireless-drivers-offline/1244412#1244412">Another answer</a> in another thread suggested installing the package via <code>apt</code> once the WiFi works so future updates would be managed by <code>apt</code>, which I thought was a nice idea.</p>

<pre><code>$ sudo apt install b43-firmware-installer</code></pre>

<p>The WiFi signal appeared to be pretty weak in the top bar, however I didn't face any issues while running <code>apt install</code>, interesting.</p>

<p>So problem solved, right? Not reaally. When I logged into the machine a few days later, the WiFi stopped connecting once again.</p>

<p>On further searching, I came across <a href="https://github.com/Dunedan/mbp-2016-linux">mbp-2016-linux</a> - an absolute gem of a resource that mentions what works and doesn't work on Linux for the 2016 and 2017 MacBook Pro's. Particularly, the section about WiFi caught my eye:</p>

<blockquote>
  <p>The MacBook Pro models with Touch Bar come with a <code>Broadcom Limited BCM43602 802.11ac Wireless LAN SoC (rev 02)</code> which is also supported by <code>brcmfmac</code>, but has several issues rendering it unusable, caused by the available firmware.</p>
</blockquote>

<p>My machine has a Touch Bar, let me check what hardware I have:</p>

<pre><code>$ lspci -nn | grep Network
03:00.0 Network controller [0280]: Broadcom Inc. and subsidiaries BCM43602 802.11ac Wireless Lan SoC [14e4:43ba] (rev 02)</code></pre>

<p>Aaand it's the same one. This was confusing since I got WiFi working earlier, that wasn't a dream. So now I needed to figure out what apart from installing the drivers made it work.</p>

<p>Then I remembered a command I ran in my initial attempt at fixing WiFi - copy-pasting commands without knowing what they do. It seemed relevant enough here for some reason, so I ran it again.</p>

<pre><code>$ sudo iwconfig wlp3s0 txpower 10dBm</code></pre>

<p>Reconnected the WiFi, and I had a working connection again! I tried figuring out what led to this confusion, and here's what happened:</p>
<ul>
  <li>The drivers were still missing when I ran the command the first time, so it didn't work on its own.</li>
  <li>Once I had the correct driver, I forgot I ran this command and thought that the drivers alone made it work.</li>
</ul>

<p>This command is suggested as a workaround after installing the driver, and comes from this <a href="https://bugzilla.kernel.org/show_bug.cgi?id=193121">bug report</a>. I'm not really sure why this works (if you do, let me know!), but it does explain why the WiFi signal appears weak after connecting.</p>

<p>Based on replies in the report and issues on mbp-2016-linux, this isn't a guaranteed fix but rather a "your mileage may vary" fix. I been using WiFi with this fix for a few weeks and haven't faced any issues, so looks like it seemed to work fine in my case (yay!)</p>

<p>Okay one last thing before I wrap up this post. This workaround isn't a one-time fix, so I had to type the command and restart the WiFi on each boot to make it work. That got repetitive, so I recently created and enabled a <code>systemd</code> service to run these commands on boot.</p>

<pre><code># mbp-linux-wifi.service
[Unit]
Description=Fixes to make WiFi work on a MBP running Linux
Requires=network.target
After=network.target

[Service]
Type=oneshot
ExecStart=/usr/sbin/iwconfig wlp3s0 txpower 10dBm
ExecStart=/usr/bin/systemctl restart NetworkManager
RemainAfterExit=yes
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target</code></pre>

<p>Earlier iterations of this service failed to run, since it would sometimes try to execute commands before the wireless interface (<code>wlp3s0</code>) is even detected. To solve this, I added the <code>Restart</code> and <code>RestartSec</code> parameters to retry after it fails, and now it connects shortly after booting.</p>

]]>
</content>

<category term="linux" />

<category term="mac" />

<category term="networking" />

</entry>
<entry>
<title type="html">W35 2024: Taking a break from weeknotes</title>
<link href="https://pjg1.site/weeknotes-35-2024.html" rel="alternate" type="text/html" title="W35 2024: Taking a break from weeknotes" />
<published>2024-09-02T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-35-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-35-2024.html">
<![CDATA[<h2 id="wifi-works-on-the-linux-machine">WiFi works on the Linux machine!</h2>

<p>I figured out why the WiFi stopped working again, and went down quite an interesting rabbit hole. I've started working on a draft describing the situation (my first non-weeknote post after ages, eeeee), so I'll save details for that post.</p>

<p>One fun thing I did after WiFi started working was running an nginx server on my machine and accessing the default page from my daily driver. I've tried this on a server in the cloud before, but running it on machine and accessing it from another machine in the same network felt very cool.</p>

<p>This homelab stuff makes me feel like a kid discovering things for the first time, and it feels awesome.</p>

<h2 id="the-website-fixes-continue">The website fixes continue</h2>

<p>After having pushed the initial version of the design, I still had some TODOs left, and even some feedback regarding the colors that I wanted to implement. Since the WiFi problem got solved quickly, I ended up spending the rest of my time on this. Some changes I made:</p>

<ul>
  <li>Changed CSS - increased contrast for dark mode and made the colors more consistent across modes.</li>
  <li>Added an introduction line to the homepage</li>
  <li>Replaced terminal GIFs with <a href="https://asciinema.org">asciinema</a> recordings in posts</li>
  <li>Moved <a href="/hanukkahofdata">Hanukkah of Data</a> from a standalone page to a post</li>
  <li>Minor post edits</li>
  <li>Added alt text to images</li>
  <li>Realized that some images were probably not required while writing alt text, so deleted them</li>
  <li>Added a <a href="/colophon">colophon</a> page</li>
</ul>

<h2 id="terminal-and-text-editor-themes-updated">Terminal and text editor themes updated</h2>

<p>Inspired by the minimal design of my website, I updated the themes for my text editor (Sublime Text atm) and Terminal application (iTerm) to match. On Sublime, I'm using <a href="https://github.com/cowwabanga/sublime-almost-mono/tree/main">sublime-almost-mono</a> while I went for a completely monochrome theme for iTerm. Only white text on a black background (or vice-versa for light mode), no colors.</p>

<h2 id="changing-accountability-mechanisms">Changing accountability mechanisms</h2>

<p>I've somehow managed to post weeknotes for about three months now - sometimes weekly, sometimes bi-weekly. I think it's time to take a break from them though, as it's no longer an effective accountability mechanism for me.</p>

<p>I hoped that writing these would help me get more comfortable publishing on a regular basis, and that would translate into me writing more technical posts focused on a particular topic. While I did get comfortable publishing, I haven't written any technical posts since I started these.</p>

<p>Now that I've been able to generate ideas through other ways (identifying gaps in my knowledge and thinking of resources/projects to learn them), I would rather spend time working on those ideas and writing about them instead of weeknotes. It seems like a natural progression.</p>

<p>Weeknotes are a great format though, and I'm glad I wrote them for as long as I did. I would encourage them to anyone who wants to write but is unsure what to write about, I'm likely going to restart these whenever I'm in a rut.</p>

<p>My new accountability mechanism is something more direct - an accountability buddy! <a href="https://swagnik.netlify.app">Swagnik</a> - a fellow Recurser - and I have started weekly meetings to reflect and discuss our plans for the week. The fact that I'm being held accountable by one single person instead of putting out a message on a chat platform or this blog is making a difference, and probably what I needed.</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W33-34 2024: Minimal blog era</title>
<link href="https://pjg1.site/weeknotes-33-34-2024.html" rel="alternate" type="text/html" title="W33-34 2024: Minimal blog era" />
<published>2024-08-26T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-33-34-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-33-34-2024.html">
<![CDATA[<h2 id="farewell-old-theme-hello-new-theme">Farewell old theme, hello new theme!</h2>

<p>I liked the theme prior to this (I somehow managed to stick with it for a year), but was craving for something simpler, one where I could focus on the writing rather than obsessing over the aesthetics of the site. The inspiration came mainly from two websites â€“ <a href="http://derekkedziora.com">Derek Kedziora's website</a> for the layout and <a href="https://kevincox.ca">Kevin Cox's website</a> for the color scheme.</p>

<p>This was also a good opportunity to simplify the structure of my site, since I started from scratch with a new directory and copied necessary files from the previous directory. I also got around to making some edits to older posts. Still have a combination of major and minor fixes to make, but I think I've reached a point where I can take a break and focus on posting for sometime.</p>

<h2 id="progress-on-installing-linux-on-a-mac">Progress on installing Linux on a Mac</h2>

<p>After publishing <a href="/weeknotes-32-2024">the last weeknote</a>, a fellow Recurser reached out with links on how I could install the WiFi driver without requiring an Ethernet connection. Those links were a good starting point, and I finally found a solution that worked!</p>

<p>However, I've had issues with WiFi again the next few times I ran the machine. I didn't come around to focusing on this much, so that's on my list for the coming week. Might make a post about this once I get it working.</p>

<h2 id="touching-grass">Touching grass</h2>

<p>I've become a homebody since the last few years (thanks 2020), and I finally feel like I'm taking efforts towards going outside more. In the last two weeks, I've stepped out not once, not twice, but THRICE. Big achievement.</p>

<h2 id="linkers-are-tough-to-understand">Linkers are tough to understand</h2>

<p>I finally finished reading the chapter about Linkers from <a href="https://csapp.cs.cmu.edu">CS:APP</a>, and oh my linkers are cool but hard to grasp. I like working on some sort of tiny project to solidify my learning, but I don't have a strong project idea for linkers. So I'm just going to hope this knowledge comes to use when a linker error shows up.</p>

<p>I'm also surprised how I didn't really read physical books much during my time at uni, but I'm willingly reading one (and enjoying it) when there's no academic pressure. Oh, the joys of self-learning.</p>

<h2 id="learning-plan-is-evolving">Learning plan is evolving</h2>

<p>I spent this weekend identifying specific concepts I always wanted to learn, and tried looking for project ideas that will help me learn them. I also looked out for resources that could provide the foundational knowledge.</p>

<p>I'm scared, but also excited.</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W32 2024: Baby steps</title>
<link href="https://pjg1.site/weeknotes-32-2024.html" rel="alternate" type="text/html" title="W32 2024: Baby steps" />
<published>2024-08-13T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-32-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-32-2024.html">
<![CDATA[<h2 id="structured-learning-baby-steps">Structured learning baby steps</h2>

<p>After last week's note, I started thinking about the topics I wanted to learn more about. I was inspired by a fellow Recurser's <a href="https://ravi.fyi">Learning Sabbatical</a>, which had links to books and other useful resources.</p>

<p>I initially thought of making a similar curriculum (and even made one), but decided to focus on one book after getting started. I found out about <a href="http://csapp.cs.cmu.edu">Computer Systems: A Programmer's Perspective</a> a year ago - a fellow Recurser was reading through the book and recommended it, and I paired on one of the labs. It's also listed in the Computer Architecture section of the <a href="https://teachyourselfcs.com">Teach Yourself Computer Science</a> website.</p>

<p>I read the introductory chapter, and I love the book. The language is easy to understand, and I'm finally making sense of terminology I've vaguely heard of but didn't understand properly.</p>

<p>Reading the book in order has proved to be a bad idea in the past, so I'm trying a "choose your adventure" style of reading, where I decide the order of chapters as I go.</p>

<p>I've currently reading a chapter about Linkers, and there's a lot to process. I would ideally like to solidify my knowledge by some combination of code + blog posts, so that's something to think about.</p>

<h2 id="attempt-at-installing-linux-on-a-mac">Attempt at installing Linux on a Mac</h2>

<p>I've been curious on what it would be like to have Linux as the main OS on a machine, as I mostly use macOS and my usage of Linux is limited to Virtual Machine where I mainly use the CLI.</p>

<p>I've also heard about the concept of a homelab and doing cool networking things with it (things like setting up a firewall or using the machine as a router), and figured it would be better to experiment on a dedicated machine if I have the chance.</p>

<p>I recently got my hands on a 2017 MacBook Pro, and decided to install Ubuntu on it. The installation itself was pretty smoothâ€¦however I couldn't connect to the WiFi after rebooting.</p>

<p>Based on some initial searching, I think it didn't install the WiFi driver, which might be causing this.</p>

<p>One of the solutions is to connect an Ethernet cable directly and then install the driverâ€¦but my MacBook has no Ethernet port. I'd need to buy an adapter for that, so if anyone has any other ideas let me know!</p>

<h2 id="tuntcp-on-hold-again">tuntcp: on hold (again)</h2>

<p>This week was supposed to be my last week working on this, but most of my time was spent reading CS:APP, so this got pushed to the side. I revisited the project on the weekend, and had another breakthrough in terms of what the end result should look like. Despite this, I was struggling to write actual code.</p>

<p>Since I am working on stuff I'm more excited about now, this is going on the back burner. I might come back to it whenever I find myself feeling stuck again.</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W30-31 2024: More thinking, less doing</title>
<link href="https://pjg1.site/weeknotes-30-31-2024.html" rel="alternate" type="text/html" title="W30-31 2024: More thinking, less doing" />
<published>2024-08-05T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-30-31-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-30-31-2024.html">
<![CDATA[<h2 id="jamming-to-songs-on-loop">Jamming to songs on loop</h2>

<p>There are phases where I don't actively jam to music, but rather just play it in the background while working. I was in this phase for a while, until I heard two new songs last week and now I can't stop jamming to them:</p>

<ul>
  <li>
    <p><a href="https://www.youtube.com/watch?v=azhwDQsYD8c&amp;pp=ygURcGl5YSBwaXlhIGNhbGxpbmc%3D">Piya Piya Calling</a> by Karpe, Kaifi Khalil and Amanda Delara feat. Quick Style - I've been following Quick Style for a while and have heard a song or two from Coke Studio Pakistan, but I never expected their collaboration to be a song with my name in the title. Such a feel good track.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=a3Ue-LN5B9U&amp;pp=ygUKYWFzYSBrb29kYQ%3D%3D">Aasa Kooda</a> by Sai Abhayankar and Sai Smriti - The chorus and hook step blows my mind every.single.time. The music video is also really beautifully filmed.</p>
  </li>
</ul>

<p>I wanted to add these songs to my playlist, so I spent some time refactoring a script I wrote to download songs locally with their metadata. It was fun looking back at messy code from 2 years ago.</p>

<h2 id="letting-go-of-tuntcp">Letting go of tuntcp</h2>

<p>I think I got what I wanted out of <a href="https://github.com/pjg11/tuntcp">tuntcp</a> a long time back, but perfectionism kept coming in the way, so I kept adding/changing stuff and now I don't know where to stop.</p>

<p>Progress has been slow, and I'm loosing interest. This coming week is the last week I'll work on refactoring the code, after which I'll consider the project done.</p>

<h2 id="reflecting-on-one-year-at-rc">Reflecting on one year at RC</h2>

<p>I got accepted to the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a> in July last year, but my batch started on August 7th, which means its officially been a year since I joined!</p>

<p>The batch felt like the tip of the iceberg in terms of my capability as a programmer. However, I find myself only slightly lower from the tip a year later, unable to go deeper on one/a few topics.</p>

<p>I worked on projects post-batch too, but those where chosen based on what I found fun in that moment, rather than learning things in a structured way. More on this later.</p>

<p>A pleasant surprise came in the form of the opportunity to work as a part-time moderator. Helping maintain a community that has been a subtle support system for the past year feels good (so does having some source of income while I figure how to make this "making stuff online" thing work).</p>

<p>To top it all off, my <a href="/rc07">end-of-batch statement</a> was added to the list of <a href="https://www.recurse.com/who">RC's Testimonials</a> recently, which felt really nice! This might be the first time my blog has been linked somewhere.</p>

<h2 id="planning-ahead">Planning ahead</h2>

<p>The last few weeks were hectic, and with those commitments now over, I'm ready to focus on projects again. Reflecting on my time at RC got me thinking on how I can change my approach to learning. Here are some half-baked ideas to ponder over this week:</p>

<ul>
  <li>
    <p>I make a study curriculum for myself and follow it as much as I can. I've stayed away from structured learning for the longest time, but maybe I'm ready for it now?</p>
  </li>
  <li>
    <p>I try out the things Julia Evans wrote about during her <a href="https://jvns.ca/#recurse-center-2013">2013 batch at RC</a> as a starting point. I've hesitated to do this for two main reasons:</p>

    <ul>
      <li>
        <p>Someone already figured this stuff out 11 years ago, so I feel dumb figuring this stuff out 11 years later, even though the concepts are new to me. I know its a silly reason, which is why I'd like to get over this.</p>
      </li>
      <li>
        <p>If I end up writing about it, I fear my posts will end up looking like clones of hers, particularly if I don't have anything new to add.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Starting some kind of daily writing practice, where I attempt at writing some blog-post like every day privately. Ideas include trying to explain a concept or a passage of a book and writing setup guides for things I use commonly. Some of these might turn into actual blog posts someday.</p>
  </li>
  <li>
    <p>Have some <a href="https://www.recurse.com/manual#sub-sec-self-directives">finger work projects</a> to fall back on. Challenges like <a href="http://protohackers.com">Protohackers</a> help keep the programming momentum going as I'm presented with a set of requirements, and I only have to focus on the implementation.</p>
  </li>
</ul>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W28-29 2024: Setting priorities</title>
<link href="https://pjg1.site/weeknotes-28-29-2024.html" rel="alternate" type="text/html" title="W28-29 2024: Setting priorities" />
<published>2024-07-22T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-28-29-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-28-29-2024.html">
<![CDATA[<h2 id="working-on-tuntcp">Working on tuntcp</h2>

<p><a href="https://github.com/pjg11/tuntcp">tuntcp</a> is a project I started during my batch at the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a>. It started off great, but I lost interest by the time the batch ended.</p>

<p>Since then, completing this project has been at the back of my mind. I tend to lose interest whenever I pick it up again - I either get too ambitious about what features to include, or get stuck on how to structure the code.</p>

<p>I made some progress about 2 months ago, but the code was so hackily put together that I myself get confused if I look at it just a few days later.</p>

<p>Over the last two weeks though, I think I made some sort of breakthrough. I've been working on this locally, so nothing to show publicly yet.</p>

<h2 id="completed-raytracingweekend">Completed raytracingweekend</h2>

<p><a href="https://github.com/pjg11/raytracingweekend">raytracingweekend</a> has been complete for a while, but I kept the README brief at the time in the hopes of writing a longer blog post about it.</p>

<p>The motivation to write a post never came, so I decided add some more details to the README and call it a day.</p>

<p>I've been avoiding doing this for months. And when I finally started, it took me under an hour to complete. Fun</p>

<h2 id="re-starting-protohackers">Re-starting Protohackers</h2>

<p>I heard about Protohackers from a fellow Recurser, and attempted a few challenges at the start of this year. As happens with projects, I start them with full interest, but lose momentum at some point and leave it.</p>

<p>A few weeks/months later the interest comes back, and I make an attempt to finish it with any new knowledge I've learnt since then.</p>

<p>That happened this weekend with Protohackers. I started solving them in Go, and completed 3 challenges! The code is up <a href="https://github.com/pjg11/protohackers">here</a>.</p>

<h2 id="reflections">Reflections</h2>

<p>Surprisingly, I made some pretty good prioritizing decisions over the last two weeks.</p>

<p>Since my attention is currently divided by multiple commitments, I decided to work on/finish existing projects, as it's easier to get started and keep the momentum going.</p>

<p>I joined a few RC study groups, but found myself skipping most of them. Rather than trying to fit them in, I chose to stop attending all of them except one, that I host.</p>

<p>I also switched to posting a weeknote once every two weeks, to account for the slower pace for the meantime.</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W26-27 2024: Very distracted</title>
<link href="https://pjg1.site/weeknotes-26-27-2024.html" rel="alternate" type="text/html" title="W26-27 2024: Very distracted" />
<published>2024-07-08T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-26-27-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-26-27-2024.html">
<![CDATA[<p>Both these weeks were busy with work stuff, some changes in routine (aka me trying to keep up with a show I really like) and lots of procrastination, which meant little to no progress on personal projects.</p>

<p>I've been trying to readjust my routine to start making progress, which hasn't been successful so far, so here's a short note summarizing the last two weeks.</p>

<h2 id="making-progress-on-a-c-project">Making progress on a C project</h2>

<p>I revisited a project I mentioned in a <a href="https://pjg1.site/weeknotes-24-2024#figuring-out-how-to-read-a-file-in-the-ext4-filesystem">previous weeknote</a> and attemtpted to port the rest of code in the post. It kind of works, but some cases don't, and debugging those cases is the next step.</p>

<h2 id="starting-a-personal-wiki">Starting a personal wiki?</h2>

<p>I was recently helping out refactor the internal wiki at the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a>, and that inspired me to start my own. However, having separate wiki pages started to create confusion with the dev journal system, so I'm thinking on how to incorporate this with the journal itself.</p>

<h2 id="lots-of-thinking">Lots of thinking</h2>

<p>Since I was finding it tough to get started on something, I decided to spend time planning in the hopes that something would click. I'm in the process of breaking down projects into smaller, manageable tasks, and priortizing which ones to focus on.</p>

<h2 id="browserengineering">browser.engineering</h2>

<p>Riding the wave of the start of a new batch and new events at RC, I decided to join a study group working through the <a href="https://browser.engineering">Web Browser Engineering</a> book. I had this book saved in my "someday" list for a while, and this study group seemed like a good opportunity.</p>

<p>I'm working through the Chapter 1 exercises, and it seems interesting so far. Not sure if I'll work through the entire book though.</p>

<h2 id="reflections">Reflections</h2>

<p>The new additions to my routine meant that my existing routine isn't working. This was also reflective in my dev journal, where I have very few notes from these two weeks.</p>

<p>Attempts at readjusting my routine shall continue.</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W25 2024: You win some, you lose some</title>
<link href="https://pjg1.site/weeknotes-25-2024.html" rel="alternate" type="text/html" title="W25 2024: You win some, you lose some" />
<published>2024-06-24T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-25-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-25-2024.html">
<![CDATA[<h2 id="smol-pr-victory">Smol PR victory</h2>

<p>I submitted <a href="https://github.com/robinovitch61/wander/pull/138">a PR</a> for wander, which got merged! I came across a similar bug recently, so the fix in terms of code was simple.</p>

<p>Understanding the codebase was the tougher part, which the project's maintainer (and fellow Recurser) very kindly walked me through in a chat.</p>

<h2 id="blog-post-struggles">Blog post struggles</h2>

<p>I thought publishing these weeknotes would make writing other posts easier. I was wrong.</p>

<p>I don't feel like pushing through it either, I've sort of lost interest. Maybe having the code in a repo + a README is sufficient? Let's see.</p>

<h2 id="ctf-adventures">CTF adventures</h2>

<p>A group of Recursers decided to participate in <a href="https://capturetheflag.withgoogle.com">Google CTF</a> over the weekend! I hadn't played a CTF in a while, so my skills were rusty (they were basic to begin with). And Google CTF in particular is tough, where even the "easy" challenges are not that easy.</p>

<p>I fell into a rabbit hole with one of the challenges, and that path wasn't helpful in solving it AT ALL :/</p>

<p>Regardless, the rabbit hole was really interesting. I started looking into terminal escape sequences, a combination of characters that allow for functionality in the terminal - printing text in different colors being an example.</p>

<p>I was particularly interested in terminal injection - using certain sequences in a way that lets me execute shell commands. Turns out that wasn't possible (the code was escaping the escape sequences), but I left with a better understanding of how it works.</p>

<p>Here are some links I found helpful:</p>
<ul>
  <li><a href="https://gpanders.com/blog/state-of-the-terminal/">State of the Terminal</a></li>
  <li><a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html">XTerm Control Sequences</a></li>
  <li><a href="https://www.youtube.com/watch?v=3T2Al3jdY38&amp;pp=ygULU1RPSyBkZWZjb24%3D">Weaponizing Plaintext ANSI Escape Sequences as a Forensic Nightmare</a></li>
  <li><a href="https://www.youtube.com/watch?v=iIHw0KWgzAs&amp;pp=ygUeZGF2aWQgbGVhZGJldHRlciBibHVlIGhhdCAyMDIz">Houdini of the Terminal</a></li>
</ul>

<h2 id="reflections">Reflections</h2>

<p>Did I work in a more structured way than usual? Not really, but I had one big win: I didn't fall down the "maintaining my website" rabbit hole! I decided on this earlier in the week, and I'm happy I'm stuck to it.</p>

<p>I've also realized that there are times I'm learning things that don't feel useful in the moment, but help me in another situation.</p>

<p>For example, the bug fix for wander was something I came across in one of my personal projects. I dropped that project mid-way, so it wasn't particularly useful there. However, having worked on that code helped me figure out the exact fix in this case, which was cool.</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W24 2024: Slow but steady progress</title>
<link href="https://pjg1.site/weeknotes-24-2024.html" rel="alternate" type="text/html" title="W24 2024: Slow but steady progress" />
<published>2024-06-17T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-24-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-24-2024.html">
<![CDATA[<h2 id="impossible-stuff-day">Impossible Stuff Day</h2>

<p>Tuesday was <a href="/rc05#fn:1">Impossible Stuff Day</a> at the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a>, so I took this as an opportunity to do something I'd been putting off for a while - cleaning the dust underneath the keys of my laptop by removing them.</p>

<p>It took about half a day - I started at around 5PM and ended at 1:30AM (with breaks of course). There was A LOT of dust in there.</p>

<p>My laptop was switched off the entire time, which meant I got lots of off-screen time aside from the occasional phone checks. I was TIRED by the time I was done, and the exhaustion continued into Wednesday as well.</p>

<h2 id="completing-cool-project-idea">Completing cool project idea</h2>

<p>I initially hoped to publish the code and write a post about it this week. However, getting the code to a publishable state took time as I basically ended up having to re-arrange and/or re-write parts of the code.</p>

<p>I did manage to fix some buggy parts of the code in the process, which was nice. I started drafting a post yesterday, and it should be up sometime this week.</p>

<h2 id="figuring-out-how-to-read-a-file-in-the-ext4-filesystem">Figuring out how to read a file in the ext4 filesystem</h2>

<p>I was reading an article from my Reading List - <a href="https://fasterthanli.me/series/reading-files-the-hard-way/part-3">Reading files the hard way - Part 3</a>. I'd learnt about aspects of the ext4 filesystem from different articles, but didn't understand how they all connected together.</p>

<p>The latter half of the post explains this step by step through Rust code. Simply reading along was getting a little boring, so I decided it to port the code to C as I read the post!</p>

<p>I haven't completed the post yet, but I progressed enough to understand how it all works! Also somewhat figured how to read Rust code along the way, so that was a plus!</p>

<h2 id="slow-progress-on-mission-revamp">Slow progress on mission revamp</h2>

<p>I fell down a theme rabbit hole, and came out of it liking my original theme once again! There were some elements of the redesign I liked though, so I'm slowly adding those changes in.</p>

<p>I didn't even get to the static site generator side of things, so I'll stick to my current setup for the meantime.</p>

<h2 id="reflections">Reflections</h2>

<p>I generally tend to figure out what to do as the week progresses. This works for most things, except ones that take more effort to get started or aren't as fun.</p>

<p>As I have a bunch of those kind of tasks right now (writing a blog post, completing mission revamp, finishing up projects that are close to done), I'm going to try being more structured this week and see how that goes.</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W23 2024: Ebb and flow</title>
<link href="https://pjg1.site/weeknotes-23-2024.html" rel="alternate" type="text/html" title="W23 2024: Ebb and flow" />
<published>2024-06-10T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-23-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-23-2024.html">
<![CDATA[<p>Last week, I felt like I gained superpowers with regards to making progress. This week, the superpowers vanished, poof.</p>

<h2 id="mission-website-revamp">Mission website revamp</h2>

<p>I decided to switch to another domain provider as my previous one was charging a higher price for renewal (happening a month from now). I initiated a domain transfer, which requires getting an <a href="https://en.wikipedia.org/wiki/Auth-Code">Auth Code</a>. It took 3 DAYS just to receive that.</p>

<p>I had a solid mental plan for this revamp, and this delay derailed that plan. This waiting period somehow forced me into waiting mode - I started avoiding other projects, even ones that weren't related to the website. If I somehow got started on something, I couldn't make progress and gave up.</p>

<p>I received the Auth Code on Friday and proceeded with the transfer, which should complete sometime in the coming week! With this the waiting mode faded away, and I picked up pace over the weekend.</p>

<p>I moved the hosting of this site from Netlify to <a href="https://github.com/pjg11/website">GitHub Pages</a>, and have been experimenting with a different theme + static site generator.</p>

<h2 id="a-cool-project-idea">A cool project idea?</h2>

<p>For some reason, my attention went to my Reading List this weekend. I have some pretty cool articles saved there, that I've either half-read or not touched at all.</p>

<p>I was thinking of ways how I could make the process more enjoyable, and I was reminded of times I used to annotate and write notes in the margins of textbooks in school.</p>

<p>Bringing that feeling online sounded like fun, and I started a <a href="https://web.hypothes.is">hypothes.is</a> account to do so.</p>

<p>Hypothesis works really well, however it didn't match the exact experience of writing notes in the margins. This spawned off a tiny project, which I'll write about in a separate post. I do have some reflections about it to share here though.</p>

<p>Working on this project was a different experience than other stuff I've written. I was coding mainly in JavaScript (a language I don't use very often) and I wasn't too clear on the final result of the project. This sounds like the perfect recipe for failure - I change the scope too many times and eventually give up - and I was close.</p>

<p>However, I managed to take a step back, decide on a set of features, and posted a demo within the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a> community as a checkpoint. RC has been a supportive place to share stuff in an incomplete or not polished state, and I'd eventually like to get more comfortable doing so here on this blog.</p>

<h2 id="reflections">Reflections</h2>

<p>As I'm writing this, I'm realizing this was actually a pretty standard week, but it felt a lot worse because I kept comparing with how the previous week went. I did do things, just not in the way I expected. And that's okay.</p>

<p>On the other hand, managing to pull myself out of the "give up" stage of a project is not something I would usually do, so maybe I still have some superpowers left?</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">W22 2024: Starting weeknotes</title>
<link href="https://pjg1.site/weeknotes-22-2024.html" rel="alternate" type="text/html" title="W22 2024: Starting weeknotes" />
<published>2024-06-03T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/weeknotes-22-2024.html</id>
<content type="html" xml:base="https://pjg1.site/weeknotes-22-2024.html">
<![CDATA[<p>I've been making progress on multiple personal projects lately, so I'm using that momentum to re-establish my blogging habit by sharing weekly updates here.</p>

<h2 id="gossip-glomers">Gossip Glomers</h2>

<p>I've been writing more Go code lately, and there has coincidentally been interest around Go-related projects from folks at the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a>. I heard some gossip (pun totally intended) about Recursers working on <a href="https://fly.io/dist-sys">Gossip Glomers</a>, a series of distributed systems challenges.</p>

<p>I joined while they were a few challenges in, but my lack of familiarity with the concepts meant I couldn't contribute much, so I caught up on previous challenges over the rest of the week, <a href="https://github.com/pjg11/gossip-glomers">code here</a>.</p>

<p>I liked the challenges I solved so far, looking forward to further group programming sessions!</p>

<h2 id="virtual-rc-cli">Virtual RC CLI</h2>

<p>A project I started working on two weeks ago (also in Go) was to build a CLI tool for <a href="https://www.recurse.com/virtual-rc">Virtual RC</a>, the online representation of RC's physical space. This week, I worked on one of the main features I was interested in using their API.</p>

<p>The API is built with ActionCable, and the workflow looks something like this:</p>
<ul>
  <li>Connect to WebSocket endpoint</li>
  <li>Subscribe to channel</li>
  <li>Get initial state of Virtual RC as JSON</li>
  <li>Receive subsequent smaller updates to state as JSON</li>
  <li>Unsubscribe to channel and close connection when done.</li>
</ul>

<p>The workflow would work best with a program that runs as a server or daemon - it starts a connection once, maintains the state via the initial message and further updates, and displays the updated state.</p>

<p>However, since my tool is a CLI that terminates after the command is executed, I had to start and stop a connection for each execution.</p>

<p>This sounds fine in theory, but it lead to an initial lag of 3-4 seconds for each run in reality. With the help of some pairing, we came to the conclusion that the lag comes from the API taking time to sending the initial state message - a big chunk of JSON data.</p>

<p>Since then, I've been re-considering the direction this project should take. One solution would be to have the connection run as a background process doing the work of receiving updates and maintaining state. Then the CLI tool has to get the state from the process, which would be a lot faster.</p>

<p>This doesn't feel very different from having a Virtual RC tab open in the browser though, so is all this effort even worth it? I'm not sure, so I've decided to let the ideas simmer for a while and prioritize other projects in the meantime.</p>

<h2 id="revamping-this-website">Revamping this website?</h2>

<p>Speaking of prioritizing, I successfully managed to bring my focus to this little corner of the internet. It started off as a theme re-design (as it always does), except this time, it is slowly turning into a complete overhaul of all aspects of my personal website.</p>

<p>This is still very much in-progress, I'll have more details to share next week.</p>

<h2 id="reflections">Reflections</h2>

<p>I made progress on many different projects this week, which is a rarity, and it made me wonder where all this momentum came from.</p>

<p>I think I can trace down to the combination of starting a <a href="https://stackoverflow.blog/2024/05/22/you-should-keep-a-developer-s-journal/">dev journal</a> about two weeks ago and good luck - finding the linked blog post at a time when it resonated with me the most.</p>

<p>Writing stuff down lead to more ideas, and writing those ideas down made it more likely that I'd give them a shot instead of getting overwhelmed and giving up. This is the positive feedback loop I needed, as I'm more open to trying out new ideas than I've been in a very long time.</p>

<p>For example, I've been writing and posting checkins in the Recurse Center community for months, but I wouldn't have considered posting them publicly earlier. But here I am now, doing exactly that in the form of these weeknotes.</p>

]]>
</content>

<category term="weeknote" />

</entry>
<entry>
<title type="html">Hanukkah of Data</title>
<link href="https://pjg1.site/hanukkahofdata.html" rel="alternate" type="text/html" title="Hanukkah of Data" />
<published>2024-04-10T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/hanukkahofdata.html</id>
<content type="html" xml:base="https://pjg1.site/hanukkahofdata.html">
<![CDATA[<p><a href="https://hanukkah.bluebird.sh/5784/">Hanukkah of Data</a> is a set of data puzzles based around a storyline, released in 2022. I found out about this a few months past its release, and recently solved them using SQLite.</p>

<h2 id="0-noahs-market">0. Noah's Market</h2>

<p>The story revolves around finding the missing rug of Noah, who's the owner of a store called Noah's Market in Manhattan. The dataset is a backup of the Noah's Market database, created by Alex in 2017. However, it is password protected.</p>

<blockquote>
  <p>"Alex set up the backups to be password-protected. I can never remember the password itself, but it's just the year in the Hebrew calendar when Alex set up the database."</p>
</blockquote>

<p>Some searching later, I found the password - 5777. With the dataset unlocked, I had a look at the tables and their schema.</p>

<pre><code>sqlite&gt; .open noahs.sqlite
sqlite&gt; .schema
CREATE TABLE products (sku text,desc text,wholesale_cost decimal(10,2),dims_cm array);
CREATE TABLE customers (customerid integer,name text,address text,citystatezip text,birthdate text,phone text,timezone text,lat decimal(10,5),long decimal(10,5));
CREATE TABLE orders (orderid text,customerid text,ordered timestamp,shipped timestamp,total decimal(10,2),items array);
CREATE TABLE orders_items (orderid integer,sku text,qty integer,unit_price decimal(10,2));</code></pre>

<p>I found myself having to join these tables pretty often, so I saved the output as a view for easy reference. I also learnt about <code>NATURAL JOIN</code> as a shorthand to <code>JOIN ON primarykey = foreignkey</code>, making the command shorter (and it also removes duplicate columns!)</p>

<pre><code>CREATE VIEW IF NOT EXISTS noahsrug AS
SELECT * FROM customers
NATURAL JOIN orders
NATURAL JOIN orders_items
NATURAL JOIN products;</code></pre>

<p>The next set of puzzles have a similar structure - the description contains clues to a customer of Noah's Market who may have the rug, and submitting the customer's phone number solves the puzzle.</p>

<h2 id="1-the-investigator">1. The Investigator</h2>

<p>This person's phone number is the numeric representation of their last name. So each letter corresponds to the <a href="https://en.wikipedia.org/wiki/File:Telephone_keys.JPG#/media/File:Telephone_keys.JPG">digits printed on the phone buttons</a> - "abc" to 2, "def" to 3, and so on.</p>

<p>While the solution seemed easy conceptually, there's no existing SQLite function that maps characters to numbers, making this a lot harder than I expected. This led me to look into <a href="https://sqlite.org/loadext.html">custom extensions</a>, and I used the <a href="https://www.sqlite.org/src/file/ext/misc/rot13.c">rot13 extension</a> code as the basis for mine, <code>phonenum</code>.</p>

<details>
  <summary><code>phonenum.c</code></summary>

<pre><code>#include "sqlite3ext.h"
SQLITE_EXTENSION_INIT1
#include &lt;assert.h&gt;
#include &lt;ctype.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

static void phonenum(sqlite3_context *context, int argc, sqlite3_value **argv) {
  assert(argc == 1);

  if (sqlite3_value_type(argv[0]) == SQLITE_NULL)
    return;

  const unsigned char *in = sqlite3_value_text(argv[0]);
  int len = sqlite3_value_bytes(argv[0]), i = 0;
  unsigned char out[len];

  for (; i &lt; len; i++) {
    unsigned char c = tolower(in[i]);
    switch (c) {
    case 'a':
    case 'b':
    case 'c':
      c = '2';
      break;
    case 'd':
    case 'e':
    case 'f':
      c = '3';
      break;
    case 'g':
    case 'h':
    case 'i':
      c = '4';
      break;
    case 'j':
    case 'k':
    case 'l':
      c = '5';
      break;
    case 'm':
    case 'n':
    case 'o':
      c = '6';
      break;
    case 'p':
    case 'q':
    case 'r':
    case 's':
      c = '7';
      break;
    case 't':
    case 'u':
    case 'v':
      c = '8';
      break;
    case 'w':
    case 'x':
    case 'y':
    case 'z':
      c = '9';
      break;
    }
    out[i] = c;
  }
  out[i] = 0;
  sqlite3_result_text(context, (char *)out, i, SQLITE_TRANSIENT);
}

#ifdef _WIN32
__declspec(dllexport)
#endif
int sqlite3_phonenum_init(sqlite3 *db, char **pzErrMsg, const sqlite3_api_routines *pApi) {
  int rc = SQLITE_OK;
  SQLITE_EXTENSION_INIT2(pApi);

  (void)pzErrMsg; /* Unused parameter */

  if (rc == SQLITE_OK) {
    rc = sqlite3_create_function(db, "phonenum", 1,
                                SQLITE_UTF8 | SQLITE_INNOCUOUS | SQLITE_DETERMINISTIC,
                                0, phonenum, 0, 0);
  }

  return rc;
}</code></pre>

</details>

<p>A few hours of debugging later, I compiled the code as a shared object and loaded it in SQLite.</p>

<pre><code>sqlite&gt; .shell gcc -fPIC -I/opt/homebrew/opt/sqlite/include -shared -o phonenum phonenum.c
sqlite&gt; .load phonenum</code></pre>

<p>I used a combination of <code>substr()</code> and <code>instr()</code> to get the last name from the full name, before passing it to <code>phonenum()</code>.</p>

<pre><code>sqlite&gt; SELECT name, phone
   ...&gt; FROM customers
   ...&gt; WHERE phonenum(substr(name, instr(name, ' ') + 1)) == replace(phone, '-', '')
   ...&gt; LIMIT 1;
Sam Tannenbaum|826-636-2286</code></pre>

<h2 id="2-the-contractor">2. The Contractor</h2>

<p>Theis person's initials are 'JP' and they shopped from Noah's Market in 2017, while they were tasked with the cleaning of the rug. One of the purchased items mentioned were cleaning supplies, so I first looked for related items in the <code>products</code> database.</p>

<pre><code>sqlite&gt; SELECT sku, desc FROM products WHERE desc LIKE '%rug %';
HOM2761|Rug Cleaner</code></pre>

<pre><code>sqlite&gt; SELECT name, phone
   ...&gt; FROM noahsrug
   ...&gt; WHERE name LIKE 'J% P%'
   ...&gt;   AND strftime('%Y', shipped) == '2017'
   ...&gt;   AND desc == 'Rug Cleaner'
   ...&gt; LIMIT 1;
Joshua Peterson|332-274-4185</code></pre>

<h2 id="3-the-neighbor">3. The Neighbor</h2>

<p>The person lives in the the same neighborhood as the Contractor, and is a Cancer born in the Year of the Rabbit.
First, I got the name of the neighborhood using the phone number I found earlier.</p>

<pre><code>sqlite&gt; SELECT citystatezip FROM customers WHERE phone == '332-274-4185';
Jamaica, NY 11435</code></pre>

<p>Then coming to the clues about the person's birthdate. The astrological sign, Cancer, gives us a range for the day and month (21 June â€“ 22 July), while the Chinese zodiac, <a href="https://chinesenewyear.net/zodiac/rabbit/">Rabbit</a>, gives a range for the birth year.
I checked for the earliest and latest birth years in the dataset, to add all valid birth years in the <code>WHERE</code> clause.</p>

<pre><code>sqlite&gt; SELECT min(birthdate), max(birthdate) FROM customers;
1935-01-29|2001-12-31</code></pre>

<pre><code>sqlite&gt; SELECT name, phone
   ...&gt; FROM customers
   ...&gt; WHERE ((strftime('%d', birthdate) &gt;= '21'
   ...&gt;         AND strftime('%m', birthdate) == '06')
   ...&gt;        OR (strftime('%d', birthdate) &lt;= '22'
   ...&gt;            AND strftime('%m', birthdate) == '07'))
   ...&gt;   AND citystatezip == 'Jamaica, NY 11435'
   ...&gt;   AND strftime('%Y', birthdate) IN ('1939', '1951', '1963',
   ...&gt;                                     '1975', '1987', '1999');
Robert Morton|917-288-9635</code></pre>

<h2 id="4-the-early-bird">4. The Early Bird</h2>

<p>This person met The Neighbor at 5AM with pastries they just bought from Noah's. This means that the order happened somewhere close to 5AM. I filtered for pastry orders by checking if the <code>sku</code> starts with <code>BKY</code> for bakery items.</p>

<p>The database also contains the <code>ordered</code> and <code>shipped</code> timestamps for each order. Since they were purchased from the store (as opposed to being ordered from home for example), the ordered and shipped time would be the same for this order.</p>

<pre><code>sqlite&gt; SELECT name, phone
   ...&gt; FROM noahsrug
   ...&gt; WHERE strftime('%H', shipped) == '04'
   ...&gt;   AND sku LIKE 'BKY%'
   ...&gt;   AND ordered == shipped
   ...&gt; LIMIT 1;
Renee Harmon|607-231-3605</code></pre>

<h2 id="5-the-cat-lady">5. The Cat Lady</h2>

<p>The Early Bird decided to give away the rug during their "Marie Kondo phase", which would be sometime after 2019. They gave it to a person from Staten Island, who was wearing a "Noah's Market" sweatshirt and owned 10 or 11 old cats.</p>

<p>This means they purchased lots of cat food from Noah's. There are products for Adult and Senior cats, so I filtered for those, and searched for the person with the most cat related orders.</p>

<pre><code>sqlite&gt; SELECT name, phone
   ...&gt; FROM noahsrug
   ...&gt; WHERE citystatezip LIKE 'Staten Island%'
   ...&gt;   AND desc REGEXP '.*(Adult|Senior) Cat.*'
   ...&gt;   AND strftime('%Y', shipped) &gt;= 2019
   ...&gt; GROUP BY name
   ...&gt; ORDER BY count(*) DESC
   ...&gt; LIMIT 1;
Nicole Wilson|631-507-6048</code></pre>

<p>The sweatshirt clue turned out to be a red herring, which I did spend quite some time on. In hindsight, focusing on the puzzle's title would have helped save some time.</p>

<p>The speedrun description of this puzzle didn't have the Staten Island clue, so I removed it andâ€¦the solution didn't work. After some attempts at trying different queries and nothing really working, I looked solutions online. Turns out, I misunderstood the phrase "most cat related orders".</p>

<p>My solution finds the maximum number of ordersâ€¦but doesn't consider the "quantity" of each order. Since the person owns 10 or 11 cats, they would buy 10 or 11 quantities of the same item! Knowing this made the solution much simpler than the original, and worked for both the original puzzle and the speedrun.</p>

<pre><code>sqlite&gt; SELECT name, phone
   ...&gt; FROM noahsrug
   ...&gt; WHERE desc REGEXP '.*(Adult|Senior) Cat.*'
   ...&gt;   AND qty &gt;= 10
   ...&gt; LIMIT 1;
Nicole Wilson|631-507-6048</code></pre>

<h2 id="6-the-bargain-hunter">6. The Bargain Hunter</h2>

<p>I was stuck on this for a long time, and I ended up slightly peeking at another solution to get unstuck. The solution comes from this one line in the description:</p>

<blockquote>
  <p>In fact I like to tease her that Noah actually loses money whenever she comes in the store.</p>
</blockquote>

<p>This line always felt relevant from the beginning, but I didn't know what to look for. Now that I was sure, I focused my attention on the columns in the database. Two of them stood out, <code>wholesale_cost</code> (the price Noah's bought the item for) and <code>unit_price</code> (the price Noah's sold the item for).</p>

<p>If the unit price is lesser than the wholesale cost, that would mean a loss. The person having the maximum transactions resulting in loss would be The Bargain Hunter.</p>

<pre><code>sqlite&gt; SELECT name, phone
   ...&gt; FROM noahsrug
   ...&gt; WHERE unit_price &lt; wholesale_cost
   ...&gt; GROUP BY name
   ...&gt; ORDER BY count(*) DESC
   ...&gt; LIMIT 1;
Sherri Long|585-838-9161</code></pre>

<h2 id="7-the-meet-cute">7. The Meet Cute</h2>

<p>This person was at Noah's on the same day as the Bargain Hunter, and purchased the same item in a color that the Bargain Hunter wanted.</p>

<p>There are no clues on what day this happened, so I started with finding color related items purchased by the Bargain Hunter. All products with color variations have the <code>sku</code> starting with <code>COL</code>. Ordered and shipped time will also be the same here.</p>

<pre><code>sqlite&gt; SELECT orderid, desc
   ...&gt; FROM noahsrug
   ...&gt; WHERE sku LIKE 'COL%'
   ...&gt;   AND name == 'Sherri Long'
   ...&gt;   AND ordered == shipped;
70503|Noah's Poster (azure)
124821|Noah's Action Figure (green)</code></pre>

<p>Using these queries, I checked for orders around the same time for the same item manually, by comparing the substring of the product names (without the color) and checking for the last 5 orders before the Bargain Hunter's order.</p>

<pre><code>sqlite&gt; SELECT name, phone, orderid
   ...&gt; FROM noahsrug
   ...&gt; WHERE ordered == shipped
   ...&gt;   AND desc LIKE 'Noah''s Poster%'
   ...&gt;   AND orderid BETWEEN 70498 AND 70502;
Carlos Myers|838-335-7157|70502</code></pre>

<p>The first one returned an answer, and it happens to be the previous order ID! However, just to be sure I checked for the Action Figure query too, which returned no result.</p>

<pre><code>sqlite&gt; SELECT name, phone, shipped
   ...&gt; FROM noahsrug
   ...&gt; WHERE ordered == shipped
   ...&gt;   AND desc LIKE 'Noah''s Action Figure%';
   ...&gt;   AND orderid BETWEEN 124816 AND 124820;</code></pre>

<p>However, this solution didn't work for the speedrun for two reasons.</p>

<ol>
  <li>
    <p>There are items apart from the ones starting with <code>COL</code> that have color variations. The format of the items is <code>Item name (color)</code>, so I changed the <code>WHERE</code> condition to look for a bracket in the description instead.</p>
  </li>
  <li>
    <p>The speedrun dataset returned more queries than the two I got here, so manually checking each order was time-consuming. I looked for a way to automate the checking, which I did using the <code>WITH</code> clause to save the initial orders as a temporary table. The table is then joined with <code>noahsrug</code>, and the <code>WHERE</code> conditions become <code>JOIN</code> conditions.</p>
  </li>
</ol>

<pre><code>sqlite&gt; WITH a AS
   ...&gt;   (SELECT orderid, desc
   ...&gt;    FROM noahsrug
   ...&gt;    WHERE desc LIKE '%(%'
   ...&gt;      AND name == 'Sherri Long'
   ...&gt;      AND ordered == shipped)
   ...&gt; SELECT b.name, b.phone
   ...&gt; FROM a
   ...&gt; JOIN noahsrug AS b
   ...&gt;   ON substr(a.desc, 1, instr(a.desc, '(')) == substr(b.desc, 1, instr(b.desc, '('))
   ...&gt;  AND b.orderid BETWEEN a.orderid - 5 AND a.orderid - 1;
Carlos Myers|838-335-7157</code></pre>

<h2 id="8-the-collector">8. The Collector</h2>

<p>Ending on an easier note after some tough puzzle. This person has a set of all Noah's collectibles. These are items that contain "Noah's" in their name, or more precisely, the <code>sku</code> starts with <code>COL</code> - short for collectibles. Finding the person with the maximum collectibles purchases solves this puzzle.</p>

<pre><code>sqlite&gt; SELECT name, phone
   ...&gt; FROM noahsrug
   ...&gt; WHERE sku LIKE 'COL%'
   ...&gt; GROUP BY name
   ...&gt; ORDER BY count(*) DESC
   ...&gt; LIMIT 1;
James Smith|212-547-3518</code></pre>

<h2 id="speedrun-mode">Speedrun Mode</h2>

<p>Completing all the puzzles unlocks the speedrun mode, which lets you attempt the same puzzles with a different dataset and slightly different clues. I also got the idea of solving all puzzles at once while reading <a href="https://ellakaye.rbind.io/posts/2023-12-07_hanukkah-of-data-5784">Ella Kaye's writeup</a> and wanted to do something similar for SQLite.</p>

<p>I considered writing a Python script, but then remembered that I can execute multiple queries written in a SQL file from SQLite directly. That seemed about right, so I started by adding all of the solution queries to <code>noahs-speedrun.sql</code>.</p>

<p>I saved each solution query to a temporary view, so that I could reference them in puzzles where information from a previous puzzle is required. Lastly, I added <code>SELECT</code> statements which display all of the solutions together.</p>

<details>
  <summary><code>noahsrug-speedrun.sql</code></summary>

<pre><code>.load phonenum
.mode list
.headers off

CREATE VIEW IF NOT EXISTS noahsrug AS
SELECT * FROM customers
NATURAL JOIN orders
NATURAL JOIN orders_items
NATURAL JOIN products;

CREATE TEMP VIEW investigator AS
SELECT phone
FROM customers
WHERE phonenum(substr(name, instr(name, ' ') + 1)) == replace(phone, '-', '')
LIMIT 1;

CREATE TEMP VIEW contractor AS
SELECT phone, citystatezip
FROM noahsrug
WHERE name LIKE 'D% S%'
  AND strftime('%Y', shipped) == '2017'
  AND desc == 'Rug Cleaner'
LIMIT 1;

CREATE TEMP VIEW neighbor AS
SELECT phone
FROM customers
WHERE ((strftime('%d', birthdate) &gt;= '23'
        AND strftime('%m', birthdate) == '09')
       OR (strftime('%d', birthdate) &lt;= '22'
           AND strftime('%m', birthdate) == '10'))
  AND citystatezip == (SELECT citystatezip FROM contractor)
  AND strftime('%Y', birthdate) IN ('1943', '1955', '1967',
                                    '1979', '1991');

CREATE TEMP VIEW earlybird AS
SELECT phone
FROM noahsrug
WHERE sku LIKE 'BKY%'
  AND strftime('%H', shipped) == '04'
  AND ordered == shipped
LIMIT 1;

CREATE TEMP VIEW catlady AS
SELECT phone
FROM noahsrug
WHERE desc REGEXP '.* (Adult|Senior) Cat.*'
  AND qty &gt;= 10
LIMIT 1;

CREATE TEMP VIEW bargainhunter AS
SELECT name, phone
FROM noahsrug
WHERE unit_price &lt; wholesale_cost
GROUP BY name
ORDER BY count(*) DESC
LIMIT 1;

CREATE TEMP VIEW meetcute AS
WITH a AS
  (SELECT phone, desc, orderid
   FROM noahsrug
   WHERE desc LIKE '%(%'
     AND name == (SELECT name FROM bargainhunter)
     AND ordered == shipped)
SELECT b.name, b.phone
FROM a
JOIN noahsrug AS b
  ON substr(a.desc, 1, instr(a.desc, '(')) == substr(b.desc, 1, instr(b.desc, '('))
 AND b.orderid == a.orderid - 1;

CREATE TEMP VIEW collector AS
SELECT name, phone
FROM noahsrug
WHERE sku LIKE 'COL%'
GROUP BY name
ORDER BY count(*) DESC
LIMIT 1;

SELECT phone FROM investigator;
SELECT phone FROM contractor;
SELECT phone FROM neighbor;
SELECT phone FROM earlybird;
SELECT phone FROM catlady;
SELECT phone FROM bargainhunter;
SELECT phone FROM meetcute;
SELECT phone FROM collector;</code></pre>

</details>

<pre><code>sqlite&gt; .open noahs-speedrun.sqlite
sqlite&gt; .read noahsrug-speedrun.sql
767-365-7269
838-351-0370
914-594-5535
716-789-4433
347-835-2358
838-295-7143
516-544-4187
516-638-9966</code></pre>

]]>
</content>

<category term="sql" />

</entry>
<entry>
<title type="html">Minified versions of fonts</title>
<link href="https://pjg1.site/minfonts.html" rel="alternate" type="text/html" title="Minified versions of fonts" />
<published>2024-03-30T00:00:00+04:00</published>
<updated>2024-03-30T00:00:00+04:00</updated>
<id>https://pjg1.site/minfonts.html</id>
<content type="html" xml:base="https://pjg1.site/minfonts.html">
<![CDATA[<p>While looking for custom fonts for this website, I came across <a href="https://github.com/joelesko/minlo-font/tree/main">Minlo</a>, a minified version of the Apple system font Menlo.</p>

<p>I first thought it was made smaller by some cool compression technique. While that is true (converting to WOFF2 leads to smaller file sizes), I found that it even contains lesser characters (glyphs) than in a regular font file.</p>

<p>As I'd been considering using custom fonts in my website, I wanted to make something like Minlo, but for other fonts.</p>

<h2 id="manual-approach">Manual approach</h2>

<p>The first idea I had was to manually remove glyphs I don't need using <a href="https://fontforge.org/en-US/">Font Forge</a>, a font editor.</p>

<p>I found a post that describes the exact same steps I tried - <a href="https://barrd.dev/article/create-a-custom-font-with-only-the-glyphs-you-need/">Create a custom font with only the glyphs you need</a>. To summarize:</p>

<ul>
  <li>Open a font file in FontForge</li>
  <li>Select glyphs you wish to remove and delete them</li>
  <li>Regenerate the font file</li>
</ul>

<p>The process is straight-forward but time-consuming. Font files can have lots of glyphs, specifically any combination of characters from this <a href="https://en.wikipedia.org/wiki/List_of_Unicode_characters">List of unicode characters</a>. There are also separate files for different weights, so the process became repetitive really quickly.</p>

<p>There had to be a way to do this programmatically, right?</p>

<h2 id="meet-pyftsubset">Meet <code>pyftsubset</code></h2>

<p>I found an answer to this in the most unexpected of places - the <a href="https://rsms.me/inter/#faq-subset">FAQ section of Inter</a>, a sans-serif font family.</p>

<p>The tool is called <code>pyftsubset</code>, and since I'm looking for to create a file that contains a subset of the original set of glyphs, this sounds like the right one to go with!</p>

<p>Looking at the <a href="https://fonttools.readthedocs.io/en/latest/subset/">documentation</a>, the tool is part of the <code>fonttools</code> Python package, which I installed using <code>pip</code>.</p>

<pre><code>$ python3 -m pip install fonttools
$ pyftsubset -h
usage: pyftsubset font-file [glyph...] [--option=value]...
Try pyftsubset --help for more information.</code></pre>

<p>Looking back at the docs, there are a couple of ways to provide which glyphs I want to keep. The easiest option seems to be using <code>--unicodes</code><sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup>, that takes a list of the Unicode code points (<code>U+</code> followed by hexadecimal digits) representing each character.</p>

<p>If you're familiar with ASCII, Unicode (more specifically UTF-8) is another encoding standard that contains more characters than ASCII, like symbols and characters from other languages.</p>

<h3 id="demo-time">Demo time!</h3>
<p>I created a subset file of the font Inter Regular, containing only uppercase and lowercase letters, digits and punctuation.</p>

<pre><code>$ pyftsubset Inter-Regular.otf --unicodes=U+0020-007E \
  --layout-features='' \
  --passthrough-tables</code></pre>

<p>The unicode range is from the <a href="https://en.wikipedia.org/wiki/Basic_Latin_(Unicode_block)">Basic Latin unicode block</a>, the <code>--layout-features</code> flag is set to an empty string to remove all glyphs associated with <a href="https://rsms.me/inter/#features">OpenType features</a>, while <code>--passthrough-tables</code> skips over any sections of the file that the tool doesn't know what to do with.</p>

<p>With this, the file size has reduced from 591KB to 12KB, a 97% decrease!</p>

<pre><code>$ du -Ahs Inter-Regular.otf Inter-Regular.subset.otf
591K    Inter-Regular.otf
 12K    Inter-Regular.subset.otf</code></pre>

<p>Converting the file to the WOFF2 format reduces the size further to 7.5KB!</p>

<pre><code>$ pyftsubset Inter-Regular.otf --unicodes=U+0020-007E \
  --layout-features='' \
  --passthrough-tables \
  --flavor=woff2

$ du -Ahs Inter-Regular.otf Inter-Regular.subset.woff2
591K    Inter-Regular.otf
7.5K    Inter-Regular.subset.woff2</code></pre>

<p>This was pretty fun to experiment with, and after quite a lot of trial and error, I settled on a combination of Verdana for the text (same as before) and a custom subset of Menlo for code blocks.</p>

<h2 id="font-trivia">Font trivia</h2>

<p>While re-designing my blog, I found out more about the origins of Verdana and Menlo, and apparently they share common links to other fonts!</p>

<p>Verdana was the basis for the design of <a href="https://en.wikipedia.org/wiki/Bitstream_Vera">Bitstream Vera Sans</a>. Bitstream Vera also has a monospaced font, which is what Menlo is based on.</p>

<p>Another common link is the <a href="https://en.wikipedia.org/wiki/DejaVu_fonts">DejaVu font family</a>, an extension to the Bitstream Vera font family that adds support for more glyphs, which Menlo includes.</p>

<p>This explains why the two work well together!</p>
<h2 id="notes">Notes</h2>
  <ol class="footnotes" role="doc-endnotes">
    <li id="fn:1" role="doc-endnote">
      <p>I got confused for a bit on why the <code>unicodes</code> flag was displayed with one dash instead of two. This a bug in the docs, which has been discussed in <a href="https://github.com/fonttools/fonttools/issues/2900">fonttools#2900</a>.&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
    </li>
  </ol>

]]>
</content>

<category term="blogging" />

<category term="font" />

</entry>
<entry>
<title type="html">Python type conversion cheatsheet</title>
<link href="https://pjg1.site/python-type-cheatsheet.html" rel="alternate" type="text/html" title="Python type conversion cheatsheet" />
<published>2024-02-11T00:00:00+04:00</published>
<updated>2025-01-24T00:00:00+04:00</updated>
<id>https://pjg1.site/python-type-cheatsheet.html</id>
<content type="html" xml:base="https://pjg1.site/python-type-cheatsheet.html">
<![CDATA[<p>The built-in functions to convert between different types in Python is something I keep forgetting and have to look up each time. So I decided to make a cheatsheet for reference. Tested on version 3.11.7.</p>

<h2 id="int---hex">int &lt;-&gt; hex</h2>

<p>There are two <a href="https://docs.python.org/3.11/library/functions.html">built-in functions</a> for this, <code>hex()</code> and <code>int()</code>.</p>

<p><code>hex()</code> converts an integer (base 10) to a hexadecimal number (base 16). Hexadecimal numbers aren't a separate type in Python, so the resuting output is of type <code>str</code>.</p>

<pre><code>&gt;&gt;&gt; hex(255)
'0xff'</code></pre>

<p>For the other way round, <code>int()</code> is called with the base set to 16. The function accepts input with and without the <code>0x</code> prefix.</p>

<pre><code>&gt;&gt;&gt; int('0xff', 16)
255
&gt;&gt;&gt; int('ff', 16)
255</code></pre>

<p>Even typing the hex string with the <code>0x</code> prefix but without the quotes will output its integer representation.</p>

<pre><code>&gt;&gt;&gt; 0xff
255</code></pre>

<h2 id="hex---bytes">hex &lt;-&gt; bytes</h2>

<p>The functions are part of the <a href="https://docs.python.org/3.11/library/stdtypes.html#bytes"><code>bytes</code></a> built-in type.</p>

<p><code>bytes.hex()</code> converts bytes to a hexadecimal string.</p>

<pre><code>&gt;&gt;&gt; bytes.hex(b'hello')
'68656c6c6f'</code></pre>

<p><code>bytes.fromhex()</code> converts a hex string to its byte form. Unlike <code>int()</code>, this function only accepts input without the <code>0x</code> prefix.</p>

<pre><code>&gt;&gt;&gt; bytes.fromhex('68656c6c6f')
b'hello'
&gt;&gt;&gt; bytes.fromhex('0x68656c6c6f')
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
ValueError: non-hexadecimal number found in fromhex() arg at position 1</code></pre>

<h2 id="int---bytes">int &lt;-&gt; bytes</h2>

<p>The <code>int</code> data type has <a href="https://docs.python.org/3.11/library/stdtypes.html#additional-methods-on-integer-types">functions</a> for converting to and from bytes.</p>

<p><code>int.to_bytes()</code> requires two parameters apart from the number itself.</p>

<p>The first is the length of the resulting output, or the number of bytes. I can either specify a number, or use another function, <code>int.bit_length()</code> to calculate the number of bytes.</p>

<pre><code>&gt;&gt;&gt; num = 5
&gt;&gt;&gt; bin(num)
'0b101'
&gt;&gt;&gt; int.bit_length(num)
3
&gt;&gt;&gt; nbytes = (int.bit_length(num) + 7) // 8
&gt;&gt;&gt; nbytes
1</code></pre>

<p>The next parameter is the byte order. When in doubt, default to big-endian.</p>

<pre><code>&gt;&gt;&gt; int.to_bytes(num, (int.bit_length(num) + 7) // 8, 'big')
b'\x05'
&gt;&gt;&gt; int.to_bytes(num, 2, 'big')
b'\x00\x05'
&gt;&gt;&gt; int.to_bytes(num, 2, 'little')
b'\x05\x00'</code></pre>

<p>The function can also be written as <code>num.to_bytes(2, 'big')</code>, however I prefer the earlier notation as its more consistent with <code>int.from_bytes()</code>, the reverse function.</p>

<pre><code>&gt;&gt;&gt; int.from_bytes(b'\x00\x05', 'big')
5
&gt;&gt;&gt; int.from_bytes(b'\x05\x00', 'little')
5</code></pre>

<h2 id="str---bytes">str &lt;-&gt; bytes</h2>

<p>The process of translating human readable characters (string) to something
computers can understand (bytes) is called encoding. Strings can be encoded using <a href="https://docs.python.org/3.11/library/stdtypes.html#str.encode"><code>str.encode()</code></a>. <code>utf-8</code> is the default encoding, however this can be changed.</p>

<pre><code>&gt;&gt;&gt; str.encode('hello', 'utf-8')
b'hello'
&gt;&gt;&gt; str.encode('hello', 'utf-16')
b'\xff\xfeh\x00e\x00l\x00l\x00o\x00'</code></pre>

<p>The reverse process, i.e., decoding (converting bytes to string) is done using <a href="https://docs.python.org/3.11/library/stdtypes.html#bytes.decode"><code>bytes.decode()</code></a>.</p>

<pre><code>&gt;&gt;&gt; bytes.decode(b'hello', 'utf-8')
'hello'
&gt;&gt;&gt; bytes.decode(b'\xff\xfeh\x00e\x00l\x00l\x00o\x00', 'utf-16')
'hello'</code></pre>

<h2 id="int---str">int &lt;-&gt; str</h2>

<p>This is the one I have most confusion about, as the <code>int</code>s are one of two types:</p>
<ul>
  <li>ASCII characters, either individual or appended together</li>
  <li>All characters of a string are added up as one single integer/long</li>
</ul>

<p>For ASCII characters, its two <a href="https://docs.python.org/3.11/library/functions.html">built-in functions</a> - <code>chr()</code> for number to character, and <code>ord()</code> for the opposite.</p>

<pre><code>&gt;&gt;&gt; chr(65)
'A'
&gt;&gt;&gt; ord('A')
65</code></pre>

<p>For longer ints, the process is the same as <a href="#int--bytes">int &lt;-&gt; bytes</a>, with the additional step of encoding or decoding where needed.</p>

<p>In the case of int to str, the resulting bytes are decoded.</p>

<pre><code>&gt;&gt;&gt; num = 448378203247
&gt;&gt;&gt; int.to_bytes(num, 5, 'big').decode('utf-8')
'hello'</code></pre>

<p>For the other way round, the string input is encoded before converting.</p>

<pre><code>&gt;&gt;&gt; text = 'hello'
&gt;&gt;&gt; int.from_bytes(text.encode('utf-8'), 'big')
448378203247</code></pre>

<p>The same holds true for <a href="#hex--bytes">hex &lt;-&gt; bytes</a> as well.</p>

]]>
</content>

<category term="python" />

</entry>
<entry>
<title type="html">RC07: return statement;</title>
<link href="https://pjg1.site/rc07.html" rel="alternate" type="text/html" title="RC07: return statement;" />
<published>2023-11-04T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/rc07.html</id>
<content type="html" xml:base="https://pjg1.site/rc07.html">
<![CDATA[<p>A <code>return statement;</code> is a post to describe and reflect on one's time at the Recurse Center<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup>. I completed my batch last week, and decided to follow this tradition to close out the RC series of posts on this blog.</p>

<p>This post goes over the highlights of my batch, a combination of the technical stuff I learnt, and most importantly the discoveries I made - about myself and my approach to programming and learning in general.</p>

<h2 id="starting-a-personal-project">Starting a personal project</h2>

<p>I've had a mental block regarding personal coding projects for the longest time, and thought that working on multiple, tiny projects at RC would be the solution to the problem.</p>

<p>Initial attempts at tiny projects left me with mixed feelings:</p>

<ul>
<li>
  <p>I attempted to make a HTTP Client and Server using <a href="http://beej.us/guide/bgnet/html/">Beej's Guide to Network Programming</a>. I learnt how sockets worked, but got bored before I could finish it.</p>
</li>
<li>
  <p>I wrote Go code for the first time while building <a href="https://pjg1.site/rc02">a TUI using Bubble Tea</a>. Great learning experience, and became a topic for my first RC presentation, but I stopped using the app at one point.</p>
</li>
</ul>

<p>I was looking for something more, a programming project I didn't get bored of.</p>

<p>A chat with a fellow Recurser, a two-day break from RC and lurking in a group project encouraged me to finally commit to a project in Week 6 - <a href="https://github.com/pjg11/tuntcp"><code>tuntcp</code></a>.</p>

<p>This was the big turning point of my batch, I started looking back on the first 5 weeks more positively thanks to this, despite them being all over the place. And while this was the opposite of a tiny project, something about it felt right.</p>

<p>After hopping between projects for a while, it was great to have moments of focused programming, where I was also learning things in line with my goals.</p>

<p>This also made me realize that I'm a fairly good programmer, who can write decent code outside of one-off shell scripts.</p>

<p>Overall, this project really boosted my confidence, and my approach to RC changed for the better after this. One such change was in the group project I mentioned earlier.</p>

<h2 id="contributing-to-a-group-project">Contributing to a group project</h2>

<p>A group of Recursers from a previous batch kept weekly meeting to work on a <a href="https://github.com/zrottman/dns_c">C implementation</a> of Julia Evans' <a href="https://implement-dns.wizardzines.com">Implement DNS in a Weekend</a>.</p>

<p>My batch started when they were probably half-way through the guide. I started off on the listening end in the first few weeks, as I was unfamiliar with the code and was intimidated to contribute.</p>

<p>At one point, I considered dropping out of this session, as it was taking up a big chunk of time, and I wasn't gaining much out of it. However I continued to attend, hoping I would contribute someday.</p>

<p>That day finally came in Week 7, as I lead the group session for the first time. Working on <code>tuntcp</code> gave me the confidence of writing C code, and the 6 weeks of observing made me familiar with the codebase.</p>

<p>This project was also responsible for the initial progress of <code>tuntcp</code>, as I used the project's source code as inspiration for the code structure.</p>

<p>I went from wanting to leave this session, to completing the core of the project on the last day of my batch.</p>

<h2 id="pair-programming--coffee-chats">Pair programming + coffee chats</h2>

<p>Pair programming as a concept is first introduced during the pairing workshop, as part of the orientation events. By the end of that session, I was a fan. I knew this would become my favorite way of learning, and looking back at 31 pairing sessions on my calendar, I was right!</p>

<p>Pairing is when I find myself most engaged with the task at hand and make significant progress or learn something new. A downside is that it demands a lot of energy and focus, which meant I had to schedule these carefully.</p>

<p>It also contributed significantly to building my <a href="https://www.recurse.com/self-directives#volitional-muscles">volitional muscles</a>, as many of these pairing sessions were initiated based on what I found interesting<sup id="fnref:2"><a class="footnote" href="#fn:2" rel="footnote" role="doc-noteref">2</a></sup>.</p>

<p>Here are some pairing highlights:</p>

<ul>
  <li>
    <p>My most memorable pairing session - contributing to a fellow Recurser's project, <a href="http://github.com/friedenberg/tacky">tacky</a>. I expected to be on the listening end for this session, but thanks to encouragement from my pairing partner, I ended up writing code in the session AND committed changes to the repository, which was really fun!</p>
  </li>
  <li>
    <p>Completing the <a href="http://csapp.cs.cmu.edu/3e/attacklab.pdf">Attack Lab</a>, which helped me get more familiar with writing assembly, using GDB and how the stack works.</p>
  </li>
  <li>
    <p>Working on past Capture The Flag challenges I struggled to solve on my own. These include solving <a href="https://github.com/Nautilus-Institute/quals-2023/tree/main/omgzip/src">omgzip</a> and nearly solving <a href="https://github.com/project-sekai-ctf/sekaictf-2023/tree/main/misc/a-letter-from-the-human-resource-management">A letter from the Human Resource Management</a>.</p>
  </li>
  <li>
    <p>Working through other challenges like <a href="https://sadservers.com">Sad Servers</a> and <a href="https://pwn.college">pwn.college</a>. These were great ways to put things I learnt into practice, and the structure of these challenges were very suitable for pairing sessions!</p>
  </li>
</ul>

<p>Another form of 1:1 interactions I enjoyed were coffee chats - casual chats with fellow Recursers. I really looked forward to these, as these were the closest I got to informal chats one would have when attending in-person.</p>

<p>It was a great way to meet people with both similar and completely different interests, and to get to hear about different approaches to programming, learning and even jobs.</p>

<h2 id="impossible-stuff-day">Impossible Stuff Day</h2>

<p>Impossible Stuff Day is a day organized by RC to work on something that is well beyond what you're currently capable of. The idea is to take the first step towards doing something seemingly impossible, and seeing how far you can get.</p>

<p>This concept clicked really well with me. The first Impossible Stuff Day in Week 3 helped me get unstuck, while the second one in Week 9 exceeded my own expectations.</p>

<p>The week of the first Impossible Stuff Day was possibly the lowest point of my batch, as I was unsure of what I really wanted to do. I'd heard about how installing <a href="https://www.gentoo.org">Gentoo Linux</a> helps you understand how the operating system really works, as it's installation requires a fair amount of manual configuration.</p>

<p>I didn't attempt it until then as I was fine with the current Linux distribution I was using. However, it seemed like a good idea for Impossible Stuff Day, and after some doubting, I committed to installing it as a Virtual Machine on my laptop.</p>

<p>It took me two days, but I did it! I experienced lots of firsts - partitioning disks, configuring the bootloader, selecting drivers to install and compiling the kernel! It made me more willing to <em>try different things</em>, even if they didn't feel important to work on in that moment.</p>

<p>By the time the second Impossible Stuff Day happened, I felt like a completely different person, and my choice of project reflected that.</p>

<p>I created and hosted a tiny Capture The Flag challenge using Docker on a DigitalOcean droplet. I wrote <a href="https://pjg1.site/rc05">a blog post</a> covering all the details and gave a presentation about it the week after, which was also well-received!</p>

<p>I think I blew my own mind with this project. This was the first time I made something that people could try out, and that too in two days! It's the closest I got to <a href="https://www.recurse.com/self-directives#work-at-the-edge">working at the edge of my abilities</a>, and it felt great.</p>

<h2 id="learning-in-public">Learning in public</h2>

<p><a href="https://www.recurse.com/self-directives#learn-generously">Learning generously</a> is a big part of RC and is encouraged from the very first day. I did so in three significant ways - blog posts, written checkins and presentations.</p>

<p>I had picked up blogging again shortly before RC but didn't have a regular schedule. So I thought blogging during my batch would be a good way to establish a schedule.</p>

<p>There were initial troubles, as it was hard to find time to write with so many activities going on during the week. The momentum came in the latter half of my batch, where I posted once every 7-10 days.</p>

<p>Writing the posts themselves was a tedious process - I'd start with a good idea, but would excessively edit my post while writing it, to the point where I'd be annoyed by the time I'd hit publish. It's something I'm still working on getting better at.</p>

<p>I posted written checkins daily as an accountability mechanism during my batch. Writing these came naturally to me, and became an integral part of my days at RC as someone attending entirely remotely.</p>

<p>I also wrote weekly reflections as part of the checkins each Monday, where I reflected on the past week - what worked, what didn't and what to do differently. In hindsight, this is what kept me fairly on track, I would have been a lot more distracted otherwise.</p>

<p>RC hosts Presentations every week, where Recursers present what they're working on in 5-minute time slots. Attending this made me feel closest to the community, as almost all Recursers would join in the call every Thursday.</p>

<p>Since my focus was on blogging, my approach for presentations was "whenever I felt like and had a cool topic to talk about", which happened to be a total of three times:</p>
<ul>
  <li>A demo of the <a href="https://github.com/pjg11/todo">todo TUI app</a> in Week 2</li>
  <li>An interesting error while working on <code>tuntcp</code> in Week 7, which started as <a href="https://pjg1.site/rc03">a blog post</a></li>
  <li>How I created and hosted the <a href="https://github.com/pjg11/quack-ctf-challenge">CTF challenge</a> during Impossible Stuff Day in Week 10</li>
</ul>

<p>I enjoyed presenting, and it was a good practice on how to convey information in a limited timeframe, despite the preparation taking forever.</p>

<h2 id="finding-a-routine-that-works-for-me">Finding a routine that works for me</h2>

<p>Being self-directed meant I was responsible for making my own schedule.</p>

<p>The first two weeks were a little confusing. I was adjusting to Zoom fatigue and an 8-hour time difference (Zoom fatigue being the bigger problem), which meant that the scheduling was slightly all over the place.</p>

<p>Around Week 3, I figured a set of hours that worked - with my timezone and core RC hours. I kept the core RC hours being open for chats, group meetings and other events, and did solo work either before or after that.</p>

<p>Scheduling things within those set of hours was another task. I had all different kinds of weeks, ranging from great to not great. I realized that my ideal schedule requires an equal mix of social interactions and solo work. Overdoing any one will leave me feeling completely drained.</p>

<p>How I ensure this balance post-batch will be an interesting task. At least I have the weekly reflections and checkins to help along the way.</p>

<h2 id="discovering-a-field-i-like">Discovering a field I like</h2>

<p>I came to RC with a vague idea of the topics I liked - Linux, security, networking, and low-level programming. I came to RC to find a field or job role that included the right combination of these interests.</p>

<p>Thanks to coffee chats and exposure to so many different topics, I found <em>DevOps</em> to be a field that matched my interests the most.</p>

<p>This gives me a good idea of what I need to learn next, and makes the job search easier as I know what I'm looking for.</p>

<h2 id="whats-next">What's next?</h2>

<p>The Recurse Center helped develop a set of learning strategies that work for me, and now is the time to put them to practice.</p>

<p>At the end of a batch, you <a href="https://www.recurse.com/manual#sub-sec-never-graduate">Never Graduate</a>, and can continue to be a part of the community and events. That's what I'll be doing for a while - working on existing projects (maybe starting new ones?) blogging, pairing away and learning new things.</p>

<p>I remember thinking "this is exactly what I was looking for" when reading through RC's website for the first time, and it still holds true three months later.</p>
<h2 id="notes">Notes</h2>
  <ol class="footnotes" role="doc-endnotes">
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://writing.natwelch.com/post/401">This Return Statement</a> from another Recurser goes explains the origin of the term.&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>I'm particularly feeling this post-batch, as I'm a lot better at initiating/joining pairing sessions where I end up learning a lot.&nbsp;<a class="reversefootnote" href="#fnref:2" role="doc-backlink">â†©</a></p>
    </li>
  </ol>

]]>
</content>

<category term="recurse" />

</entry>
<entry>
<title type="html">RC06: Notes on creating a TUN interface</title>
<link href="https://pjg1.site/rc06.html" rel="alternate" type="text/html" title="RC06: Notes on creating a TUN interface" />
<published>2023-10-23T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/rc06.html</id>
<content type="html" xml:base="https://pjg1.site/rc06.html">
<![CDATA[<p>While working on <a href="http://github.com/pjg11/tuntcp"><code>tuntcp</code></a>, one of the first things I did was set up a <a href="https://en.wikipedia.org/wiki/TUN/TAP">TUN interface</a>, a virtual network device operating at the Network Layer/Layer 3 of the network stack. I used the following commands:</p>

<pre><code>sudo ip tuntap add dev tun0 mode tun user $USER
sudo ip addr add 192.0.2.1 peer 192.0.2.2 dev tun0
sudo ip link set dev tun0 up</code></pre>

<p>I ran these commands with only a vague understanding of what's going on - it creates a TUN device called <code>tun0</code>, assigns it two IP addresses for some reason and sets the interface to <code>up</code>.</p>

<p>However, I had questions, which I kept ignoring as I was making progress in terms of code. When I got stuck on another part of the project and was unable to code, I decided figure out what these commands <em>really</em> do. This post is a collection of notes I made along the way.</p>

<h2 id="why-are-there-two-addresses-for-a-tun-interface">Why are there two addresses for a TUN interface?</h2>

<p>The first thing that confused me was the mention of two addresses. I initially thought that it might not be significant, however any packets I created used the peer address as the source address, <code>192.0.2.2</code>, which didn't make sense.</p>

<p>I started with the <code>man</code> page for <code>ip-address</code>:</p>

<pre><code>peer ADDRESS
        the address of the remote endpoint for pointopoint interâ€
        faces.
...</code></pre>

<p>Well, this just threw more terms I don't understand at me, particularly <code>pointopoint interfaces</code>. So I focused on understanding that first, with the help of <a href="https://stackoverflow.com/a/37327065">an answer</a> on StackOverflow.</p>

<p>A point-to-point interface connects two hosts directly. The only hosts on this interface are <code>192.0.2.1</code> and <code>192.0.2.2</code> in our example.</p>

<p>Due to this, this interface does not have a MAC address and anything related to Layer 2, like ARP or the Address Resolution Protocol. This is in contrast to an Ethernet device, like the <code>eth0</code> interface, which is not a point to point interface and has a MAC address.</p>

<p>Next came the <code>remote endpoint</code> part of the explanation. Point-to-point interfaces were still not clicking at this point, so I was confused on why the remote address was even required.</p>

<p>This confusion was cleared by a <a href="https://unix.stackexchange.com/a/589432">beautiful answer</a> on Stack Exchange. I didn't even try paraphrasing this because it's explained so well:</p>

<blockquote>
  <p>In ancient times, when people used to connect a modem device to the telephone line and dial the phone of an internet provider to establish connection to the internet, the <code>pppd</code> daemon used to be responsible to establish a point-to-point tunnel to the server located at the other end of the call. On those tunnels, the local address was the address assigned to the network interface being created in kernel network stack and the remote address was the local address of the other computer answering the phone call and, also, the address set as the default gateway in the local side.</p>

  <p>For TUN virtual devices, your user-space program will act as <em>the remote computer at the other side of the tunnel</em>. Therefore, in order to have your program injecting IP packets to the kernel network stack, the program is supposed to generate packets with source address set to the tunnel remote address (<code>192.168.69.1</code>) and receive packets whose destination is set to the same address.</p>
</blockquote>

<p>After reading this, it all clicked into place.</p>

<h2 id="why-is-the-state-down-when-i-set-the-link-to-up">Why is the state DOWN when I set the link to UP?</h2>

<p>This confusion comes from the output of the TUN interface settings.</p>

<pre><code>$ ip addr show tun0
8: tun0: &lt;NO-CARRIER,POINTOPOINT,MULTICAST,NOARP,UP&gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 500
    link/none
    inet 192.0.2.1 peer 192.0.2.2/32 scope global tun0
       valid_lft forever preferred_lft forever</code></pre>

<p>This situation appears when the link is set to up, but is not being actively used by a program, indicated by the <code>NO_CARRIER</code> flag. This seems similar to a laptop charger being connected to a socket, but the charger isn't plugged into a laptop yet.</p>

<p><code>tuntcp</code> contains <a href="https://github.com/pjg11/tuntcp/blob/main/tuntcp.c#L131">code that connects to this interface</a>. So once the program is run, the connection is established, and the interface's output changes:</p>

<pre><code>$ ip addr show tun0
8: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 500
    link/none
    inet 192.0.2.1 peer 192.0.2.2/32 scope global tun0
        valid_lft forever preferred_lft forever
    inet6 fe80::2db0:db32:1898:51a5/64 scope link stable-privacy proto kernel_ll
        valid_lft forever preferred_lft forever</code></pre>

<p><code>NO_CARRIER</code> disappears, <code>LOWER_UP</code> appears and the state changes to <code>UP</code>.</p>

<h2 id="why-does-pinging-work-differently-for-the-two-addresses">Why does pinging work differently for the two addresses?</h2>

<p>Another useful resource I found was a post titled <a href="https://backreference.org/2010/03/26/tuntap-interface-tutorial/index.html">Tun/Tap interface tutorial</a>. It goes in-depth about how TUN interfaces work and are set up.</p>

<p>The part that confused me was the section about pinging the two addresses (<code>10.0.0.1</code> and <code>10.0.0.2</code> in the article), and the difference in output. The interface address (<code>10.0.0.1</code>) responded to pings, whereas the peer address (<code>10.0.0.2</code>) did not, and I didn't fully understand the explanation provided.</p>

<p>I also wanted to know where the ping packets for the interface address were going, which wasn't specified in the post. So I decided to capture ICMP packets at all interfaces to find out.</p>

<p>First, I ran <code>openvpn</code> in the background to keep the TUN interface up, which I discovered from the tutorial post.</p>

<pre><code>$ sudo openvpn --dev tun0
2023-10-22 18:21:54 OpenVPN 2.6.1 x86_64-pc-linux-gnu [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [PKCS11] [MH/PKTINFO] [AEAD] [DCO]
2023-10-22 18:21:54 library versions: OpenSSL 3.0.8 7 Feb 2023, LZO 2.10
2023-10-22 18:21:54 TUN/TAP device tun0 opened
2023-10-22 18:21:54 Could not determine IPv4/IPv6 protocol. Using AF_INET
2023-10-22 18:21:54 UDPv4 link local (bound): [AF_INET][undef]:1194
2023-10-22 18:21:54 UDPv4 link remote: [AF_UNSPEC]
^Z
[1]+  Stopped                 sudo openvpn --dev tun0
$ bg
[1]+ sudo openvpn --dev tun0 &amp;</code></pre>

<p>This command is also used to create the interface, but in this case works as the program connecting to the interface. Then, I started capturing packets with <code>tcpdump</code>:</p>

<pre><code>$ sudo tcpdump -ni any 'icmp'
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes</code></pre>

<p>I ran <code>ping -c 1 192.0.2.1</code> from another shell session, which returned the following packets (emphasis mine):</p>

<pre><code>18:22:35.053952 <strong>lo</strong>    In  <strong>IP 192.0.2.1 &gt; 192.0.2.1</strong>: ICMP echo request, id 28435, seq 1, length 64
18:22:35.053964 <strong>lo</strong>    In  <strong>IP 192.0.2.1 &gt; 192.0.2.1</strong>: ICMP echo reply, id 28435, seq 1, length 64</code></pre>

<p>The packets came from the loopback interface (<code>lo</code>) and the source and destination addresses are the same, interesting.</p>

<p>Running <code>ping -c 1 192.0.2.2</code> returned only one packet (emphasis mine):</p>

<pre><code>18:22:37.419702 <strong>tun0</strong>  Out <strong>IP 192.0.2.1 &gt; 192.0.2.2</strong>: ICMP echo request, id 6389, seq 1, length 64</code></pre>

<p>The source address is <code>192.0.2.1</code>, and the packet is captured at the <code>tun0</code> interface. Also, no echo reply packet after the request.</p>

<p>Why does each address return different packets? I think it may have something to do with the routes created when the interface is set up.</p>

<h3 id="checking-the-routing-table">Checking the routing table</h3>

<pre><code>$ ip route show table all | grep tun0
192.0.2.2 dev tun0 proto kernel scope link src 192.0.2.1
local 192.0.2.1 dev tun0 table local proto kernel scope host src 192.0.2.1</code></pre>

<p>Once again, lots of words and little to no idea of what they mean. A Recurser pointed to <a href="http://linux-ip.net/html/tools-ip-route.html">this guide</a>, which helped make sense of the terminology.</p>

<p>Let's start with the route for <code>192.0.2.2</code>, the peer address:</p>

<pre><code>192.0.2.2          the route is for the address 192.0.2.2
dev tun0           reachable from interface tun0
proto kernel       route added by the kernel when setting up the interface
scope link         valid only on the mentioned interface, tun0
src 192.0.2.1      preferred source address when sending to this destination</code></pre>

<p>The <code>src</code> part was a little confusing at first, however it does match the ping output for <code>192.0.2.2</code> mentioned earlier. The source address there was <code>192.0.2.1</code>, and I think its because the peer address is reachable only via the local address, being a point-to-point link.</p>

<p>The reason there is no echo reply from <code>192.0.2.2</code> is because the connected program has no implementation for ICMP or the TCP/IP stack in general. It doesn't use the kernel's network stack and requires its own, which is where <code>tuntcp</code> steps in.</p>

<p>Next comes the route for <code>192.0.2.1</code>, which looks pretty different:</p>

<pre><code>local 192.0.2.1    address 192.0.2.1 is locally hosted on this machine
dev tun0           reachable from interface tun0
table local        part of the local routing table
proto kernel       route added by the kernel when setting up the interface
scope host         valid only on this machine
src 192.0.2.1      preferred source address when sending to this destination</code></pre>

<p>This is specified as a local address, an address directly accessible to the machine. The route is added to a routing table called <code>local</code>. When pinging local addresses, the packets are sent in and out from the loopback interface, <code>lo</code>, as seen earlier.</p>

<p>Being a local address, the kernel's network stack manages the sending and receiving, which is why there was an echo reply when pinging to this address.</p>

<p>This is how local routes for other interfaces are written too, which I found out from <a href="https://backreference.org/2010/03/26/tuntap-interface-tutorial/index.html#comment-24909">this comment</a> from the tutorial post.</p>

<h2 id="more-questions">More questions?</h2>

<p>TUN interfaces are starting to make a lot more sense now, but it also raises one more question. Both the hosts are only accessible within this machine. I'd like to send TCP packets to other hosts on the Internet, which doesn't seem possible with this setup.</p>

<p>Turns out it is possible, with the help this of NAT or Network Address Translation, which I'll save for another post as this one has a TUN of information already.</p>

]]>
</content>

<category term="networking" />

<category term="recurse" />

</entry>
<entry>
<title type="html">RC05: Creating and hosting a CTF challenge in two days</title>
<link href="https://pjg1.site/rc05.html" rel="alternate" type="text/html" title="RC05: Creating and hosting a CTF challenge in two days" />
<published>2023-10-11T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/rc05.html</id>
<content type="html" xml:base="https://pjg1.site/rc05.html">
<![CDATA[<p>I've <a href="https://writeups-pjg1.netlify.app">played quite a few CTFs</a>, but hadn't attempted to create and host one before. With the announcement of Impossible Stuff Day at RC last week<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup>, it seemed like a good opportunity to finally give it a shot.</p>

<p>This post covers details of the challenge and the underlying infrastructure. The source files are up on <a href="https://github.com/pjg11/quack-ctf-challenge">pjg11/quack-ctf-challenge</a> if you'd like to try it out yourself!</p>

<h2 id="the-initial-plan">The initial plan</h2>

<p>My aim was to host one (1) challenge, with interaction via <code>ssh</code> or <code>nc</code>, as a web-based challenge would take more time to set up.</p>

<p>For challenge ideas, I was inspired by <a href="https://ctftime.org/task/25620">Wordle Bash</a>, a challenge I attempted while playing NahamCon CTF this year. I found it tricky to solve, but liked the game-like nature of the challenge.</p>

<p>The target audience were fellow Recursers - some with CTF experience but a big majority of people being possibly new to them. Having an easy challenge made sense here, allowing me to focus more on the setup.</p>

<p>Looking for existing CTF setups brings up mentions of open-source frameworks like <a href="https://ctfd.io">CTFd</a>. This seemed overkill for my case, so I decided to host the challenge from one (1) Docker container on a server - a DigitalOcean droplet with a 25GB SSD and 1GB RAM.</p>

<p>The challenge would be accessible over SSH, but on a different port than 22 as that is the droplet's SSH port.</p>

<p>The container acts as a sandboxed environment - the user is restricted to the container's filesystem and can't access the server's files<sup id="fnref:2"><a class="footnote" href="#fn:2" rel="footnote" role="doc-noteref">2</a></sup>.</p>

<h2 id="a-test-dockerfile">A test Dockerfile</h2>

<p>When looking for Dockerfiles of other CTF challenges, I came across a video - <a href="https://www.youtube.com/watch?v=gSoNSgzp5Yo">BASH ALIAS CTF Challenge built w/ Docker</a> by John Hammond, a cybersecurity content creator and also the author of Wordle Bash.</p>

<p>He used <a href="https://github.com/rastasheep/ubuntu-sshd"><code>ubuntu-sshd</code></a> as the base image. The image hadn't been updated for recent Ubuntu versions, so I took the code from one of the Dockerfiles and used that as my starting point.</p>

<pre><code>FROM ubuntu:latest

RUN apt-get update -y &amp;&amp; \
    apt-get install -y openssh-server &amp;&amp; \
    apt-get clean -y

RUN useradd -m user -s /bin/bash &amp;&amp; \
    echo "user:pass" | chpasswd &amp;&amp; \
    mkdir /var/run/sshd

WORKDIR /home/user

EXPOSE 22
CMD ["/usr/sbin/sshd", "-D"]</code></pre>

<p>This creates a user apart from the root user, and runs the SSH server with <a href="https://www.man7.org/linux/man-pages/man8/sshd.8.html"><code>-D</code></a>:</p>

<blockquote>
  <p><code>-D</code> <br>
When this option is specified, sshd will not detach and does not become a daemon. This allows easy monitoring of sshd.</p>
</blockquote>

<p>I tried a test run, and it worked! I also found out that Docker makes it very easy to forward ports from the Docker container to the host machine, something I wasn't sure how to do before.</p>

<pre><code># docker build -t test .
[+] Building 32.4s (9/9) FINISHED
# docker run -d -p 20000:22 test
858a1c9b2658715ebc074f8c6de189ebf5fb0b3005d02a5466159a63bae1f75e
# ssh -p 20000 localhost
user@858a1c9b2658:~$ </code></pre>

<p>SSH by default logs into a shell session, however I wanted to run the challenge file at login instead. I couldn't find how to do so, which lead me to change the setup entirely, as I was short on time.</p>

<h2 id="sshd--xinetd"><code>sshd</code> â†’ <code>xinetd</code></h2>

<p>I came across another CTF challenge - <a href="https://github.com/JohnHammond/misfortune-ctf-challenge">misfortune</a>, also written by John Hammond. This challenge used <code>xinetd</code> instead of <code>sshd</code> to run the challenge, and users can access the challenge over <code>nc</code> or netcat.</p>

<p>I wanted to test this out with the Wordle Bash source code, so the following changes were made:</p>

<pre><code>FROM ubuntu:latest

RUN apt-get update -y &amp;&amp; \
    apt-get install -y gpg curl xinetd &amp;&amp; \
    mkdir -p /etc/apt/keyrings &amp;&amp; \
    curl -fsSL https://repo.charm.sh/apt/gpg.key \
    | gpg --dearmor -o /etc/apt/keyrings/charm.gpg &amp;&amp; \
    echo "deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *" \
    | tee /etc/apt/sources.list.d/charm.list &amp;&amp; \
    apt-get update -y &amp;&amp; \
    apt-get install -y gum &amp;&amp; \
    apt-get clean -y</code></pre>

<p>The installation is a lot longer, as it includes the setup required for <code>gum</code>, used for the input fields in Wordle Bash.</p>

<pre><code>RUN useradd -m user -s /bin/bash &amp;&amp; \
    echo "user:pass" | chpasswd

COPY challenge.xinetd /etc/xinetd.d/challenge
COPY entrypoint.sh /entrypoint.sh

RUN chmod 551 entrypoint.sh &amp;&amp; \
    chown -R root:root /home/user

WORKDIR /home/user</code></pre>

<p>There are two additional files required for this to work, which I'll talk about shortly.</p>

<pre><code>EXPOSE 9999
CMD ["/entrypoint.sh"]</code></pre>

<p>The port number changes from <code>22</code> to <code>9999</code>, and a shell script called <code>entrypoint.sh</code> is provided as the starting command, which runs the <code>xinetd</code> server.</p>

<pre><code>#!/bin/sh

/etc/init.d/xinetd start;
trap : TERM INT; sleep infinity &amp; wait</code></pre>

<p>Lastly, <code>challenge.xinetd</code> contains information provided to the server, like the type of server, port number and the script to be run.</p>

<pre><code>service challenge
{
    disable     = no
    socket_type = stream
    protocol    = tcp
    wait        = no
    user        = root
    type        = UNLISTED
    port        = 9999
    bind        = 0.0.0.0

    server      = /home/user/wordle_bash.sh

    log_type       = FILE /var/log/challenge.log
    log_on_success = HOST PID
    log_on_failure = HOST
}</code></pre>

<p>This wasn't successful, as the script wouldn't wait for input.</p>

<pre><code># nc localhost 9999

  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘                                                  â•‘
  â•‘                                                  â•‘
  â•‘                   WORDLE DATE                    â•‘
  â•‘            Uncover the correct date!             â•‘
  â•‘                                                  â•‘
  â•‘                                                  â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

We've selected a random date, and it's up to you to guess it!
Attempt 1:
Please select the year you think we've chosen:
Now, enter the month of your guess:
Finally, enter the day of your guess:
pubdate: extra operand 'to'
Try 'date --help' for more information.
Invalid date! Your guess must be a valid date in the format YYYY-MM-DD.</code></pre>

<p>To see if this was specific to this script, I tested the same setup with a much smaller script, which worked fine.</p>

<pre><code>#!/bin/sh

echo "What is your name?"
read name
echo Hello $name</code></pre>

<p>This meant my plans for the challenge had to change, and I couldn't use <code>gum</code> for input as I initially hoped<sup id="fnref:3"><a class="footnote" href="#fn:3" rel="footnote" role="doc-noteref">3</a></sup>. At this point, Impossible Stuff Day was coming to an end, so I decided to work on the challenge aspect the next day.</p>

<h2 id="building-quack">Building QUACK</h2>

<p>Once the initial plan for the challenge failed, I started to think about it more seriously. I looked to challenges I'd played before, particularly these two:</p>
<ul>
  <li><a href="https://github.com/pjg11/CTF-Writeups/blob/main/2022-NahamCon-CTF/Prisoner.md">Prisoner</a> from NahamCon CTF 2022</li>
  <li><a href="https://heinandre.no/htb-cyber-apocalypse-2022/misc/compressor/">Compressor</a> from Cyber Apocalypse 2021</li>
</ul>

<p>I didn't have the source code for either challenge, but Prisoner seemed easier to recreate, so I went in that direction.</p>

<p>Prisoner is an example of the <code>pyjail</code> style of challenges, where one is provided access to a restricted Python environment, and you're required to escape the <em>jail</em> to access the flag. The core steps are as follows:</p>
<ul>
  <li>Start off with an input prompt that doesn't do anything when typing into it. The challenge can be assisted with ASCII art, to create a story.</li>
  <li>Pressing <kbd>Ctrl</kbd> + <kbd>D</kbd> prints a traceback and displays a Python interactive shell.</li>
  <li>In this shell, one can execute system commands and read the flag.</li>
</ul>

<p>For my challenge, I added one extra step. I stored the flag in a hidden directory called <code>.secret</code>, which might add an extra step for those who tend to use <code>ls</code> without any flags to list files.</p>

<p>For the ASCII art, I went with this <a href="https://www.asciiart.eu/animals/birds-water">cute duck</a>, because the challenge started off as a reference to Rubber Duck Debugging.</p>

<pre><code>    __
___( o)&gt;
\ &lt;_. )
 `---'   hjw</code></pre>

<p>While the challenge eventually took a different turn in terms of the story, the duck stuck around. I named the challenge after it too - QUACK.</p>

<p>I found two ways to show the traceback + interactive shell:</p>
<ol>
  <li>Embed an interactive shell within the Python script.</li>
  <li>Run the source code with <code>python3 -i</code></li>
</ol>

<p>I went with the second option first, making the source code itself very simple. I start off with a sleepy duck, and a while loop asking for input.</p>

<pre><code>#!/usr/bin/env python3

print("""
    __
___( -)&gt;
\ &lt;_. )
 `---'   hjw
""")

while True:
    input("&gt; ")</code></pre>

<p>I was still using the <code>xinetd</code> setup, so I faced issues with interactivity once again.</p>

<pre><code># nc localhost 9999
    __
___( -)&gt;
\ &lt;_. )
 `---'   hjw

&gt; quack
&gt; hello?
&gt; ^D
Traceback (most recent call last):
  File "/home/user/quack.py", line 10, in &lt;module&gt;
    input("&gt; ")
EOFError: EOF when reading a line
&gt;&gt;&gt;
# </code></pre>

<p>It displays the traceback and shell prompt as expected, but exits before I could even type anything in the interactive shellâ€¦ *sigh*</p>

<p>I'm not entirely sure why this happened, but my guess is <code>nc</code> sees <kbd>Ctrl</kbd> + <kbd>D</kbd> as a signal to close the connection. I couldn't find any solution for this, so I tried the first option I mentioned earlier, to embed a shell in the code. This recreates the output produced by <code>python3 -i</code>.</p>

<pre><code>#!/usr/bin/env python3

import readline
import code
from traceback import print_exc
from sys import exit

print("""
A sleepy duck is blocking the way to my files.
Can you wake it up (gently) and ask it for
flag.txt? Thanks!
    __
___( -)&gt;
\ &lt;_. )
 `---'   hjw
""")

try:
    while True:
        input("&gt; ")
except KeyboardInterrupt:
    exit()
except EOFError:
    print("""
    __
___( O)&gt;
\ &lt;_. )
 `---'    hjw
""")
    print_exc()
    code.InteractiveConsole(globals()).interact(banner="",exitmsg="")</code></pre>

<p>The challenge now has a more cohesive story tying it together. The goal is to try and wake up a sleepy duck (gently), and access the flag. <kbd>Ctrl</kbd> + <kbd>D</kbd> or <code>EOFError</code> works, however <kbd>Ctrl</kbd> + <kbd>C</kbd> or <code>KeyboardInterrupt</code> would be considered too harsh, and the program exits.</p>

<p><code>print_exc()</code> prints the traceback, and <code>code.InteractiveConsole()</code> provides the interactive shell.</p>

<p>Lastly, I created a flag file, which the users can read on entering the shell. It follows the format commonly found in CTFs - some text written within braces in 1337 5p34k to make it look all cool and fancy:</p>

<pre><code># echo "CTF{th4nks_f0r_p14y1ng}" &gt; flag.txt</code></pre>

<p>I made the changes to the Dockerfile and other files, and once again, I faced the "EOF when reading a line" I saw earlier. I found myself at a dead end, so I gave up on this setup, and finally went back to using SSH.</p>

<h2 id="xinetd--sshd"><code>xinetd</code> â†’ <code>sshd</code></h2>

<p>The only thing stopping me from using <code>sshd</code> earlier was not knowing a way to run a script at login, which I <a href="https://askubuntu.com/a/397696">finally figured</a>. With that sorted, I got rid of <code>challenge.xinetd</code> and <code>entrypoint.sh</code>, and made the final set of changes to the Dockerfile:</p>

<p><code>python3</code> got added to the installation, as the challenge file is a Python script. The base image changed from Ubuntu to Debian for a smaller image size.</p>

<pre><code>FROM debian:latest

RUN apt-get update -y &amp;&amp; \
    apt-get install -y python3 openssh-server &amp;&amp; \
    apt-get clean -y</code></pre>

<p>Alongside user creation, I created the hidden directory, copied required files to the container and set appropriate permissions.</p>

<pre><code>RUN useradd -m user -s /bin/bash &amp;&amp; \
    echo "user:pass" | chpasswd &amp;&amp; \
    mkdir "/home/user/.secret"

COPY quack.py /home/user/quack.py
COPY flag.txt /home/user/.secret/flag.txt

WORKDIR /home/user

RUN chmod 444 /home/user/.secret/flag.txt &amp;&amp; \
    chmod u+x /home/user/quack.py &amp;&amp; \
    chown -R root:root /home/user</code></pre>

<p>Lastly, the <code>ForceCommand</code> parameter is added to the config file before running the SSH server.</p>

<pre><code>RUN echo "ForceCommand /home/user/quack.py" &gt;&gt; /etc/ssh/sshd_config && \
    mkdir /var/run/sshd

EXPOSE 22
CMD ["/usr/sbin/sshd", "-D"]</code></pre>

<h2 id="finishing-touches">Finishing touches</h2>

<p>As I was testing and running containers multiple times throughout the process, I created a script to automate it.</p>

<pre><code>#!/bin/bash
set -ex

docker build -t quack .
docker run --pids-limit 100 --read-only -d -p 9999:22 quack</code></pre>

<p>This builds and runs the container in read-only mode, preventing the user from making changes to the flag or any other file. The number of processes running at once have also been limited, to <a href="https://stackoverflow.com/questions/28237906/limit-number-of-processes-started-inside-docker-container">prevent forkbombs</a>. This wouldn't be much of an issue in my case, but I had it there just in case.</p>

<p>After multiple detours and changes, I tested the challenge setup, and it worked.</p>

<figure>
<a href="https://asciinema.org/a/674178" target="_blank"><img alt="" loading="lazy" src="https://asciinema.org/a/674178.svg"></a>
<figcaption>Click to view on asciinema</figcaption>
</figure>
<p>After making sure it all works for the 100th time, I sent a message on our chat platform, in the style of a CTF challenge description:</p>

<blockquote>
<pre><code>QUACK
easy | 100 points | misc

A sleepy duck is blocking the way to my files.
Can you wake it up (gently) and ask it for
flag.txt? Thanks!</code></pre>

  <p>The challenge is accessible at <code>ssh -p 9999 user@&lt;IP&gt;</code>, the password is <code>pass</code>.</p>
</blockquote>

<h2 id="running-the-challenge">Running the challenge</h2>

<p>For the 5 hours the challenge was live, I received a total of 6 flag submissions! (For context, this was posted this later in the day on a Friday without much prior notice, so not bad at all!)</p>

<p>The way to submit was to message me the flag directly. Not the most sophisticated setup, but it worked, and I got some positive feedback too:</p>

<blockquote>
  <ul>
    <li>What a lovely capture the flag!</li>
    <li>Fun challenge, thank you!</li>
    <li>I really love what you made here! Thanks again!</li>
  </ul>
</blockquote>

<p>The only thing I really found missing was some form of logging, which I made an attempt at while writing this post.</p>

<h2 id="logging">Logging</h2>

<p>I was looking to log two things - the number of challenge attempts and the number of solves. Nothing too fancy, just everything in one log file would also work.</p>

<h3 id="challenge-attempts">Challenge attempts</h3>
<p>SSH logs to <code>/var/auth/auth.log</code> by default. However, that I wasn't seeing any log file when I ran my setup. At the time, I missed a crucial detail - my container was set to read-only, so the log file wasn't being created.</p>

<p>To solve this, I added the <code>-e</code> flag to the Dockerfile command, which prints the logs to <code>stderr</code> instead of a file:</p>

<pre><code>CMD ["/usr/bin/sshd", "-e", "-D"]</code></pre>

<h3 id="challenge-solves">Challenge solves</h3>
<p>I started thinking of a backup to reliably track submissions, in case someone solved it but forgot to send me a message. I figured logging the number of times the file was accessed could be a potential solution.</p>

<p>I came across a <a href="https://stackoverflow.com/a/56774750">StackOverflow answer</a> for the same, which used <code>inotifywait</code> to watch at the flag file, and performs commands when the file is opened. I looked up the <code>man</code> page and changed the syntax according to what I was looking for.</p>

<pre><code>inotifywait -e open .secret/flag.txt -m --format "[%T] %w %e" --timefmt "%F %T"</code></pre>

<p>The command watches for the flag file to open, and prints output in the following format when the file is accessed:</p>

<pre><code>[2023-10-11 13:30:00] .secret/flag.txt OPEN</code></pre>

<p>It would run in the background, as the SSH server is running in the foreground. Since there are multiple shell commands to be executed at build time, using an entrypoint script, like in the <code>xinetd</code> setup makes more sense. This would run the <code>inotifywait</code> command as a background process and <code>sshd</code> as a server.</p>

<pre><code>#!/bin/bash

inotifywait -e open .secret/flag.txt -m --format "[%T] %w %e" --timefmt "%F %T" &amp;
/usr/sbin/sshd -D -e</code></pre>

<p>The Dockerfile would also change accordingly.</p>

<pre><code>COPY entrypoint.sh /entrypoint.sh
RUN chmod 551 /entrypoint.sh
CMD ["/entrypoint.sh"]</code></pre>

<p>This doesn't account for multiple flag reads by the same user, but some manual filtering of the logs could solve the issue in my case.</p>

<h3 id="accessing-logs">Accessing logs</h3>
<p>The logs are accessible using the <code>docker logs</code> command:</p>

<pre><code># docker logs &lt;containerid&gt;</code></pre>

<p>During the challenge, appending the <code>-f</code> file to command shows log entries as they appear. Once the challenge is over and I've stopped the container, I can redirect the output of the command (without <code>-f</code>) to a file.</p>

<p>This would contain a combination of SSH logs and the file access logs, which I could filter out using tools like <code>grep</code>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Given the timeframe of two days, I'm honestly surprised I was even able to do this much. I'm glad to be a part of a community like RC, that encourages people to try things they otherwise wouldn't.</p>

<p>I hope you enjoyed reading this, and if you do try out the challenge, feel free to leave a comment with any feedback!</p>
<h2 id="notes">Notes</h2>
  <ol class="footnotes" role="doc-endnotes">
    <li id="fn:1" role="doc-endnote">
      <p>It's a day to work on something that is well beyond what you're currently capable of. The idea is to take the first step towards doing something seemingly impossible, and seeing how far you can get. It's been one of the highlights of my RC experience.&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>However, messing up the challenge files for someone else or escaping the container due to a vulnerability is a possibility. Solutions to this include <a href="https://github.com/google/nsjail"><code>nsjail</code></a>, which spawns individual read-only instances whenever someone accesses a challenge. It seemed extra for my use case, but I found the concept very cool.&nbsp;<a class="reversefootnote" href="#fnref:2" role="doc-backlink">â†©</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>While writing this post, I happened to test Wordle Bash with the final SSH setup and it works just fine, probably an issue specific to <code>xinetd</code> then.&nbsp;<a class="reversefootnote" href="#fnref:3" role="doc-backlink">â†©</a></p>
    </li>
  </ol>

]]>
</content>

<category term="ctf" />

<category term="docker" />

<category term="python" />

<category term="recurse" />

</entry>
<entry>
<title type="html">RC04: Shell Scripting Adventures</title>
<link href="https://pjg1.site/rc04.html" rel="alternate" type="text/html" title="RC04: Shell Scripting Adventures" />
<published>2023-09-30T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/rc04.html</id>
<content type="html" xml:base="https://pjg1.site/rc04.html">
<![CDATA[<p>One of the things that helped me rediscover the joy of programming was making tiny shell scripts/functions for a specific purpose.</p>

<p>As I started using the command line more often, these functions helped execute commands that I would otherwise have to type repeatedly. And that set off a chain reaction - the more functions I made, the more ideas I'd get.</p>

<p>The first one I made was <a href="https://github.com/pjg11/scripts/tree/main/tzc"><code>tzc</code></a>, to convert time from another timezone to my local time. It was inspired by two things:</p>
<ul>
  <li>The RC application process, where all the interview timings were displayed in EST, and I needed a way to convert timings quickly, and</li>
  <li><a href="https://lazybear.io/posts/cli-of-the-week-19/">CLI of the week #19</a>, a post that introduced me to the idea of tiny shell functions.</li>
</ul>

<p>Things I like about making tiny functions:</p>
<ul>
  <li><a href="https://blubsblog.bearblog.dev/i-am-the-only-user/">I am the only user</a>, so I make them in a way that makes sense to me (bonus points if it makes sense to others too).</li>
  <li>It takes less time to go from idea to execution.</li>
  <li>It's a great learning opportunity, as I end up learning something new about shell scripting concepts or the thing I'm making the script for.</li>
</ul>

<p>I made a couple of functions while at RC - particularly over the last two weeks - which I'll share in this post.</p>

<h2 id="note"><code>note</code></h2>

<p>I came across <a href="https://github.com/cdkini/quicknote"><code>quicknote</code></a> recently, and really liked the way it was structured. It made it easier to capture notes and search them, so I made its <code>zsh</code> equivalent:</p>

<pre><code>#!/bin/zsh
note() (
    dir=$HOME/notes
    cd $dir

    case $1 in
        "open")    shift ; micro $@ ;;
        "ls")      fd . $dir --type file | cut -d'/' -f5- ;;
        "grep")    rg $2 ;;
        "rm")      shift ; rm $@ ;;
        *)         _usage ; return ;;
    esac
)</code></pre>

<p>A neat trick I learnt while making this is writing the function block in regular braces <code>()</code> - which runs commands in a subshell as opposed to curly braces <code>{}</code> - to prevent changing the directory of my current shell session.</p>

<p>The full code with error handling and usage information can be found <a href="https://github.com/pjg11/scripts/blob/a3d5993b8cf75f7ad50b684154735c8ab187ab10/note">here</a>.</p>

<h2 id="blog"><code>blog</code></h2>

<p>This function is my take on a basic CLI content management system.</p>

<pre><code>$ blog -h
blog - commands to manage your Jekyll blog

Usage:
  blog command [args]

Options:
  -h|--help                  Show this message and exit

Commands:
  new filename               Create a new post
  update commitmsg           Build and deploy changes to existing content
  publish filename           Publish post now
  publish filename time      Schedule post for a later time
  preview                    View recent post/draft locally
  preview all                View entire website locally</code></pre>

<p>These commands have existed as a bunch of separate shell scripts and aliases. Some of them have featured on this blog - a <a href="/jekyll-blog-setup#publish-your-website#testing-the-script">publish script</a> and a post on <a href="/schedule-jekyll">scheduling</a>.</p>

<p>The upgraded function code can be found <a href="https://github.com/pjg11/scripts/blob/a3d5993b8cf75f7ad50b684154735c8ab187ab10/blog">here</a>.</p>

<h2 id="rlist"><code>rlist</code></h2>

<p>I use Safari's Reading List to save links, and I wanted a way to manage them from the command line, particularly to view all links and delete a link once I'm done reading.</p>

<pre><code>$ rlist -h
rlist - Manage Safari's Reading List from the command line

Usage:
  rlist [options] command

Options:
  -h|--help    Show this message and exit

Commands:
  init         Extracts links and saves to $HOME/notes/rlist.tsv
  search       Search all links and open chosen link in browser
  delete       Delete chosen link from list</code></pre>

<p>The Reading List is stored in <code>~/Library/Safari/Bookmarks.plist</code>, and the function uses a utility called <a href="https://www.marcosantadev.com/manage-plist-files-plistbuddy/"><code>PlistBuddy</code></a> to interact with the file.</p>

<p>Here's a demo of the search command, that works like a fuzzy finder, along with the option to open the link in a browser.</p>

<figure>
<a href="https://asciinema.org/a/674173" target="_blank"><img alt="Thumbnail of an asciinema recording of the rlist's searching functionality" loading="lazy" src="https://asciinema.org/a/674173.svg"></a>
<figcaption>Click to view on asciinema</figcaption>
</figure>
<p>This is possibly the coolest script I've made yet, as it took a while to understand how <code>PlistBuddy</code> works, and using <a href="https://github.com/charmbracelet/gum"><code>gum</code></a> for the fancy formatting added to the coolness.</p>

<h2 id="mkfunc"><code>mkfunc</code></h2>

<p>Lastly, a meta function whose naming is inspired by <code>mkdir</code>. It creates a file with starter code for a shell function when called.</p>

<pre><code>#!/bin/zsh

mkfunc() {

    if [[ ( $# -eq 0 ) || ( "$*" =~ "-h" ) || ("$*" =~ "--help") ]] ; then
            echo "Usage: mkfunc name"
            return
    fi

    tee $HOME/.oh-my-zsh/functions/$1 &lt;&lt; 'EOF'
#!/bin/zsh

func() {

    _usage() {
        # Add help text here
    }

    if [[ ( $# -eq 0 ) || ( "$*" =~ "-h" ) || ( "$*" =~ "--help") ]] ; then
        _usage
        return
    fi

    # Remove this block if function contains no subcommands
    case $1 in
        # Add other cases here
        *) echo "invalid option - $1" ; _usage ;;
    esac
}
EOF
    chmod u+x $1
    $EDITOR $1
}</code></pre>

<p>This function contains simpler help text, as there are no additional commands. The heredoc delimiter <code>EOF</code> is wrapped with single quotes to prevent execution of the variables within the heredoc (<code>$*</code>, <code>$@</code>,<code>$1</code>).</p>

<hr>

<p>Thanks for tuning in to another episode of "things I didn't plan to work on"! It's interesting how I get more post ideas from side quests rather than the main ones ğŸ˜…</p>

<p>You can find all the above scripts (and more, whenever I make them) at <a href="https://github.com/pjg11/scripts">pjg11/scripts</a>.</p>

]]>
</content>

<category term="recurse" />

<category term="scripting" />

</entry>
<entry>
<title type="html">RC03: The Curious Case of the Inconsistent Byte Order</title>
<link href="https://pjg1.site/rc03.html" rel="alternate" type="text/html" title="RC03: The Curious Case of the Inconsistent Byte Order" />
<published>2023-09-18T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/rc03.html</id>
<content type="html" xml:base="https://pjg1.site/rc03.html">
<![CDATA[<p>This week, I started working on building a <a href="https://github.com/pjg11/tuntcp">TCP/IP stack from scratch in C</a>.</p>

<p>Part of the IP implementation is calculating the checksum, which requires splitting the IP header into 16-bit chunks. I'll be using these hex bytes from an IP packet as an example</p>

<pre><code>45 00 00 14 00 01 00 00 40 06 00 00 0a 00 02 0f d0 5e 75 2b</code></pre>

<p>which should be converted into the following chunks</p>

<pre><code>4500 0014 0001 0000 4006 0000 0a00 020f d05e 752b</code></pre>

<p>Sounds simple, right? I thought so too, but not when C code is involved! Things didn't work as I expected them to, and I decided to start an investigation.</p>

<h2 id="initial-assessment">Initial assessment</h2>

<p>Here's the program I was working with. It first creates an IP packet, then displays the packet as 16-bit chunks.</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;arpa/inet.h&gt;

struct ipv4 {
    uint8_t     version_ihl;
    uint8_t     tos;
    uint16_t    len;
    uint16_t    id;
    uint16_t    frag_offset;
    uint8_t     ttl;
    uint8_t     proto;
    uint16_t    checksum;
    uint32_t    src;
    uint32_t    dst;
};

int main(void) {
    struct ipv4 * ip = calloc(1, sizeof(struct ipv4));
    uint32_t saddr, daddr;
    inet_pton(AF_INET, "10.0.2.15", &amp;saddr);
    inet_pton(AF_INET, "208.94.117.43", &amp;daddr);

    ip-&gt;version_ihl = 4 &lt;&lt; 4 | 5;
    ip-&gt;tos = 0;
    ip-&gt;len = 20;
    ip-&gt;id = 1;
    ip-&gt;frag_offset = 0;
    ip-&gt;ttl = 6;
    ip-&gt;proto = 6;
    ip-&gt;checksum = 0;
    ip-&gt;src = saddr;
    ip-&gt;dst = daddr;

    uint16_t *p = (uint16_t *) ip;
    size_t count = sizeof(*ip);

    while (count &gt; 1)  {
        printf("%.4x ", *p++);
        count -= 2;
    }
    printf("\n");

    return 0;
}</code></pre>

<p>Andâ€¦that didn't go as I hoped.</p>

<pre><code>$ gcc -o checksum checksum.c
$ ./checksum
0045 0014 0001 0000 0640 0000 000a 0f02 5ed0 2b75</code></pre>

<p>Chunks 2, 3, 4 and 6 look correct, but the other chunks display individual bytes in the opposite order.</p>

<h2 id="the-prime-suspect---byte-order">The prime suspect - Byte Order</h2>

<p>I suspected this to have something to do with the byte order - the way in which our machines stores bytes.</p>

<p>I explain these terms briefly in the following sections, but I would also recommend reading the <a href="https://beej.us/guide/bgnet/html/#byte-order">Byte Order section</a> from Beej's Guide to Network Programming, an excellent resource for low-level network stuff.</p>

<p>I assigned all variables in the way I usually would, so I assumed that the machine would store and print all values in the same order.</p>

<p>But seeing an inconsistent byte order in the output instead confused me, which made the investigation very interesting.</p>

<h2 id="trial-and-error">Trial and error</h2>

<p>I started with changing the printing section of the code to display individual bytes instead of chunks.</p>

<pre><code>uint8_t *p = (uint8_t *) ip;
size_t count = sizeof(*ip);

while (count &gt; 1) {
    printf("%.2x ", *p++);
    count -= 1;
}

printf("\n");</code></pre>

<p>The output confused me even more.</p>

<pre><code>$ gcc -o checksum checksum.c
$ ./checksum
45 00 14 00 01 00 00 00 40 06 00 00 0a 00 02 0f d0 5e 75 2b</code></pre>

<p>This looks closer to the header at the start of the post, but bytes 3-6 are written in the wrong order. The correct order would be <code>00 14 00 01</code>.</p>

<p>Additionally, the bytes are displayed in the exact opposite order of the chunks output. What? Why?</p>

<p><em>*sigh*</em></p>

<p>Let's put some questions in place to guide the investigation:</p>
<ol>
  <li>Why are bytes in the chunks output reversed?</li>
  <li>Why does the bytes output have only some bytes printed in the opposite order?</li>
</ol>

<h2 id="getting-somewhere">Getting somewhere</h2>

<p>The answer to the first question is an example I remembered, from the book <a href="https://nostarch.com/hacking2.htm">Hacking: The Art of Exploitation</a>. It prints the contents of a register in different ways in <code>gdb</code>. Here's a modified version of it:</p>

<pre><code>(gdb) x/xh $eip
0x8048384 &lt;main+16&gt;: 0x45c7
(gdb) x/2xb $eip
0x8048384 &lt;main+16&gt;: 0xc7 0x45</code></pre>

<p>The second command is the individual byte representation of the first command, the same as my case. The book also explains why this happens - the byte order, my prime suspect.</p>

<p>The little-endian order, which is the byte order of my machine (and most machines), stores bytes in the reverse order to what we think they would store.</p>

<p>Keeping the above example in mind, it is common to think that the value <code>0x45c7</code> would be stored as the bytes <code>45 c7</code> in memory. But nope, my machine stores the value as <code>c7 45</code> instead, which is the output of the second command.</p>

<p>If they were stored in the opposite order, they would have to be read in the opposite order too. So the bytes are read from right to left, and hence displayed in the reverse order when printed as a chunk.</p>

<p>Okay, one question solved, one more to go!</p>

<h2 id="the-case-gets-complicated">The case gets complicated</h2>

<p>The second question was not as easy to answer.</p>

<p>The bytes in question, bytes 3-6 of the header, correspond to the <code>len</code> and <code>id</code> fields of the IP header. Let's start with the declaration of these fields:</p>

<pre><code>uint16_t    len;
uint16_t    id;</code></pre>

<p>They're both 16 bits or 2 bytes in size. Here's how they were assigned:</p>

<pre><code>ip-&gt;len = 20;
ip-&gt;id = 1;</code></pre>

<p>As little-endian is the default byte order of the machine, it applies to these assignments too. So the value 20 is represented as hex bytes <code>14 00</code> in memory, while the value 1 corresponds to the hex bytes <code>01 00</code>.</p>

<p>Now this is a problem, as network programs store and read data from left to right, like we do. That order is called the big-endian order.</p>

<p>To solve this, we would need to switch the byte order from little to big-endian, which can be done with a function called <a href="https://beej.us/guide/bgnet/html/#htonsman"><code>htons()</code></a> or <strong>h</strong>ost <strong>to</strong> <strong>n</strong>etwork <strong>s</strong>hort. Host Byte Order in this case is little-endian, while Network Byte Order is always big-endian.</p>

<p>This would solve the problem, but something doesn't feel right. Changing the byte order just for two fields doesn't make sense. <em>Why do the other values not require <code>htons()</code>?</em></p>

<h2 id="mystery-solved">Mystery solved</h2>

<p>It's something I struggled to figure even as I started typing this post, but I think I got it:</p>

<ul>
  <li>
    <p>The fields <code>version_ihl</code>, <code>tos</code>, <code>ttl</code>, and <code>proto</code> are 8 bits or 1 byte in size. Byte order matters for values greater than a byte, which isn't the case here. So no <code>htons()</code> required.</p>
  </li>
  <li>
    <p>The <code>src</code> and <code>dst</code> fields (32 bits each) are in big-endian order already, thanks to <a href="https://beej.us/guide/bgnet/html/#inet_ntopman"><code>inet_pton()</code></a>. No <code>htons()</code> required here either.</p>
  </li>
  <li>
    <p>The <code>frag_offset</code> and <code>checksum</code> fields were a big reason behind the confusion. They're the same size as <code>len</code> and <code>id</code>, but were intialized to 0. As the bytes <code>00 00</code> are the same as <code>00 00</code><sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup>, it looked like they displayed correctly. <code>htons()</code> would be required for these fields.</p>
  </li>
</ul>

<p>And now, it all makes sense. The byte order needs to be switched for all fields greater than a byte.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Based on the above findings, I made the following changes, making the byte order consistent across the struct.</p>

<pre><code>ip-&gt;version_ihl = 4 &lt;&lt; 4 | 5;
ip-&gt;tos = 0;
ip-&gt;len = htons(20);
ip-&gt;id = htons(1);
ip-&gt;frag_offset = htons(0);
ip-&gt;ttl = 64;
ip-&gt;proto = 6;
ip-&gt;checksum = htons(0);
ip-&gt;src = saddr;
ip-&gt;dst = daddr;</code></pre>

<p>With that set, the bytes print correctly!</p>

<pre><code>$ gcc -o checksum checksum.c
$ ./checksum
45 00 00 14 00 01 00 00 40 06 00 00 0a 00 02 0f d0 5e 75 2b</code></pre>

<p>I changed the printing section to display chunks again, and they look fine too.</p>

<pre><code>$ gcc -o checksum checksum.c
$ ./checksum
0045 1400 0100 0000 0640 0000 000a 0f02 5ed0 2b75</code></pre>

<p>I was wondering if this swapped order of chunks would affect the resulting checksum. According to <a href="https://www.rfc-editor.org/rfc/rfc1071#page-3">RFC 1071</a>, it doesn't:</p>

<blockquote>
<p>Therefore, the sum may be calculated in exactly the same way
regardless of the byte order ("big-endian" or "little-endian")
of the underlaying hardware.  For example, assume a "little-
endian" machine summing data that is stored in memory in network
("big-endian") order.  Fetching each 16-bit word will swap
bytes, resulting in the sum [4]; however, storing the result
back into memory will swap the sum back into network byte order.</p>
</blockquote>

<p>Interesting! I didn't know little-endian byte order worked like this.</p>

<p>With that, I declare this investigation complete. Detective Piya, signing off!</p>

<h2 id="the-mystery-was-already-solved">The mystery was already solved?</h2>

<p>Just as I was finishing up this post, I read through the <a href="https://beej.us/guide/bgnet/html/#byte-order">Byte Order section</a> in Beej's Guide again, where I found this:</p>

<blockquote>
<p>A lot of times when you're building packets or filling out data structures you'll need to make sure your two- and four-byte numbers are in Network Byte Order.</p>
</blockquote>

<p>Oh well, the solution was here all along. Regardless, I enjoyed sharpening my debugging skills by figuring things out step by step (and making this post, too!).</p>

<h2 id="notes">Notes</h2>

<ol class="footnotes" role="doc-endnotes">
<li id="fn:1" role="doc-endnote">
<p>Which one refers to which byte order is totally up to you :P <a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
 </li>
</ol>

]]>
</content>

<category term="c" />

<category term="recurse" />

<category term="networking" />

</entry>
<entry>
<title type="html">RC02: Extending filtering capabilities in Bubble Tea apps</title>
<link href="https://pjg1.site/rc02.html" rel="alternate" type="text/html" title="RC02: Extending filtering capabilities in Bubble Tea apps" />
<published>2023-09-11T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/rc02.html</id>
<content type="html" xml:base="https://pjg1.site/rc02.html">
<![CDATA[<p>In Week 1 of RC, I started building a todo app. It's built using <a href="https://github.com/charmbracelet/bubbletea">Bubble Tea</a>, a TUI framework written in Go.</p>

<p>The app uses the <a href="https://github.com/charmbracelet/bubbles/#list">List component</a> as its base, which comes from Bubbles, a repo containing TUI components for Bubble Tea.</p>

<p>One of the features it provides is filtering items. Filtering works by pressing the <kbd>/</kbd> key, followed by entering the text of choice. The filter value, i.e., what field to filter can be declared in the code.</p>

<p>In the case of the <a href="https://github.com/charmbracelet/bubbletea/tree/master/examples/list-default">default list</a> (what I used for this app), it can be either the Title or the Description. I went ahead with the description, where I store the status of a task.</p>

<p>The app takes the tasks from a plaintext file. Each task in the app can be written in one of these formats.</p>

<pre><code>- Todo
* Today
/ Waiting
X Done
! Idea 
&lt; Archive</code></pre>

<p>The description stores textual versions of these symbols, and that's what used as the filter value.</p>

<p>Since the filter values are limited, I wanted a way to access these filters faster. Rather than having to type <code>/today</code> for tasks tagged Today, I wanted to press <kbd>1</kbd>. Similarly, I wanted the key <kbd>2</kbd> for Todo, <kbd>3</kbd> for Waiting and so on.</p>

<p>I set out to look for if there was a function that did this. The first place I checked was the Issues tab on GitHub to see if others had asked for something similar.</p>

<p>While there is no built-in function to do this, there are pull requests that offer functions to be added to the source code. <a href="https://github.com/charmbracelet/bubbles/pull/335">This pull request</a> has been around for a while, but hasn't been merged to the main source code for some reason.</p>

<h2 id="attempt-1-use-a-modified-local-bubbles-instance">Attempt #1: Use a modified, local Bubbles instance</h2>

<p>I was impatient and really wanted to implement this feature in my app. So I cloned the entire Bubbles repository to my directory.</p>

<p>I specifically cloned the user's fork, so I could use the newly created function, <code>SetFilterText()</code>. The code was located in the branch called <code>feat-prefilter-opt</code>.</p>

<pre><code>$ git clone https://github.com/taigrr/bubbles
$ cd bubbles &amp;&amp; git checkout remotes/origin/feat-prefilter-opt</code></pre>

<p>I implemented the core logic in my app's <code>Update()</code> function, which handles keypresses.</p>

<pre><code>switch msg := msg.(type) {
case tea.KeyMsg:
    switch msg.String() {
    case "1":
        m.list.SetFilterText("today")
    case "2":
        m.list.SetFilterText("todo")
    case "3":
        m.list.SetFilterText("waiting")
    case "4":
        m.list.SetFilterText("done")
    case "5":
        m.list.SetFilterText("idea")
    case "6":
        m.list.SetFilterText("archive")
    }
}</code></pre>

<p>Next, I needed to create keybindings for the keys above, so that they show up in the help section of the app.</p>

<p>I created keybindings for the keys <kbd>1</kbd> - <kbd>6</kbd> and a combined key called <code>Section</code> using the following code:</p>

<pre><code>type listKeyMap struct {
    Section key.Binding
    Today   key.Binding
    Todo    key.Binding
    Waiting key.Binding
    Done    key.Binding
    Idea    key.Binding
    Archive key.Binding
}

func newListKeyMap() *listKeyMap {
    return &amp;listKeyMap{
        Section: key.NewBinding(
            key.WithKeys("1-6"),
            key.WithHelp("1-6", "section"),
        ),
        Today: key.NewBinding(
            key.WithKeys("1"),
            key.WithHelp("1", "today"),
        ),
        Todo: key.NewBinding(
            key.WithKeys("2"),
            key.WithHelp("2", "todo"),
        ),
        Waiting: key.NewBinding(
            key.WithKeys("3"),
            key.WithHelp("3", "waiting"),
        ),
        Done: key.NewBinding(
            key.WithKeys("4"),
            key.WithHelp("4", "done"),
        ),
        Idea: key.NewBinding(
            key.WithKeys("5"),
            key.WithHelp("5", "idea"),
        ),
        Archive: key.NewBinding(
            key.WithKeys("6"),
            key.WithHelp("6", "archive"),
        ),
    }
}</code></pre>

<p>The program has two help sections - a short help displayed below the app, and a full help that can be accessed by pressing the <kbd>?</kbd> key.</p>

<p>The <code>Section</code> key is created for the short help, to avoid making the short help too long. Each of the individual section keys go in the full help. This code goes in the app's <code>newModel()</code> function, that builds the list.</p>

<pre><code>list.AdditionalFullHelpKeys = func() []key.Binding {
    return []key.Binding{
        listKeys.Today,
        listKeys.Todo,
        listKeys.Waiting,
        listKeys.Done,
        listKeys.Idea,
        listKeys.Archive,
        listKeys.Edit,
    }
}
list.AdditionalShortHelpKeys = func() []key.Binding {
    return []key.Binding{
        listKeys.Section,
    }
}

return model{
    list: list,
    keys: listKeys,
}</code></pre>

<p>One last change I had to make was to add the following line to the <code>go.mod</code> file to use the local import.</p>

<pre><code>replace github.com/charmbracelet/bubbles =&gt; ./bubbles</code></pre>

<p>Andâ€¦it worked!</p>

<figure>
<a href="https://asciinema.org/a/674186" target="_blank"><img alt="Thumbnail of an asciinema recording of the todo application - showing the various filtering options and the help menu to show the keybindings" loading="lazy" src="https://asciinema.org/a/674186.svg"></a>
<figcaption>Click to view on asciinema</figcaption>
</figure>
<p>Now I could have stopped here. But having the entire Bubbles package as part of my repo made it unnecessarily bulky. There had to be another way, a way which didn't require a local copy.</p>

<p>I tried adding <code>SetFilterText()</code> to my <code>main.go</code> file, but it accessed private variables and functions of the List component, which I couldn't access.</p>

<h2 id="an-undocumented-workaround">An undocumented workaround</h2>

<p>The same pull request also contains the following text:</p>

<blockquote>
<p>A workaround of using Program.Send works, but is limiting.</p>
</blockquote>

<p>I got curious as to what this hack was. From an <a href="https://github.com/charmbracelet/bubbletea/tree/master/examples/send-msg">example</a> provided by Bubble Tea, <code>Program.Send()</code> can be used to send messages from outside of the program.</p>

<p>The term "workaround" suggests that there could be a way to send keypresses from within the program through this function.</p>

<p>The pull request had no sample code to show how it works though, so I was left to figure it out on my own.</p>

<p>I read through the documentation and tried to put something together, but thanks to the combination of working with a new language (Go) and a new code structure, I couldn't get it to work.</p>

<h2 id="recursers-to-the-rescue">Recursers to the rescue</h2>

<p>Around the same time, <a href="https://the.scapegoat.dev">Manuel</a> - a Recurse alum<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup> - shared progress on his project, that also used Bubble Tea! I reached out to him for help, and we decided to figure this out together in a pairing session.</p>

<p>A few days later, we got on call, and actually got it to work. He did most of the figuring out, so here's my attempt of explaining the concept by showing how I implemented it in the app.</p>

<h2 id="proof-of-concept">Proof of concept</h2>

<p>Let's start with the documentation for <code>Program.Send</code>, which contains the function declaration:</p>

<pre><code>func (p *Program) Send(msg Msg)</code></pre>

<p>It takes an input of type <code>Msg</code>, which is the message to be sent. I need a way to send keys to the program. For this, there is a type of <code>Msg</code> called <code>KeyMsg</code>.</p>

<pre><code>type KeyMsg Key

type Key struct {
    Type  KeyType
    Runes []rune
    Alt   bool
}</code></pre>

<p><code>KeyMsg</code> is of type <code>Key</code> which requires one or two parameters:</p>
<ul>
  <li><code>Type</code> which is the type of key, which could be one of the constant values defined <a href="https://pkg.go.dev/github.com/charmbracelet/bubbletea#KeyType">here</a>. If the type is set to <code>KeyRunes</code>, then the next parameter is required.</li>
  <li><code>Runes</code> is an array of the type <code>rune</code>, a Go type that stores characters as 32-bit integers. This can be used to send keys not described in the constant values above or multiple keys at once.</li>
</ul>

<p>Putting together the learnings from the pairing session and the docs, here's the syntax for sending a key press via <code>Program.Send()</code>:</p>

<pre><code>p.Send(tea.KeyMsg(tea.Key{Type: tea.CtrlC}))
p.Send(tea.KeyMsg(tea.Key{Type: tea.KeyRunes, Runes: []rune("q")}))</code></pre>

<p>Both examples cause the program to quit. The first example sends the key <kbd>Ctrl</kbd> + <kbd>C</kbd>, while the second sends the <kbd>Q</kbd> key using the rune syntax.</p>

<p>Having this figured out, I made my own function, called <code>SetFilter()</code></p>

<h2 id="attempt-2-setfilter">Attempt #2: <code>SetFilter()</code></h2>

<pre><code>/* Custom function to switch between the different categories. */
func (m model) SetFilter(s string) {
    go func() {
        if m.list.IsFiltered() {
            p.Send(tea.KeyMsg(tea.Key{Type: tea.KeyEsc}))
        }
        p.Send(tea.KeyMsg(tea.Key{Type: tea.KeyRunes, Runes: []rune{'/'}}))
        p.Send(tea.KeyMsg(tea.Key{Type: tea.KeyRunes, Runes: []rune(s)}))
        time.Sleep(2 * time.Millisecond)
        p.Send(tea.KeyMsg(tea.Key{Type: tea.KeyEnter}))
    }()
}</code></pre>

<p>Lines 7-10 are the core of this function:</p>
<ul>
  <li>It starts by sending the <kbd>/</kbd> key, which enables filtering mode.</li>
  <li>Then the filter text is sent, stored in <code>s</code>.</li>
  <li>After a delay of 2 milliseconds (required to update the pagination), the <kbd>Enter</kbd> key is sent to apply the filter.</li>
</ul>

<p>Lines 4-6 check if a filter is currently set, in which case the <kbd>Esc</kbd> key is sent to reset the filter before switching to another one.</p>

<p>Lastly, all the statements are wrapped in a goroutine. This is to ensure that neither of these commands can block the entire program.</p>

<p>With the function complete, I changed the <code>Update()</code> function.</p>

<pre><code:
    m.SetFilter("today")
case "2":
    m.SetFilter("todo")
case "3":
    m.SetFilter("waiting")
case "4":
    m.SetFilter("done")
case "5":
    m.SetFilter("idea")
case "6":
    m.SetFilter("archive")</code></pre>

<p>Lastly, I declared the Program <code>p</code> as a global variable, so that <code>SetFilter()</code> could access it.</p>

<h2 id="limitations">Limitations</h2>

<ol>
  <li>
    <p>The app can flicker sometimes when switching filters, due to the 2 millisecond delay in the function.</p>
  </li>
  <li>
    <p><a href="#attempt-1-use-a-modified-local-bubbles-instance">Attempt #1</a> displayed a "0 items found" screen when there were no items for a given filter. This happens due to the function setting a state called <code>FilterApplied</code>. The actual behavior for this case is different though (exits filter mode and displays all items) which is how <a href="#attempt-2-setfilter">Attempt #2</a> works. I prefer the first approach, but haven't found a way to implement it yet.</p>
  </li>
</ol>
<h2 id="notes">Notes</h2>
  <ol class="footnotes" role="doc-endnotes">
    <li id="fn:1" role="doc-endnote">
      <p>I came across his blog posts from Bearblog's <a href="http://bearblog.dev/discover">discovery feed</a>, and I had no idea he was an RC alum before joining. Never thought I'd actually get to meet him, so pairing with him was totally unexpected and a really cool experience!&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
    </li>
  </ol>

]]>
</content>

<category term="bubbletea" />

<category term="golang" />

<category term="recurse" />

<category term="tui" />

</entry>
<entry>
<title type="html">RC01: Some InconsistenC's</title>
<link href="https://pjg1.site/rc01.html" rel="alternate" type="text/html" title="RC01: Some InconsistenC's" />
<published>2023-08-19T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/rc01.html</id>
<content type="html" xml:base="https://pjg1.site/rc01.html">
<![CDATA[<p>I came across an older CTF writeup of mine - <a href="https://github.com/pjg11/CTF-Writeups/blob/main/2021-Space-Race/3-Space-Snacks.md#50-points-there-might-be-more-cake">There might be more cake</a> from Hacky Holidays 2021. The solution for it was this C program:</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;time.h&gt;

int generatePin() {
    srand(1893497025);
    return rand();
}

int main() {
    printf("%d\n", generatePin());
    return 0;
}</code></pre>

<div class="note">
<p><strong>Note:</strong> Shell commands preceeded with a <code>$</code> run on a Linux machine, while commands preeceded with <code>%</code> run on a macOS machine.</p>
</div>

<p>At the time, I tried running this on my machine (macOS) and received the wrong output.</p>

<pre><code>% gcc -o pin pin.c
% ./pin
444334282</code></pre>

<p>One of my teammates happened to solve the challenge. Our programs were the same, but he solved it on a Linux machine.</p>

<pre><code>$ gcc -o pin pin.c
$ ./pin
1376299761</code></pre>

<p>Same code, different output? I think we attributed the issue to some differences in implementations on different operating systems, I'm not sure. Regardless, I left it there and didn't look into it further.</p>

<p>However, upon seeing the writeup recently, I got curious and decided to figure out why this happened, and if there was a way to get the correct output on macOS.</p>

<h2 id="gcc-isnt-the-issue"><code>gcc</code> isn't the issue</h2>

<p>During the search process, I was shocked to find out that Apple actually executes <code>clang</code> whenever you type <code>gcc</code> in the terminal.</p>

<pre><code>% gcc
clang: error: no input files
% gcc --version
Apple clang version 13.1.6 (clang-1316.0.21.2.5)
Target: arm64-apple-darwin21.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin</code></pre>

<p>Thinking that this was the issue, I installed <code>gcc</code> on my machine via Homebrew to have the same compiler on both operating systems.</p>

<pre><code>% brew install gcc</code></pre>

<p>The Homebrew version of <code>gcc</code> contains the version number (<code>gcc-13</code> in this case), most likely to prevent conflict with the built-in <code>gcc</code> executable.</p>

<pre><code>% gcc-13 --version
gcc-13 (Homebrew GCC 13.1.0) 13.1.0
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</code></pre>

<p>I thought this would solve my issue, but nope, the output is the same.</p>

<pre><code>% gcc-13 -o pin pin.c
% ./pin
444334282</code></pre>

<h2 id="the-libc-looks-different">The libc looks different</h2>

<p>I was reading through a post from Julia Evans before starting RC - <a href="https://jvns.ca/blog/2023/08/03/behind--hello-world">Behind Hello World on Linux</a> - which briefly mentions about libc, the C standard library. These lines stood out:</p>

<blockquote>
<p>But there are different libc implementations, and sometimes they behave differently. The two main ones are glibc (GNU libc) and musl libc.</p>
</blockquote>

<p>The <code>srand()</code> and <code>rand()</code> functions come from this library, so maybe they're implemented differently in macOS and Linux. This explains why the previous step to change my compiler didn't work.</p>

<p>My Linux VM uses glibc, which I can confirm by running <code>ldd</code> on the binary.</p>

<pre><code>$ ldd pin
    linux-vdso.so.1 (0x00007ffcb23d4000)
    libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbc44b9f000)
    /lib64/ld-linux-x86-64.so.2 (0x00007fbc44d9b000)</code></pre>

<p>This commands checks for the libraries that a binary needs to load at runtime. <code>libc.so.6</code> is the glibc file.</p>

<p>For macOS, I used the command <code>otool -L</code> as suggested by Julia at the end of her post.</p>

<pre><code>% otool -L pin
pin:
    /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1311.100.3)</code></pre>

<p>In macOS, libc is part of Apple's system library, called <a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/intro.3.html">libSystem</a>. It seems to be Apple's own implementation, as I found a link to the <a href="https://opensource.apple.com/source/Libc/">libc source code</a> while searching.</p>

<p>So different implementations mean there is no way I could get the same output on both Linux and Mac? That doesn't seem right.</p>

<h2 id="meet-srandom-and-random">Meet <code>srandom()</code> and <code>random()</code></h2>

<p>I came across a pair of similar functions in my searches - <code>srandom()</code> and <code>random()</code>. In their <a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/random.3.html">man page</a> on macOS, they're termed as the "better random number generator".</p>

<p>I replaced the functions in my code on macOS, and now I get the correct output!</p>

<pre><code>% gcc-13 -o pin pin.c
% ./pin
1376299761</code></pre>

<p>So how is it better? According to the man page, while <code>rand()</code> and <code>random()</code> work in a similar way, certain implementations of <code>rand()</code> generate less random numbers:</p>

<pre><code>The random() and srandom() functions have (almost) the same calling
sequence and initialization properties as the rand(3) and srand(3)
functions.  The difference is that rand(3) produces a much less
random sequence â€” infact, the low dozen bits generated by rand go
through a cyclic pattern.  All of the bits generated by random()
are usable.  For example, 'random()&amp;01' will produce a random
binary value.</code></pre>

<p><code>srandom()</code> and <code>random()</code> are also not part of the <a href="https://www.csse.uwa.edu.au/programming/ansic-library.html#stdlib">C Standard</a>. They come from the <a href="https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/stdlib.h.html">POSIX Standard</a>, which contains additional functions for C.</p>

<p>Okay, so use <code>srandom()</code> instead of <code>srand()</code> and <code>random()</code> instead of <code>rand()</code> and problem solved, right? Technically yes, but I still had one more question.</p>

<p>I went ahead and tested the newly found functions on Linux, and they return the correct output. While this is expected, why does <code>srand()/rand()</code> return the incorrect output only on macOS?</p>

<table>
<thead>
<tr>
<th></th>
<th style="text-align: center"><code>srand()/rand()</code></th>
<th style="text-align: center"><code>srandom()/random()</code></th>
</tr>
</thead>
<tbody>
<tr>
<th style="text-align: center">macOS</th>
<td style="text-align: center">444334282</td>
<td style="text-align: center">1376299761</td>
</tr>
<tr>
<th style="text-align: center">Linux</th>
<td style="text-align: center">1376299761</td>
<td style="text-align: center">1376299761</td>
</tr>
</tbody>
</table>

<p>I think this is where the standards matter. Here's a snippet of the man pages for <code>srand()/rand()</code> from both operating systems:</p>

<figure>
<pre><code>STANDARDS
  The rand() and srand()
  functions conform to ISO/IEC
  9899:1990 ("ISO C90").</code></pre>
<figcaption><a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/srand.3.html">macOS</a></figcaption>
</figure>

<figure>
<pre><code>STANDARDS
  The functions rand() and
  srand() conform to SVr4,
  4.3BSD, C99, POSIX.1-2001.</code></pre>
<figcaption><a href="https://man7.org/linux/man-pages/man3/rand.3.html">Linux</a></figcaption>
</figure>

<p>Different standards likely mean different implementations, which explains the difference in output.</p>

<p>The standard mentioned in the macOS code, ISO C90, is an older standard, so the implementation might be outdated, and hence is declared obsolete in the man pages. This isn't the case for the Linux version though, which is interesting.</p>

<p>The man pages also seem to have quite a few differences in both OS's, the major one being that the macOS pages date back to the 1990s, while the Linux ones are from this year.</p>

]]>
</content>

<category term="c" />

<category term="recurse" />

</entry>
<entry>
<title type="html">RC00: Interesting Python behavior</title>
<link href="https://pjg1.site/rc00.html" rel="alternate" type="text/html" title="RC00: Interesting Python behavior" />
<published>2023-08-10T00:00:00+04:00</published>
<updated>2025-07-27T00:00:00+04:00</updated>
<id>https://pjg1.site/rc00.html</id>
<content type="html" xml:base="https://pjg1.site/rc00.html">
<![CDATA[<p>I attended a pair programming workshop as part of the RC orientation talks. After a brief introduction to pairing and how it works, the attendees were paired with each other to code <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway's Game of Life</a> within an hour.</p>

<p>My pairing partner, <a href="https://msabin.github.io">Manuel</a> and I decided to code in Python, the first step being to create a grid. For some reason, my mind came up with this line of code:</p>

<pre><code>grid=[['X']*10]*10</code></pre>

<p>I don't recall seeing something like this before, so it came as a bit of a surprise. It seemed like a very neat trick and I was proud of myself. On running the code, it worked as intended, which was even more surprising.</p>

<pre><code>$ python3 gameoflife.py
[['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']]</code></pre>

<p>The next stage was to set the initial state of the grid. This meant changing certain cells of the grid to another value, <code>O</code> in our case.</p>

<pre><code>grid[5][5] = 'O'
grid[4][5] = 'O'
grid[4][6] = 'O'</code></pre>

<p>I was sure this was going to work, but this happened instead.</p>

<pre><code>$ python3 gameoflife.py
[['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X'],
['X', 'X', 'X', 'X', 'X', 'O', 'O', 'X', 'X', 'X']]</code></pre>

<p>Instead of specific cells, entire columns were updated? Strange.</p>

<p>My attention went towards the multiply sign. <strong>Was it duplicating the same object instead of each row being a different object?</strong></p>

<p>Turns out, yes! However, I wasn't entirely convinced and needed some evidence to back this up. After the workshop I came across <a href="https://pythontutor.com/python-debugger.html">Python Tutor</a>, a tool to visualize Python code, and decided to try it out.</p>

<figure>
<img alt="There are three blocks - a global frame with the variable name &quot;grid&quot;. The variable name points to block representing an array of index 10. Every index of the array points to a third block - an array of index 10 with the string &quot;X&quot; written for each index." class="dim" loading="lazy" src="/public/images/rc00/viz1.png">
<figcaption><a href="https://pythontutor.com/render.html#code=grid%3D%5B%5B'X'%20for%20_%20in%20range%2810%29%5D%20for%20_%20in%20range%2810%29%5D&amp;cumulative=false&amp;curInstr=144&amp;heapPrimitives=nevernest&amp;mode=display&amp;origin=opt-frontend.js&amp;py=3&amp;rawInputLstJSON=%5B%5D&amp;textReferences=false">View on Python Tutor</a></figcaption>
</figure>

<p>The intended scenario would be to see 11 different arrays - the <code>grid</code> array, and 10 arrays containing 10 X's each. However, here we see 2 arrays - <code>grid</code> and one array with 10 X's, which I'll call <code>row</code> from now on.</p>

<p>Each index of <code>grid</code> points to <code>row</code>. So <code>grid[0] == grid[1] ... == grid[9]</code>. Any changes to <code>row</code> will reflect a change in <code>grid[n]</code>, <code>n</code> being 0-9 in this case. Evidence found!</p>

<p>We can also see this in action using <code>id()</code> within Python, which prints out the memory address of an object.</p>

<pre><code>&gt;&gt;&gt; grid=[['X']*10]*10
&gt;&gt;&gt; id(grid[0])
4315577152
&gt;&gt;&gt; id(grid[1])
4315577152</code></pre>

<p>The memory address for both <code>row</code>s is the same, indicating that they're pointing to the same object.</p>

<p>This property is true for lists of other items as well, for example a dictionary:</p>

<pre><code>&gt;&gt;&gt; data = [{'x':1, 'y':2}]*10
&gt;&gt;&gt; id(data[0])
4312977664
&gt;&gt;&gt; id(data[1])
4312977664</code></pre>

<p>How does this explain the two columns being changed? Here's what happened:</p>
<ul>
  <li>The indexes we tried to change were <code>grid[5][5]</code>, <code>grid[4][5]</code> and <code>grid[4][6]</code>.</li>
  <li><code>grid[4]</code> and <code>grid[5]</code> are pointing to <code>row</code>, so indexes 5 and 6 were changed in <code>row</code>.</li>
  <li>As all indexes of <code>grid</code> point to <code>row</code>, printing <code>grid</code> basically prints <code>row</code> 10 times, giving the illusion of columns being changed.</li>
</ul>

<p>The solution to this is to create the grid a different way. Either a combination of two for-loops, or a list comprehension.</p>

<pre><code># For-loops
grid=[]
for i in range(10):
    grid.append([])
    for j in range(10):
        grid[i].append('X')

# List comprehension
grid=[['X' for _ in range(10)] for _ in range(10)]</code></pre>

<p>The visualization for either of these approaches looks completely different from the earlier approach.</p>

<figure>
<img alt="There are a total of tweleve blocks - a global frame with the variable name &quot;grid&quot;. The variable name points to block representing an array of index 10. Each index of the array points to a separate block - an array of index 10 with the string &quot;X&quot; written for each index." class="dim" loading="lazy" src="/public/images/rc00/viz2.png">
<figcaption><a href="https://pythontutor.com/render.html#code=grid%3D%5B%5B'X'%20for%20_%20in%20range%2810%29%5D%20for%20_%20in%20range%2810%29%5D&amp;cumulative=false&amp;curInstr=144&amp;heapPrimitives=nevernest&amp;mode=display&amp;origin=opt-frontend.js&amp;py=3&amp;rawInputLstJSON=%5B%5D&amp;textReferences=false" target="_blank">View on Python Tutor</a></figcaption>
</figure>

<p>Instead of one <code>row</code>, there are now 10 <code>row</code>s. Changes to one <code>row</code> won't affect the other <code>row</code>s, which is the intended behavior.</p>

<p>In Python, <code>id()</code> returns different addresses for each <code>row</code>, further confirming that they're different objects.</p>

<pre><code>&gt;&gt;&gt; grid=[['X' for _ in range(10)] for _ in range(10)]
&gt;&gt;&gt; id(grid[0])
4315575040
&gt;&gt;&gt; id(grid[1])
4315559744</code></pre>
]]>
</content>

<category term="python" />

<category term="recurse" />

</entry>
<entry>
<title type="html">Schedule Posts in Jekyll</title>
<link href="https://pjg1.site/schedule-jekyll.html" rel="alternate" type="text/html" title="Schedule Posts in Jekyll" />
<published>2023-07-22T00:00:00+04:00</published>
<updated>2025-01-12T00:00:00+04:00</updated>
<id>https://pjg1.site/schedule-jekyll.html</id>
<content type="html" xml:base="https://pjg1.site/schedule-jekyll.html">
<![CDATA[<p>My current blogging setup consists of me publishing a post shortly after I've finished typing and proofreading. While this is a straightforward and easy process, it also makes it easy for me to procrastinate on posts for days on end.</p>

<p>As a way to introduce some form of accountability, I thought having the option to schedule a post in advance might be a good idea.</p>

<p>My current publishing setup is a <code>bash</code> script, that builds the site files and deploys the build to Netlify. I started finding ways to automate the execution of this script at a later date.</p>

<p>A cron job might be the first thing comes to mind, however I would like to introduce another tool that is just as useful.</p>

<h2 id="say-hello-to-at">Say hello to <code>at</code></h2>

<p>A cron job will run more than once at the schedule set by you - the same time every hour/day/week/month/year. However, I was looking for something more flexible.</p>

<p>Maybe I feel like posting one article in one week, but two the next week. One week I may post on a Monday, the other week I may feel like posting on a Friday. This is where <a href="https://linux.die.net/man/1/at"><code>at</code></a> comes in, a command line tool to execute a set of commands once, at the specified date and time.</p>

<p>It is installed by default on both Linux and Mac, however, it requires additional steps to work on Mac.</p>

<h3 id="setup-at-on-mac">Setup <code>at</code> on Mac</h3>
<p><code>atrun</code>, the utility that executes <code>at</code> jobs, is disabled by default on Mac. Enable it with the following command:</p>

<pre><code>$ sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.atrun.plist</code></pre>

<p>Additionally, <code>atrun</code> requires disk access to execute commands:</p>
<ol>
<li>Go to System Preferences &gt; Security and Privacy &gt; Privacy</li>
<li>Scroll to the Full Disk Access section and click on the + symbol.</li>
<li>Press <kbd>Cmd</kbd> + <kbd>Shift</kbd> + <kbd>G</kbd> and enter <code>/usr/libexec/atrun</code>.</li>
<li>Select <code>atrun</code>, after which it should appear in the list.</li>
</ol>

<h2 id="creating-the-script">Creating the script</h2>

<p>The script takes the following arguments:</p>

<ul>
<li><code>$1</code> - the filename of the post to publish</li>
<li><code>$2</code> - post date and time, see <code>man at</code> for the list of valid formats</li>
<li><code>$3</code> - git commit message</li>
</ul>

<p>I maintain all draft posts in the <code>_drafts</code> directory, so the script adds the directory to the filename. Additionally, I use <code>sed</code> and <code>cut</code> to get the title of the post, which I'll use later on.</p>

<pre><code>cd ~/website
post="_drafts/$1"
title="$(sed '3q;d' $post | cut -c 8-)"</code></pre>

<p>The draft posts are undated, so this script adds the date and time to the post. Since the user input can vary in format, I use <code>date</code> to convert it to a standard format â€“ <code>YYYY-MM-DD</code> for the date and <code>HH:MM:SS +00:00</code> for the time.</p>

<pre><code>date=$(date -d "$2" +%F)
pubtime=$(date -d "$2" "+%T %z")</code></pre>

<p><code>at</code> takes the set of commands in multiple ways. For this script, I'm using a <a href="https://phoenixnap.com/kb/bash-heredoc">heredoc</a>, that sends multiple commands at once.</p>

<pre><code>at $2 &lt;&lt;- EOF</code></pre>

<p><code>at</code> is supplied with the time entered by the user, followed by the heredoc syntax.</p>

<p>Any command entered after this statement is sent to <code>at</code>, till it encounters the delimiter, <code>EOF</code>. The hyphen is added to remove any tabs at the start of the line, which may be there if the heredoc is indented for readability.</p>

<p>The first command is to add the post date to the post's front matter, which I've done using <code>sed</code>. The line number can vary depending on how your front matter is formatted.</p>

<pre><code>sed -i "7 i date: $date $pubtime" $post</code></pre>

<p>The post date is also added to the filename, to match <a href="https://jekyllrb.com/docs/posts/#creating-posts">Jekyll's naming convention for posts</a>. Along with the filename change, the post is moved to the <code>_posts</code> directory, from where it will be published.</p>

<pre><code>mv $post _posts/$date-$1</code></pre>

<p>Lastly, I add the commands from my current publishing script.</p>

<pre><code>rm -rf _site
git add .
git commit -m "$3"
bundle exec jekyll build
netlify deploy --prod --dir=_site</code></pre>

<p>I could add the delimiter, <code>EOF</code> at the end of the script and call it a day. But then I started to think of additional features to add.</p>

<p>How would I know if the build was successful? Apart from checking the site after the scheduled time, was there a way I could receive a notification?</p>

<h2 id="implementing-notifications">Implementing notifications</h2>

<p>I initially thought that this would be a very complex process. I was totally wrong, as both Linux and Mac have command line tools that make creating a notification easy peezy lemon squeezy (well, almost, as Mac seems to require extra steps once again)</p>

<h3 id="linux">Linux</h3>

<p>Notifications can be created using <a href="https://letsdebug.it/post/30-linux-desktop-notifications/#combine-notify-send-with-at"><code>notify-send</code></a>. Add the following line followed by the delimiter and you're good to go!</p>

<pre><code>notify-send "Published! The post, $title, is now live."
EOF</code></pre>

<p>The first string is the title of the notification, and the second string is the message below the title.</p>

<h3 id="mac">Mac</h3>

<p>Creating a notification is very simple thanks to <a href="https://apple.stackexchange.com/a/238820"><code>osascript</code></a>:</p>

<pre><code>osascript -e \
'display notification "The post, $title is now live." with title "Published!"'</code></pre>

<p>Getting it to work with <code>at</code> is a whole different story.</p>

<p>When I tried testing with a sample notification, there was no output. Turns out, <code>at</code> sends all output, success or failure to the local mail account, which can be accessed through the <code>mail</code> command line tool.</p>

<p>Here's the output from one of the mails:</p>

<pre><code>2023-07-06 01:59:25.338 osascript[17789:286727] NSNotificationCenter connection invalid
2023-07-06 01:59:25.338 osascript[17789:286727] Connection to notification center invalid. ServerConnectionFailure: 1 invalidated: 0
2023-07-06 01:59:25.338 osascript[17789:286727] Connection to notification center invalid. ServerConnectionFailure: 1 invalidated: 0</code></pre>

<p>After some searching, I figured the problem, but had trouble finding a solution for it. The issue is with <code>at</code> executing in a different namespace, one that does not have access to create notifications. A notification can be created in the user namespace, which is why I was able to create one easily from the command line.<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup></p>

<p>I happened to find the solution from a <a href="https://github.com/julienXX/terminal-notifier/issues/144#issuecomment-311246686">GitHub issue</a>, to use a tool called <code>reattach-to-user-namespace</code>.</p>

<pre><code>$ brew install reattach-to-user-namespace
$ reattach-to-user-namespace
fatal: usage: reattach-to-user-namespace [-l] &lt;program&gt; [args...]

    Reattach to the per-user bootstrap namespace in its "Background"
    session then exec the program with args. If "-l" is given,
    rewrite the program's argv[0] so that it starts with a '-'.</code></pre>

<p>Using the mentioned format, I added the following line to the script, followed by the delimiter to complete the heredoc.</p>

<pre><code>reattach-to-user-namespace osascript -e \
'display notification "The post, $title is now live." with title "Published!"'
EOF</code></pre>

<h2 id="testing-the-script">Testing the script</h2>

<p>I've implemented the script as a shell function, <a href="https://github.com/pjg11/scripts/tree/main/schedule"><code>schedule</code></a>. I've additionally added tab completion for the filename, which works with the <code>zsh</code> shell. However, these commands would work just as fine as a <a href="https://github.com/pjg11/scripts/tree/main/schedule/schedule.sh">shell script</a>.</p>

<p>Using this post as an example, I ran <code>schedule</code> to publish this post at 4pm UTC on July 22nd.</p>

<pre><code>$ schedule schedule-jekyll.md "Jul 22 4pm" "new post: schedule posts in jekyll"
job 1 at Thu Jul 22 16:00:00 2023</code></pre>

<blockquote>
  <p><strong>Note:</strong> Your computer needs to be up and running at the specified date and time for <code>at</code> to work. So set the date and time accordingly.</p>
</blockquote>

<p>You can access the job's script by typing using the <code>-c</code> option with the job number from the resulting output. It includes the default set of environment variables followed by the commands entered from the script.</p>

<pre><code>$ at -c job 1
#!/bin/sh
# atrun uid=501 gid=20
...

cd /Users/piya/website || {
     echo 'Execution directory inaccessible' &gt;&amp;2
     exit 1
}
OLDPWD=/Users/piya; export OLDPWD
sed -i "7 i date: 2023-07-22 16:00:00 +0000" _drafts/schedule-jekyll.md
mv _drafts/schedule-jekyll.md _posts/2023-07-22-schedule-jekyll.md
rm -rf _site
git add .
git commit -m "new post: schedule posts in jekyll"
bundle exec jekyll build
netlify deploy --prod --dir=_site
reattach-to-user-namespace osascript -e \
'display notification "The post, Schedule Posts in Jekyll, is now live." with title "Published!"'</code></pre>

<p>To delete a job before its executed, type <code>atrm job $ID</code>. You can also schedule multiple posts, and view the job queue using <code>atq</code>.</p>

<p>If you're seeing this post, that means the script worked :)</p>

<h2 id="notes">Notes</h2>

<ol class="footnotes" role="doc-endnotes">
<li id="fn:1" role="doc-endnote">
<p>The closest thing to documentation I could find about this is <a href="https://github.com/ChrisJohnsen/tmux-MacOSX-pasteboard/blob/master/README.md">this article</a> for a different tool.&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
</li>
</ol>

]]>
</content>

<category term="blogging" />

<category term="jekyll" />

<category term="scripting" />

</entry>
<entry>
<title type="html">Website Updates</title>
<link href="https://pjg1.site/website-updates.html" rel="alternate" type="text/html" title="Website Updates" />
<published>2023-07-16T00:00:00+04:00</published>
<updated>2025-07-22T00:00:00+04:00</updated>
<id>https://pjg1.site/website-updates.html</id>
<content type="html" xml:base="https://pjg1.site/website-updates.html">
<![CDATA[<div class="note">
<p><strong>TL;DR</strong> The website now uses a custom domain, along with an updated theme, RSS feed and the addition of a now page.</p>
</div>

<h2 id="using-a-custom-domain">Using a Custom Domain</h2>

<p>After thinking it over for a while, I went ahead and made the switch to a custom domain â€“ <strong>pjg1.site</strong>! To celebrate, here's a little animation for the site's header, made entirely using HTML and CSS.<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup></p>

<details>
<summary>View animation</summary>
<iframe height="45px" loading="lazy" src="/public/domainanimation.html"></iframe>
</details>

<h2 id="new-page-now">New page: Now</h2>

<p>I first came across the concept of a <a href="https://nownownow.com/about">Now page</a> from <a href="https://huma.id">Humaid Alqasimi's website</a>. Rather than a general About page, a Now page mentions what a person is focused on at a particular point in time, which I related to. The page is live at <a href="/now">/now</a>.</p>

<h2 id="rss-feed-changes">RSS feed changes</h2>

<p>Once this website was up and running, I had plans to gradually move all of my <a href="https://github.com/pjg11/CTF-Writeups">CTF writeups</a> here. However, after posting a few of them here, I figured it would be easier to maintain as a separate project. For those who came to this blog for the writeups, I'll post an update soon.</p>

<p>This change breaks the RSS feed for those who've subscribed, as there are fewer posts here than the feed you see. Additionally, my drafts got added to the feed while testing a new publishing setup, spamming the feed further with unfinished posts.</p>

<p>The feed is now fixed, and I would request those who have subscribed to <a href="/feed.xml">re-subscribe</a>. Apologies for the mess.</p>

<h2 id="new-theme">New theme</h2>

<p>The part I find most annoying about maintaining a website is being able to choose a theme and stick to it. In less than a year, this site has gone through multiple re-designs, most of which didn't stick longer than a few days or weeks.</p>

<p>The issue stems from being too picky about the dark mode theme, my preferred mode of choice. Dark mode versions of most themes I made looked cluttered and/or boring compared to the light mode versions, and pre-made themes often contained too much CSS. I needed something simpler.</p>

<p>So, I decided to experiment with one theme that works for both modes.</p>

<ul>
<li>The color scheme is inspired by <a href="https://upload.wikimedia.org/wikipedia/commons/6/69/Netscape_Navigator_2_Screenshot.png">Netscape Navigator</a>, which used the combination you see here â€“ grey background, black text and underlined blue links.</li>
<li>I went for a darker grey for for all the block elements (like the TL;DR and animation blocks above) to help differentiate them from the rest of the text.</li>
<li>I experimented a bit with the layout too, a grid-like structure brought about by my inner designer.</li>
</ul>

<p>What are your thoughts about the theme? Let me know using the comment button so I can get a better idea about the look and feel.</p>

<h2 id="notes">Notes</h2>

<ol class="footnotes" role="doc-endnotes">
<li id="fn:1" role="doc-endnote">
<p>The start might be a bit wonky for Firefox users, but subsequent loops work fine. Additionally, the animation doesn't work in an RSS reader.&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
</li>
</ol>

]]>
</content>

<category term="blogging" />

</entry>
<entry>
<title type="html">Why Blog?</title>
<link href="https://pjg1.site/why-blog.html" rel="alternate" type="text/html" title="Why Blog?" />
<published>2023-06-09T00:00:00+04:00</published>
<updated>2025-07-20T00:00:00+04:00</updated>
<id>https://pjg1.site/why-blog.html</id>
<content type="html" xml:base="https://pjg1.site/why-blog.html">
<![CDATA[<p>I've been trying to write a new post for a while, but keep coming up with ways to avoid it. In this very meta post, I list down the reasons of why I want to maintain a blog.</p>

<h2 id="document-things-i-struggled-with">Document things I struggled with</h2>

<p>If I had to go through multiple sources of information to get the hang of a concept, chances are that others learning the same thing might be going through a similar process<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup>. By documenting what I struggled with, two things happen:</p>

<ol>
<li>
<p>It strengthens my understanding of the topic, and I have notes that I could reference later.</p>
</li>
<li>
<p>Hopefully, the post could serve as a good starting point for someone else, making their process to learn this concept easier.</p>
</li>
</ol>

<p>For a while, this was the only reason I had, and that created problems. Sometimes what I struggled with is pretty well-explained somewhere else, and I felt like my post on the topic would be repetitive.</p>

<p>Since I felt like this pretty often, that led to no posts for a very long time. I had to figure other reasons to keep blogging.</p>

<h2 id="making-the-learning-process-enjoyable">Making the learning process enjoyable</h2>

<p>Whenever I try to learn anything new, I either get overwhelmed by having to plan what to learn, or get extremely bored when there's too much information.</p>

<p>Taking notes has helped sometimes during learning, as I engage with the material in a more interactive way. It makes an otherwise frustrating experience slightly more entertaining.</p>

<p>It's a great approach in theory, but I don't do it very often as it's very mentally demanding. It's something that I resort to only when I have a deadline or an exam coming up.</p>

<p>I would like to revisit this approach in the form of something I actually enjoy, blogging! It will be a little harder as there's no proper structure with self-learning, and that's a discussion I'll save for another blog post.</p>

<h2 id="knowing-what-ive-learned">Knowing what I've learned</h2>

<p>I know this sounds a little weird, but let me explain.</p>

<p>Since I learnt stuff last minute all throughout school/university, I forgot most of it after the academic year ended. The stuff I learnt from assignments stuck really well with me, but I forgot most of the theory.</p>

<p>Thanks to this, my knowledge on most topics is half-baked. I know some of the basics, and then there are the basics I either don't know or forgot. I cannot say "I know the basics of [insert topic here]"</em> confidently for any topic.</p>

<p>This is a big reason why I struggle to learn, as I get stuck on learning and re-learning the basics.</p>

<p>With this blog, I have confirmed evidence of knowing a topic if I can write a decent post about it. Then I can finally stop getting stuck on the basics and focus on more advanced topics.</p>

<h2 id="figuring-out-my-niche">Figuring out my niche</h2>

<p>While I have a vague idea of topics I'm interested in, I don't find myself diving deep into any of them as my interests shift very quickly.</p>

<p>As I make more posts, it'll give me a sense of which of those interests I gravitate more towards and would like to learn more about.</p>

<p>While I would still continue to occasionally explore other unrelated topics, it's nice to have a niche that I can always go back to.</p>

<h2 id="i-like-the-concept-of-blogging">I like the concept of blogging</h2>

<p>I've liked blogging ever since I discovered it a little over 10 years ago. Other mediums/resources can get either too overwhelming (videos, discussions on social media) or too boring (books, academic papers, lectures).</p>

<p>A blog feels somewhere right in between.</p>

<p>Unlike social media, there's no algorithm controlling what posts you see or how many characters you can type. Also, having your OWN website just sounds way cooler.</p>

<p>I don't relate to the language and structure of academic papers and most books. Blog posts feel more approachable, while still explaining a concept just as well.</p>

<p>I zone out during lectures/videos if the pace is too slow or too fast. Blog posts let me read and understand stuff at my own pace.</p>

<p>Of course, blogs are not a complete replacement to any of these mediums, but it's my most preferred medium.</p>

<h2 id="strengthen-writingthinking-skills">Strengthen writing/thinking skills</h2>

<p>I believe <a href="/writing-is-thinking">writing is thinking</a>, as I've usually resorted to writing to make things clearer in my head. Lately, I got lazy and stopped writing stuff down, and that has been reflecting in my confused thinking process.</p>

<p>To get back into the practice of writing, blogging seems like a good place to start.</p>

<h2 id="interact-with-like-minded-people">Interact with like-minded people</h2>

<p>Apart from being a faciliator to the learning process, I would like the posts on this blog to hopefully spark some online interactions with people of similar interests.</p>

<p>My current method for receiving comments is through the <em>Comment via email</em> button at the bottom of each post, a neat idea I discovered from <a href="https://kevquirk.com/adding-the-post-title-to-my-reply-by-email-button">Kev Quirk's blog</a>.</p>

<h2 id="notes">Notes</h2>

<ol class="footnotes" role="doc-endnotes">
<li id="fn:1" role="doc-endnote">
<p>Julia Evans shares similar thoughts in her post, <a href="https://jvns.ca/blog/2021/05/24/blog-about-what-you-ve-struggled-with/">Blog about what you've struggled with</a>. It got me to start typing this post, so I'd recommend giving it a read if you've been finding it hard to either start or continue blogging.&nbsp;<a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
</li>
</ol>

]]>
</content>

<category term="blogging" />

</entry>
<entry>
<title type="html">Writing Is Thinking</title>
<link href="https://pjg1.site/writing-is-thinking.html" rel="alternate" type="text/html" title="Writing Is Thinking" />
<published>2023-01-26T00:00:00+04:00</published>
<updated>2025-07-06T00:00:00+04:00</updated>
<id>https://pjg1.site/writing-is-thinking.html</id>
<content type="html" xml:base="https://pjg1.site/writing-is-thinking.html">
<![CDATA[<p>I started reading <a href="https://www.goodreads.com/en/book/show/34507927">How To Take Smart Notes</a> by SÃ¶nke Ahrens recently (by recently I mean read it for a bit months ago and never picked it up since), and the overarching concept of <strong>Writing Is Thinking</strong> stood out to me. There have been many instances where I've followed this concept without even realizing:</p>

<ul>
<li>Journaling thoughts to make more sense of the situation</li>
<li>Creating notes to understand concepts</li>
<li>Brainstorming ideas by either free-form writing or mind-mapping</li>
<li>Writing down all thoughts and tasks from my head to make sense of things, also known as a braindump</li>
</ul>

<p>This excerpt from the book summarizes this idea perfectly:</p>

<blockquote>
<p>Richard Feynman once had a visitor in his office, a historian who wanted to interview him. When he spotted Feynman's notebooks, he said how delighted he was to see such "wonderful records of Feynman's thinking."</p>
<p>"No, no!" Feynman protested. "They aren't a record of my thinking process. They are my thinking process. I actually did the work on the paper."</p>
<p>"Well," the historian said, "the work was done in your head, but the record of it is still here."</p>
<p>"No, it's not a record, not really. It's working. You have to work on paper, and this is the paper."</p>
</blockquote>
]]>
</content>

<category term="blogging" />

<category term="productivity" />

</entry>
<entry>
<title type="html">Linux x64 VM on an M1 Macbook</title>
<link href="https://pjg1.site/linux-vm-setup.html" rel="alternate" type="text/html" title="Linux x64 VM on an M1 Macbook" />
<published>2023-01-13T00:00:00+04:00</published>
<updated>2025-07-06T00:00:00+04:00</updated>
<id>https://pjg1.site/linux-vm-setup.html</id>
<content type="html" xml:base="https://pjg1.site/linux-vm-setup.html">
<![CDATA[<p>I upgraded to the M1 MacBook Pro as my daily driver laptop about a year ago. Being a machine with the ARM architecture, I couldn't use the same setup that I had on my previous machine, an Intel machine. I tried a bunch of solutions before finally settling on the current one, which I'll describe in detail in this post.</p>

<h2 id="initial-trial-and-error">Initial trial and error</h2>

<p>I started off with a free trial of Parallels Desktop, which at the time (2021) provided support for M1 Macs over VMWare, which is what I used on my previous laptop. After the trial got over, I was looking for a more permanent solution. VMWare did release a version of their app for M1 soon enough, but that too seemed buggy at times.</p>

<p>Enter <a href="https://mac.getutm.app">UTM</a>, a free and open-source alternative to Parallels and VMWare for MacOS. Setting up the VM using it was a breeze due to the installation instructions as part of their documentation. I used a Kali Linux ARM VM on UTM for a long time, till I started noticing some core issues.</p>

<h2 id="core-issues">Core issues</h2>

<p>One of the them was not being able to run Linux x64 (x86-64) binaries. Oftentimes for Reverse Engineering challenges as part of CTFs, we were provided with an executable, which were created on and for the x64 architecture. Using virtualization tools (like Parallels and VMWare) meant that I could only set up an ARM VM on an ARM machine, so this stayed an issue regardless of the tool I used.</p>

<p>The main solution was emulation, i.e., running a different architecture than the host system's architecture. I tried emulation in two ways:</p>

<ul>
<li>
<p><a href="https://karton.github.io">Karton</a> is a tool to run Linux programs on macOS, a different Linux distro, or a different architecture, with Docker as its base. This worked fine but having a VM for most tasks and a docker container just to run x64 binaries felt too much.</p>
</li>
<li>
<p>UTM runs <a href="https://www.qemu.org">QEMU</a> under the hood, which provides support for Emulation. I tried setting up and running a x64 VM, but it ran extremely slowly.</p>
</li>
</ul>

<p>The other issue was that some software wouldn't install due to certain dependencies not providing support for ARM machines (at least at the time).</p>

<p>Additionally, I had some tools installed on the Mac since they worked better there, so working between two different GUIs was getting a little annoying, and the VM took up a lot of space because of the GUI. This struck an idea, which led to the current setup.</p>

<h2 id="setup-overview">Setup Overview</h2>

<ol>
<li>
<p><a href="#terminal-only-vm">Terminal-only VM</a>: From the emulation attempts, I realized that what made emulated VMs really slow was the GUI, as Karton didn't have any speed issues but the emulated VM did. I figured that having a GUI on the VM was not necessary, as most of my work was done in the terminal.</p>
</li>
<li>
<p><a href="#emulated-vlan">Emulated VLAN</a>: This part is specific for situations that require connecting to a VPN, as is my case with Hack The Box machines. I needed to use both the host and the VM for different parts of the challenge. I could only connect the VPN on one place at a time, so that meant changing the connection between the two. This got extremely annoying, and Emulated VLAN, a network mode in UTM was the solution.</p>
</li>
<li>
<p><a href="#remote-access-via-ssh">Remote access via SSH</a>: With the GUI gone, the suggested settings for file and clipboard sharing between the host and virtual machine didn't work. This was solved by accessing the VM using SSH and Port Forwarding as part of Emulated VLAN.</p>
</li>
</ol>

<h2 id="terminal-only-vm">Terminal-only VM</h2>

<p>This step requires downloading UTM and the 64-bit installer from the Kali Linux website. The setup for this VM is the same as a usual setup except for a few changes.</p>

<p>When creating a new VM, select Emulate instead of Virtualize. Most of the settings are set properly by default, so the rest of the setup continues as usual.</p>

<p>During the Kali Linux installation process, deselect the default desktop envrionment (Xfce) in the Software selection section. Since no GUI is selected, this should lead to a terminal-only installation.</p>

<figure>
<img src="/public/images/linux-vm-setup/terminal-only-selection.png "alt="Software Selection section in the Kali Linux installation process. The boxes next to all of the three listed desktop environments are unchecked." loading="lazy">
</figure>

<p>Once the installation is complete, the VM reboots and you may see the installation screen again. To fix this, switch off the VM and open the Settings page. Under the Drives section, remove the installer Drive, which has the type CD/DVD (ISO Image). The VM should boot fine from here.</p>

<figure>
<img src="/public/images/linux-vm-setup/utm-delete-installer.png" alt="The first drive (the installer) is selected in the sidebar of the UTM settings page. The &quot;Delete Drive&quot; button in displayed in red." loading="lazy">
</figure>

<h2 id="emulated-vlan">Emulated VLAN</h2>

<p>Emulated VLAN is one of the different networking modes on UTM, which are described here. The reason why this mode allows VPN connections to go through over the other modes is described by one of the UTM devs in a <a href="https://github.com/utmapp/UTM/issues/3238#issuecomment-959911107">GitHub issue</a>:</p>

<blockquote>
<p>Have you tried the emulated VLAN network mode? If any, that one would definitely go through the VPN since it's emulated in userspace. The other modes use macOS Virtualization features that might not respect the host's VPN settings.</p>
</blockquote>

<p>Open the settings page of the virtual machine, click on Network and change the network mode to Emulated VLAN.</p>

<figure>
<img alt="The Network tab is selected in the sidebar of the UTM settings page, with the Network Mode set to Emulated VLAN." loading="lazy" src="/public/images/linux-vm-setup/emulated-vlan.png">
</figure>

<p>To be able to access this machine remotely, port forwarding needs to be set, which is covered in the next step.</p>

<h2 id="remote-access-via-ssh">Remote access via SSH</h2>

<p>With the Emulated VLAN mode, we're now dealing with two separate networks - your local network containing the host machine (<code>192.168.0.X</code> range) and the Virtual LAN containing the VM (<code>10.0.2.X</code> range). To access services on the VM from the host machine, we need to map required ports of the VM to ports on the host machine.</p>

<p>This technique is called port forwarding. One such service is <code>ssh</code> at port <code>22</code>, which we can use to access the machine on the host's terminal.</p>

<h3 id="port-forwarding">Port Forwarding</h3>

<p>After changing the network mode in the previous step, a new section called Port Forwarding should appear in the sidebar. Click on New button in the Port Forwarding section, and forward port <code>22</code> on the guest machine to port <code>2200</code> on the host machine. The IP address fields are left blank, so that the defaults are applied.<sup id="fnref:1"><a class="footnote" href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup></p>

<figure>
<img src="/public/images/linux-vm-setup/port-forwarding-add.png" alt="A pop-up box appears after clicking on the New button. There are four text fields - Guest Address (left empty), Guest Port (set to 22), Host Address (left empty) and Host Port (set to 2200)." loading="lazy">
</figure>

<p>You can add additional ports to transfer files between the host and VM or as ports to listen for incoming reverse shells. I added 3 additional ports - <code>1234</code>, <code>4444</code> and <code>8888</code>).</p>

<h3 id="start-ssh-server">Start SSH server</h3>

<p>Save the settings and start up the VM. Once logged in, start the <code>ssh</code> server. If <code>ssh</code> is not installed, install it first.</p>

<pre><code>$ sudo apt install openssh-server
$ sudo systemctl start ssh.service</code></pre>

<p>If you prefer that <code>ssh</code> is enabled by default each time the VM starts, use the following command after installation:</p>

<pre><code>$ sudo systemctl enable ssh.service</code></pre>

<p>Once started, open a terminal on the host machine and log in with the credentials you've set, <code>127.0.0.1</code> as the IP and <code>2200</code> as the port number.</p>

<pre><code>$ ssh pjg1@127.0.0.1 -p 2200</code></pre>

<p>Now that your VM is accessible on the host, copy and pasting text should be the same as always. The process to transfer files is a little longer.</p>

<h3 id="file-transfer---host-to-vm">File Transfer - Host to VM</h3>

<p>First, start a server using Python on the host machine.</p>

<pre><code>$ python3 -m http.server
Serving HTTP on :: port 8000 (http://[::]:8000/) ...</code></pre>

<p>Then on the virtual machine, retrieve the file of choice using <code>wget</code> and the IP address of the host machine, generally seen in the <code>en0</code> interface.</p>

<pre><code>$ wget http://$IP:8000/$FILE</code></pre>

<h3 id="file-transfer---vm-to-host">File Transfer - VM to Host</h3>
<p>First, start a server using Python on the virtual machine, using any of the port forwarded ports.</p>

<pre><code>$ python3 -m http.server 8888
Serving HTTP on :: port 8888 (http://[::]:8888/) ...</code></pre>

<p>Then on the host machine, retrieve the file of choice using <code>wget</code> and <code>127.0.0.1</code> as the IP address.</p>

<pre><code>$ wget http://127.0.0.1:8888/$FILE</code></pre>

<p>I currently use these steps with <a href="https://iterm2.com">iTerm2</a> as the terminal. Another option is the <a href="https://medium.com/@lizrice/linux-vms-on-an-m1-based-mac-with-vscode-and-utm-d73e7cb06133">remote-ssh</a> extension, if you're a VSCode user.</p>

<p>With all of that, you're good to go!</p>

<h2 id="notes">Notes</h2>

<ol class="footnotes" role="doc-endnotes">
<li id="fn:1" role="doc-endnote">
<p>The UTM UI has been updated with their recent release. When setting a port forward in the <a href="https://www.webscalability.com/blog/2021/05/access-your-utm-vm-over-ssh/">earlier UI</a>, each text field had a label next to it, making it clear which is which. Now for some reason, the labels have been removed, but the order is the same. <a class="reversefootnote" href="#fnref:1" role="doc-backlink">â†©</a></p>
</li>
</ol>
]]>
</content>

<category term="linux" />

<category term="mac" />

<category term="virtualization" />

</entry>
<entry>
<title type="html">Creating A Blog with GitHub Pages and Jekyll</title>
<link href="https://pjg1.site/jekyll-blog-setup.html" rel="alternate" type="text/html" title="Creating A Blog with GitHub Pages and Jekyll" />
<published>2022-12-16T00:00:00+04:00</published>
<updated>2025-07-20T00:00:00+04:00</updated>
<id>https://pjg1.site/jekyll-blog-setup.html</id>
<content type="html" xml:base="https://pjg1.site/jekyll-blog-setup.html">
<![CDATA[<p>I have written blogs before on existing web-based blogging platforms like Blogger and Tumblr, lots of them. For this website, I was looking for fewer features that I could fine-tune locally on my machine.</p>

<p>Keeping this in mind, GitHub Pages with Jekyll seemed like the best option. I've documented the process step-by-step in the form of a setup post, hoping it might help those who are looking to setup their own website.</p>

<h2 id="hosting---github-pages">Hosting - GitHub Pages</h2>

<p><a href="https://pages.github.com">GitHub Pages</a> is a static site hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website. Using GitHub Pages for hosting the website seemed ideal since I'm familiar with GitHub and already have repositories there.</p>

<p>Start by creating a repository with the name <code>$USERNAME.github.io</code>, replacing <code>$USERNAME</code> with your GitHub username. Next, we move on to creating the files for the wesbite.</p>

<h2 id="site-generator---jekyll">Site Generator - Jekyll</h2>

<p>To create your website, you can do it from scratch by creating HTML and CSS files for each part of your website, or automate the process using a static site generator. The pages are pre-built before publishing, which leads to faster loading times and a lightweight website.</p>

<p><a href="https://jekyllrb.com">Jekyll</a> is one such site generator, which is the site generator used in GitHub Pages. It takes Markdown and HTML files and builds the website based on the layouts provided by the theme.</p>

<h2 id="local-setup">Local Setup</h2>

<p>Setting up Jekyll requires Ruby and Git to be installed on your machine. Git is usually installed by default, and the instructions for installing Ruby on different operating systems can be found <a href="https://jekyllrb.com/docs/installation/">here</a>.</p>

<div class="note">
<p><strong>Note:</strong> While installing Ruby on a Mac M1 machine, the build kept failing due to errors relating to OpenSSL. So if you're facing a similar problem, adding the following environement variables before the install command should help.</p>
<pre><code>$ export LDFLAGS="-L $(brew --prefix capstone)/lib"
$ export CPPFLAGS="-I $(brew --prefix capstone)/include"
$ export PKG_CONFIG_PATH="$(brew --prefix openssl@1.1)/lib/pkgconfig"</code></pre>
</div>

<p>Once installation is complete, create your new website:</p>

<pre><code>$ jekyll new &lt;directory name&gt;</code></pre>

<p>This provides a base theme to begin with, however you can modify it to your needs, which will be discussed in the next section. You can run the blog locally by typing:</p>

<pre><code>$ bundle exec jekyll serve --watch</code></pre>

<h2 id="theme---jekyllbear">Theme - jekyllBear</h2>

<p>There are a couple of sites where you can find Jekyll themes, they are listed in the <a href="https://jekyllrb.com/docs/themes/">Jekyll documentation</a>. After a lot of searching, I chose <a href="https://github.com/knhash/jekyllBear">jekyllBear</a>, a theme based on <a href="https://bearblog.dev">Bear Blog</a>, a writing-focused blogging platform. It has few but important features, making it easy to manage and also add/tweak stuff in the future.</p>

<p>Using the <a href="https://github.com/knhash/jekyllBear#installation">installation instructions</a>, I made changes to the required files in my website repository and added/modified files if required.</p>

<p>Additionally, you can make changes to the theme itself like modifying the CSS, adding <a href="https://mcpride.github.io/posts/development/2018/03/06/syntax-highlighting-with-jekyll/">syntax highlighting functionality</a> or adding <a href="https://jekyllrb.com/docs/plugins/installation/">Jekyll plugins</a>.</p>

<h2 id="add-content">Add content</h2>

<p>The theme provides some basic files to add content to â€” <code>index.md</code> for the home page, <code>about.md</code> for the about page, and a <code>_posts</code> directory where you can create blog posts.</p>

<p>Create your first post by adding a file in the <code>_posts</code> directory with the following format: <code>year-month-date-posttitle.md</code>. You can use the sample posts that come with the theme to know what format to follow for the content.</p>

<h2 id="publish-your-website">Publish your website</h2>

<p>Now that you have your content and theme ready, it's time to publish your site to GitHub Pages. Firstly, if your repository is not initialized as a <code>git</code> repository by this stage, enter the following commands:</p>

<pre><code>$ git init
$ git remote add origin https://github.com/$USERNAME/$USERNAME.github.io.git</code></pre>

<p>This initializes the repository and adds the URL of where to push the commits to, which is your website repository.</p>

<p>Lastly, I wanted to publish only the files from the build of the site and not the source files. <a href="https://gist.github.com/cobyism/4730490">This gist</a> explains on how to publish a subfolder to GitHub Pages. Based on it, I created a script to publish a post or update any part of the website.</p>

<pre><code>#!/bin/sh
jekyll build
git add .
git commit -m "$1"
git subtree push --prefix _site origin main</code></pre>

<p><code>jekyll build</code> generates the website files and stores it in the <code>_site</code> directory. The files are added and a commit is made with a message of choice, entered as the first parameter when running the script (hence the <code>$1</code>). The <code>git subtree</code> command pushes the contents of the <code>_site</code> subfolder to GitHub.</p>

<pre><code>$ chmod +x publish
$ ./publish "Initial commit"</code></pre>

<p>Once the script is run, you can see the files in your repository. Once the build is complete on GitHub (a green tick should appear next to the recent commit's hash), the content will appear on <code>$USERNAME.github.io</code>.</p>

]]>
</content>

<category term="blogging" />

<category term="jekyll" />

</entry>
</feed>
